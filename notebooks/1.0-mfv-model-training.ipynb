{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a73810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "341160eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/28 16:26:32 WARN Utils: Your hostname, Corei9-13900K-64GB, resolves to a loopback address: 127.0.1.1; using 143.107.145.69 instead (on interface enp3s0)\n",
      "25/07/28 16:26:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "https://mmlspark.azureedge.net/maven added as a remote repository with the name: repo-1\n",
      ":: loading settings :: url = jar:file:/home/matheus/Documents/Projects/PersonalProjects/fraud-detection/.venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/matheus/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/matheus/.ivy2.5.2/jars\n",
      "com.microsoft.azure#synapseml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d176e23d-1521-4ba5-a946-7d4c7fb1d724;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.microsoft.azure#synapseml_2.12;1.0.12 in central\n",
      "\tfound com.microsoft.azure#synapseml-core_2.12;1.0.12 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.4.1 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound org.scalactic#scalactic_2.12;3.2.14 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.15 in central\n",
      "\tfound io.spray#spray-json_2.12;1.3.5 in central\n",
      "\tfound com.jcraft#jsch;0.1.54 in central\n",
      "\tfound org.apache.httpcomponents.client5#httpclient5;5.1.3 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5;5.1.3 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5-h2;5.1.3 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.25 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpmime;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound com.linkedin.isolation-forest#isolation-forest_3.4.2_2.12;3.0.4 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.10 in central\n",
      "\tfound org.testng#testng;6.8.8 in central\n",
      "\tfound org.beanshell#bsh;2.0b4 in central\n",
      "\tfound com.beust#jcommander;1.27 in central\n",
      "\tfound org.scalanlp#breeze_2.12;2.1.0 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;2.1.0 in central\n",
      "\tfound org.typelevel#spire_2.12;0.17.0 in central\n",
      "\tfound org.typelevel#spire-macros_2.12;0.17.0 in central\n",
      "\tfound org.typelevel#algebra_2.12;2.0.1 in central\n",
      "\tfound org.typelevel#cats-kernel_2.12;2.1.1 in central\n",
      "\tfound org.typelevel#spire-platform_2.12;0.17.0 in central\n",
      "\tfound org.typelevel#spire-util_2.12;0.17.0 in central\n",
      "\tfound dev.ludovic.netlib#blas;3.0.1 in central\n",
      "\tfound net.sourceforge.f2j#arpack_combined_all;0.1 in central\n",
      "\tfound dev.ludovic.netlib#lapack;3.0.1 in central\n",
      "\tfound dev.ludovic.netlib#arpack;3.0.1 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.wendykierp#JTransforms;3.1 in central\n",
      "\tfound pl.edu.icm#JLargeArrays;1.5 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.7.0 in central\n",
      "\tfound com.microsoft.azure#synapseml-deep-learning_2.12;1.0.12 in central\n",
      "\tfound com.microsoft.azure#synapseml-opencv_2.12;1.0.12 in central\n",
      "\tfound org.openpnp#opencv;3.2.0-1 in central\n",
      "\tfound com.microsoft.azure#onnx-protobuf_2.12;0.9.3 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime_gpu;1.8.1 in central\n",
      "\tfound com.microsoft.azure#synapseml-cognitive_2.12;1.0.12 in central\n",
      "\tfound com.microsoft.cognitiveservices.speech#client-sdk;1.24.1 in central\n",
      "\tfound com.microsoft.azure#synapseml-vw_2.12;1.0.12 in central\n",
      "\tfound com.github.vowpalwabbit#vw-jni;9.3.0 in central\n",
      "\tfound com.microsoft.azure#synapseml-lightgbm_2.12;1.0.12 in central\n",
      "\tfound com.microsoft.ml.lightgbm#lightgbmlib;3.3.510 in central\n",
      ":: resolution report :: resolve 289ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.beust#jcommander;1.27 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.10 from central in [default]\n",
      "\tcom.github.vowpalwabbit#vw-jni;9.3.0 from central in [default]\n",
      "\tcom.github.wendykierp#JTransforms;3.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.54 from central in [default]\n",
      "\tcom.linkedin.isolation-forest#isolation-forest_3.4.2_2.12;3.0.4 from central in [default]\n",
      "\tcom.microsoft.azure#onnx-protobuf_2.12;0.9.3 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-cognitive_2.12;1.0.12 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-core_2.12;1.0.12 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-deep-learning_2.12;1.0.12 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-lightgbm_2.12;1.0.12 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-opencv_2.12;1.0.12 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-vw_2.12;1.0.12 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml_2.12;1.0.12 from central in [default]\n",
      "\tcom.microsoft.cognitiveservices.speech#client-sdk;1.24.1 from central in [default]\n",
      "\tcom.microsoft.ml.lightgbm#lightgbmlib;3.3.510 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime_gpu;1.8.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tdev.ludovic.netlib#arpack;3.0.1 from central in [default]\n",
      "\tdev.ludovic.netlib#blas;3.0.1 from central in [default]\n",
      "\tdev.ludovic.netlib#lapack;3.0.1 from central in [default]\n",
      "\tio.spray#spray-json_2.12;1.3.5 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\tnet.sourceforge.f2j#arpack_combined_all;0.1 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpmime;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents.client5#httpclient5;5.1.3 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5;5.1.3 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5-h2;5.1.3 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.4.1 from central in [default]\n",
      "\torg.beanshell#bsh;2.0b4 from central in [default]\n",
      "\torg.openpnp#opencv;3.2.0-1 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.15 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.7.0 from central in [default]\n",
      "\torg.scalactic#scalactic_2.12;3.2.14 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;2.1.0 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;2.1.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.25 from central in [default]\n",
      "\torg.testng#testng;6.8.8 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.typelevel#algebra_2.12;2.0.1 from central in [default]\n",
      "\torg.typelevel#cats-kernel_2.12;2.1.1 from central in [default]\n",
      "\torg.typelevel#spire-macros_2.12;0.17.0 from central in [default]\n",
      "\torg.typelevel#spire-platform_2.12;0.17.0 from central in [default]\n",
      "\torg.typelevel#spire-util_2.12;0.17.0 from central in [default]\n",
      "\torg.typelevel#spire_2.12;0.17.0 from central in [default]\n",
      "\tpl.edu.icm#JLargeArrays;1.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.2.0 by [org.scala-lang.modules#scala-collection-compat_2.12;2.7.0] in [default]\n",
      "\torg.apache.commons#commons-math3;3.5 by [org.apache.commons#commons-math3;3.2] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 by [org.slf4j#slf4j-api;1.7.25] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   55  |   0   |   0   |   4   ||   51  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d176e23d-1521-4ba5-a946-7d4c7fb1d724\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 51 already retrieved (0kB/5ms)\n",
      "25/07/28 16:26:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/28 16:26:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:1.0.12\").config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\").getOrCreate()\n",
    "\n",
    "df = spark.sql(\"Select 'spark' as hello \")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f7049e",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae4c16",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7974118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|        id|\n",
      "+--------------------+-----+----------+\n",
      "|[0.18046796286410...|    0|8589934592|\n",
      "|[1.20884843553557...|    0|8589934593|\n",
      "|[0.86605494464508...|    0|8589934594|\n",
      "|[0.52326145375459...|    0|8589934595|\n",
      "|[0.86605494464508...|    0|8589934596|\n",
      "|[0.52326145375459...|    0|8589934597|\n",
      "|[0.86605494464508...|    0|8589934598|\n",
      "|[-1.1907060006978...|    0|8589934599|\n",
      "|[0.52326145375459...|    0|8589934600|\n",
      "|[0.18046796286410...|    0|8589934601|\n",
      "|[1.20884843553557...|    0|8589934602|\n",
      "|[1.20884843553557...|    0|8589934603|\n",
      "|[0.86605494464508...|    0|8589934604|\n",
      "|[0.86605494464508...|    0|8589934605|\n",
      "|[0.86605494464508...|    0|8589934606|\n",
      "|[0.86605494464508...|    0|8589934607|\n",
      "|[-1.1907060006978...|    0|8589934608|\n",
      "|[1.20884843553557...|    0|8589934609|\n",
      "|[0.86605494464508...|    0|8589934610|\n",
      "|[1.20884843553557...|    0|8589934611|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "train_df = spark.read.parquet(\"../data/processed/Base_train.parquet\", header=True, inferSchema=True)\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba86bff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+---------------------+-------------------------+----------------------------+------------+------------------+----------------------+------------+------------+------------------+------------------+------------------+--------------------+--------------------------------+-----------------+-----------------+-------------+--------------+----------------+------------------+-----------------+---------------+---------------------+---------------+--------+-------------------------+---------+------------------+-------------------------+----------+\n",
      "|label|            income|name_email_similarity|prev_address_months_count|current_address_months_count|customer_age|days_since_request|intended_balcon_amount|payment_type|zip_count_4w|       velocity_6h|      velocity_24h|       velocity_4w|bank_branch_count_8w|date_of_birth_distinct_emails_4w|employment_status|credit_risk_score|email_is_free|housing_status|phone_home_valid|phone_mobile_valid|bank_months_count|has_other_cards|proposed_credit_limit|foreign_request|  source|session_length_in_minutes|device_os|keep_alive_session|device_distinct_emails_8w|        id|\n",
      "+-----+------------------+---------------------+-------------------------+----------------------------+------------+------------------+----------------------+------------+------------+------------------+------------------+------------------+--------------------+--------------------------------+-----------------+-----------------+-------------+--------------+----------------+------------------+-----------------+---------------+---------------------+---------------+--------+-------------------------+---------+------------------+-------------------------+----------+\n",
      "|    0|0.6000000000000001|   0.5366599069888133|                        0|                          67|          20|0.0157408513961524|    -1.216869166046248|          AD|        3877|  5146.22779050877| 4343.108794873534| 6400.249142458805|                   7|                              11|               CA|              102|            0|            BE|               1|                 1|                1|              0|                200.0|              0|INTERNET|       0.1669620554710689|    linux|                 1|                        1|8589934592|\n",
      "|    0|               0.9|   0.8350318213830241|                       90|                           7|          30|0.0223147971365485|   -0.7024944577891461|          AB|        3652| 7873.568975323203| 5583.058338561913| 6830.228515037824|                  13|                              18|               CA|               77|            1|            BB|               0|                 1|               28|              0|                200.0|              0|INTERNET|       6.1936966971622045|    other|                 0|                        1|8589934593|\n",
      "|    0|               0.8|   0.7546979514103302|                       27|                           7|          40|0.0210700629954638|     50.50430145549146|          AA|        4718| 970.3835202041548| 4572.231830080073| 6065.567092575337|                   3|                               6|               CA|               56|            1|            BC|               0|                 1|                1|              0|                200.0|              0|INTERNET|       18.803984856938165|    linux|                 1|                        1|8589934594|\n",
      "|    0|0.7000000000000001|   0.5416630258168701|                       23|                          28|          50|0.0085926981487713|   -1.0248591013171355|          AC|        1169|2614.9767271507903| 6320.958368063609| 5907.907767849808|                   0|                              13|               CD|              126|            1|            BC|               1|                 1|                0|              1|                500.0|              0|INTERNET|        9.559124714972016|    linux|                 1|                        1|8589934595|\n",
      "|    0|               0.8|   0.4094943923349221|                        0|                          38|          30|0.0337410070570566|   -2.8290611164171438|          AA|        2278|7005.9617608817125| 3882.182250906154| 6772.959832000281|                   7|                              16|               CA|               90|            1|            BC|               1|                 1|               30|              0|                200.0|              0|INTERNET|        5.660003086372662|    linux|                 1|                        1|8589934596|\n",
      "|    0|0.7000000000000001|   0.0763314588830813|                        0|                         188|          40|0.0086404048042417|     18.49106443501697|          AA|        3731|12596.697187090978| 6679.294358717591| 5947.452625621747|                1988|                              13|               CA|              143|            0|            BC|               0|                 1|               29|              0|               1000.0|              0|INTERNET|        19.57848741716441|    other|                 0|                        1|8589934597|\n",
      "|    0|               0.8|   0.2945959639632518|                       23|                           2|          60|0.0255377022257953|     50.51806742262471|          AA|        1599| 6188.974155396879| 7345.023162254761| 6384.938385476813|                1782|                               2|               CA|              169|            0|            BC|               1|                 1|               20|              0|               1500.0|              0|INTERNET|       27.870484833608984|    other|                 1|                        1|8589934598|\n",
      "|    0|               0.2|   0.8337384725244965|                        0|                          72|          30|0.0063891062787405|   -1.4191543952160748|          AB|         842| 6847.262173713553| 7442.025705854325| 5435.956928797013|                  13|                              14|               CA|               65|            0|            BB|               1|                 1|                1|              0|                200.0|              0|INTERNET|       2.6152204816107947|    linux|                 0|                        1|8589934599|\n",
      "|    0|0.7000000000000001|   0.6822512526041753|                        0|                          50|          20|0.0019327143274935|     39.22003384257879|          AA|        2441| 8287.048707790467| 6174.592246153113| 5981.255223293648|                   9|                              14|               CA|               70|            1|            BC|               1|                 1|               15|              0|                200.0|              0|INTERNET|        22.80625776703889|    linux|                 0|                        1|8589934600|\n",
      "|    0|0.6000000000000001|   0.0504076032730307|                        0|                          26|          20|0.0118799773970573|    -1.748995826623894|          AB|         914| 5974.542723732826| 4427.697007268366| 5940.850697596947|                   6|                              19|               CB|               24|            1|            BE|               1|                 1|               20|              0|                200.0|              0|INTERNET|        5.233674230007534|  windows|                 1|                        1|8589934601|\n",
      "|    0|               0.9|   0.8491461461846114|                        0|                         183|          20|0.0291680643036496|   -1.2533786246766971|          AD|        1617| 3148.073039674743|  7399.80743515708| 6728.702805090303|                   2|                               4|               CA|              207|            1|            BE|               0|                 1|               10|              0|               1500.0|              0|INTERNET|        2.466885809414353|    other|                 0|                        1|8589934602|\n",
      "|    0|               0.9|    0.084499361373387|                        0|                         172|          30|11.148700020404718|     31.90408866673161|          AA|        1015| 8832.072106647876| 5868.811165395857| 5383.085365225314|                  23|                               8|               CA|              118|            0|            BE|               0|                 1|               22|              0|                200.0|              0|INTERNET|       17.086917303276795|    other|                 0|                        1|8589934603|\n",
      "|    0|               0.8|   0.7414086057297581|                        0|                          44|          20| 0.039760222529651|     37.28471766800526|          AA|        1652| 8205.494524880312| 5549.620300497559| 5962.669037507806|                1340|                               6|               CA|              274|            0|            BC|               1|                 1|               21|              0|               1500.0|              0|INTERNET|         4.41651631805775|    other|                 1|                        1|8589934604|\n",
      "|    0|               0.8|   0.1172617768039042|                        0|                          35|          30|0.0048960304667915|   -0.5190279431480302|          AD|        1967|3112.7076369281126| 7647.646745954466| 6803.612512619809|                1930|                               8|               CA|               69|            1|            BD|               0|                 1|               30|              0|                500.0|              0|INTERNET|       30.768373648720107|    linux|                 0|                        1|8589934605|\n",
      "|    0|               0.8|   0.5360206589349374|                        0|                          39|          40| 5.882500768615E-4|   -0.8603587354652069|          AB|        2092|1464.8898852426307| 4424.999470961938| 6729.051202192838|                  31|                               8|               CF|              139|            1|            BB|               1|                 1|               21|              1|                200.0|              0|INTERNET|        4.830421388016743|    other|                 0|                        1|8589934606|\n",
      "|    0|               0.8|   0.5511399277499974|                        0|                          65|          30|0.0016136823799238|    103.40995340305562|          AA|        5784| 9238.878652220716| 5796.684906987239|6820.0308361707885|                   9|                              19|               CB|               49|            0|            BB|               0|                 1|                1|              0|                200.0|              0|INTERNET|       4.9644894934018255|    linux|                 1|                        1|8589934607|\n",
      "|    0|               0.2|   0.6948151869191205|                        0|                          77|          50| 0.030572113433685|   -0.9836773793646376|          AB|        1030| 8363.777890151734| 5618.527419566392| 6267.902604828153|                   5|                               6|               CA|               72|            0|            BB|               1|                 1|               24|              0|                200.0|              0|INTERNET|        19.93314069201016|  windows|                 1|                        1|8589934608|\n",
      "|    0|               0.9|   0.1953065471948632|                        0|                          55|          30| 14.54151439515067|    -1.019821821515938|          AD|        5356| 8263.995212366479| 5639.058550631396|6636.8121989612655|                   8|                              14|               CA|              227|            1|            BB|               0|                 1|               25|              0|                200.0|              0|INTERNET|        4.679459240464907|    other|                 1|                        1|8589934609|\n",
      "|    0|               0.8|   0.1410133732279357|                       10|                          12|          20|0.0111851207858803|   -1.2061477990694105|          AD|         431|  9640.75088093624| 6093.321908394771| 4847.276651255138|                   1|                              17|               CB|              152|            0|            BC|               0|                 1|               28|              0|                200.0|              0|INTERNET|       14.842802306486403|    other|                 1|                        1|8589934610|\n",
      "|    0|               0.9|   0.6451166754834192|                       20|                          70|          50|0.0083876382436211|   -0.5823376814869103|          AB|        1862| 8666.870674116484|7206.1772803501635| 6392.944330781203|                  31|                              18|               CA|              236|            1|            BA|               1|                 1|                2|              0|               1500.0|              0|INTERNET|       3.8487234616973898|  windows|                 0|                        1|8589934611|\n",
      "+-----+------------------+---------------------+-------------------------+----------------------------+------------+------------------+----------------------+------------+------------+------------------+------------------+------------------+--------------------+--------------------------------+-----------------+-----------------+-------------+--------------+----------------+------------------+-----------------+---------------+---------------------+---------------+--------+-------------------------+---------+------------------+-------------------------+----------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 16:26:36 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "feats_df = spark.read.parquet(\"../data/processed/Base_train_feats.parquet\", header=True, inferSchema=True)\n",
    "feats_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cee6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.join(feats_df, \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1329010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------------------+-------------------------+----------------------------+------------+------------------+----------------------+------------+------------+------------------+------------------+------------------+--------------------+--------------------------------+-----------------+-----------------+-------------+--------------+----------------+------------------+-----------------+---------------+---------------------+---------------+--------+-------------------------+---------+------------------+-------------------------+-----+------------+\n",
      "|fraud_bool|            income|name_email_similarity|prev_address_months_count|current_address_months_count|customer_age|days_since_request|intended_balcon_amount|payment_type|zip_count_4w|       velocity_6h|      velocity_24h|       velocity_4w|bank_branch_count_8w|date_of_birth_distinct_emails_4w|employment_status|credit_risk_score|email_is_free|housing_status|phone_home_valid|phone_mobile_valid|bank_months_count|has_other_cards|proposed_credit_limit|foreign_request|  source|session_length_in_minutes|device_os|keep_alive_session|device_distinct_emails_8w|month|          id|\n",
      "+----------+------------------+---------------------+-------------------------+----------------------------+------------+------------------+----------------------+------------+------------+------------------+------------------+------------------+--------------------+--------------------------------+-----------------+-----------------+-------------+--------------+----------------+------------------+-----------------+---------------+---------------------+---------------+--------+-------------------------+---------+------------------+-------------------------+-----+------------+\n",
      "|         0|               0.8|   0.5029606721569614|                       13|                          14|          30|0.0316992280870365|   -0.5045184799697513|          AC|        1011| 4886.699561070474| 3751.732496052664| 4221.211999552895|                   0|                              14|               CA|              237|            0|            BC|               0|                 1|                0|              0|               1000.0|              0|INTERNET|       5.2459033120485845|    other|                 1|                        1|    6|240518168576|\n",
      "|         0|               0.1|   0.2314734200491685|                        0|                         112|          40| 0.002115930940143|   -1.7558357274104417|          AB|        1012| 3829.724581542621|2208.9666630264865| 3751.629730783703|                   7|                               6|               CA|              113|            0|            BC|               0|                 1|                2|              0|                200.0|              0|INTERNET|        56.56877742312483|macintosh|                 0|                        1|    6|240518168577|\n",
      "|         0|               0.9|   0.8900153072703665|                        0|                          47|          60|0.0392051689076668|   -1.2360180149810416|          AA|        1159|5734.7579116277975|4342.4190330834335| 4289.924606200858|                2171|                               6|               CA|              111|            0|            BB|               1|                 1|                5|              0|                200.0|              0|INTERNET|       28.016202552888107|    other|                 1|                        1|    6|240518168578|\n",
      "|         0|               0.8|   0.0528982297670728|                       30|                           9|          20|0.0398045091184873|   -1.4357528269926785|          AD|         289|2905.9096158308807| 4766.394376951451| 4287.881670699107|                  20|                              17|               CA|              154|            1|            BC|               0|                 1|               20|              0|                500.0|              0|INTERNET|        3.605017041041143|  windows|                 0|                        1|    6|240518168579|\n",
      "|         0|               0.9|   0.7524834374105733|                        0|                          46|          20|0.0068819548520961|     49.19533463413711|          AA|         779|3152.6811193728736| 2106.470636870874| 4235.552988492704|                   6|                              12|               CA|              122|            1|            BC|               0|                 1|               28|              0|                200.0|              0|INTERNET|       1.3503711988637774|    other|                 1|                        1|    6|240518168580|\n",
      "|         0|               0.9|   0.3953765877312393|                        7|                          29|          40|0.0111441047123687|   -0.7218572329225855|          AB|        1386|1199.9804676581798| 5205.295879455175| 4280.046005504916|                  13|                              11|               CA|              168|            1|            BC|               1|                 1|                1|              0|               1000.0|              0|INTERNET|        5.351885352652825|  windows|                 1|                        1|    6|240518168581|\n",
      "|         0|               0.5|   0.1293238584234739|                        0|                         379|          30|0.0068421424645289|    22.622373135287447|          AA|        2778| 3127.151396407649| 3281.507161503297| 4226.440177003614|                  12|                               6|               CA|              251|            0|            BB|               1|                 1|               10|              1|                500.0|              0|INTERNET|       10.697630467581398|    other|                 1|                        1|    6|240518168582|\n",
      "|         0|               0.8|   0.8458410990420688|                        0|                         182|          50| 0.010511777736851|    17.522905542949758|          AA|        1801|2298.6771820420845| 5547.563002937535| 4609.325604322984|                1831|                               2|               CA|              209|            0|            BE|               1|                 1|               20|              0|                200.0|              0|INTERNET|        2.977975664443362|    linux|                 1|                        1|    6|240518168583|\n",
      "|         0|               0.9|   0.1398978088547782|                        0|                          50|          40| 2.106742128626E-4|    -1.275363671069203|          AC|         976| 4646.468869797731|    4232.756763273| 4305.394169393902|                   0|                               4|               CB|              203|            1|            BC|               1|                 1|                0|              0|                200.0|              0|INTERNET|       2.3741036526788246|    linux|                 0|                        1|    6|240518168584|\n",
      "|         0|               0.4|   0.1593801234508822|                        0|                         102|          20|0.0124778463463473|   -1.1842793531010491|          AC|        1070|5512.6001001082495| 5224.081470302272|  4389.29276289736|                   0|                               9|               CA|              135|            1|            BE|               1|                 1|               25|              0|                200.0|              0|INTERNET|        7.067816263540753|    other|                 0|                        1|    6|240518168585|\n",
      "|         0|0.6000000000000001|   0.7726606584257728|                       27|                           4|          20|0.0233756494546663|   -1.4602210085686926|          AD|         907| 6426.982863784403| 3365.557641814636| 3797.638518196041|                   9|                              15|               CA|               82|            1|            BE|               0|                 1|               29|              0|                200.0|              0|INTERNET|       16.016144200406536|  windows|                 0|                        1|    6|240518168586|\n",
      "|         0|               0.9|   0.2598587347654149|                       34|                           2|          50| 0.024283144584646|   -1.1757899457280214|          AB|         955| 4115.186501079907|3360.2390585816293| 4297.058815250077|                1465|                               8|               CA|              194|            1|            BC|               1|                 1|                1|              0|               1500.0|              1|INTERNET|        6.882448123317037|    linux|                 1|                        1|    6|240518168587|\n",
      "|         0|               0.2|   0.8999889307890866|                       29|                          25|          20|0.0458818173828943|   -1.1325945145595635|          AA|        1274| 1443.318122084013| 3175.298352243568| 4420.860216692659|                   2|                              13|               CA|              139|            1|            BE|               0|                 1|               31|              0|                990.0|              0|INTERNET|        28.24218144742085|macintosh|                 0|                        2|    6|240518168588|\n",
      "|         0|               0.8|   0.9976489800277852|                        0|                          50|          50|0.0068564640335035|    -1.595011860900383|          AB|        1487|1322.8593959888065|2481.1079047364897| 4417.263184130734|                 636|                               4|               CA|              170|            0|            BA|               1|                 1|               19|              1|                500.0|              0|INTERNET|        3.976400505328524|  windows|                 1|                        1|    6|240518168589|\n",
      "|         0|               0.9|   0.7458483565449934|                        0|                          58|          50|0.0066592745691556|    108.62991032452386|          AA|        1325| 4836.295547333962| 3584.883200863693| 4301.216302680568|                   5|                               3|               CA|              206|            1|            BE|               1|                 1|               15|              0|               1500.0|              0|INTERNET|        5.946669731166033|    other|                 1|                        1|    6|240518168590|\n",
      "|         0|               0.9|   0.6500948296866577|                        0|                         146|          50|0.0263530155454256|     99.41677868752224|          AA|        1966|1737.6148517421136| 3924.965583469497| 4267.133343787988|                  35|                               4|               CA|              222|            1|            BA|               1|                 1|               30|              0|                500.0|              0|INTERNET|       12.022119716660557|  windows|                 1|                        1|    6|240518168591|\n",
      "|         0|               0.8|   0.2539412719007642|                       37|                          11|          40|1.3864107077906327|   -1.2278834628728816|          AC|        1570|2330.3934016799785|2968.9111635414683| 3803.075727345069|                   0|                              10|               CA|              134|            1|            BB|               0|                 1|                0|              0|                200.0|              0|INTERNET|        5.675622060357902|    other|                 1|                        1|    6|240518168592|\n",
      "|         0|0.6000000000000001|   0.1366791120581633|                        0|                          84|          40|0.0297887232557228|   -0.9556607221825127|          AC|         857| 2674.184779620942|2429.3546275815183|3362.7468763056872|                   0|                              11|               CA|              265|            0|            BE|               1|                 0|                0|              1|               1000.0|              0|INTERNET|       3.4027916599131585|    linux|                 0|                        1|    6|240518168593|\n",
      "|         0|               0.8|   0.9075272527493784|                       29|                          14|          20| 72.11940618013662|    -1.181410646481353|          AC|        2445|  7655.79172247578| 4790.446417949638|  4400.96267181721|                   0|                              11|               CA|               73|            1|            BC|               0|                 1|                0|              0|                200.0|              0|INTERNET|        6.024784675000571|macintosh|                 0|                        1|    6|240518168594|\n",
      "|         0|               0.8|   0.4346150579016855|                        0|                          42|          50|10.620303543676686|    20.355750980555523|          AA|         400| 4365.612300612083| 3117.712563197348| 4335.378149838509|                1901|                               5|               CA|              110|            1|            BC|               0|                 1|               11|              0|                200.0|              0|INTERNET|        5.844216370564673|  windows|                 1|                        1|    6|240518168595|\n",
      "+----------+------------------+---------------------+-------------------------+----------------------------+------------+------------------+----------------------+------------+------------+------------------+------------------+------------------+--------------------+--------------------------------+-----------------+-----------------+-------------+--------------+----------------+------------------+-----------------+---------------+---------------------+---------------+--------+-------------------------+---------+------------------+-------------------------+-----+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "test_df = spark.read.parquet(\"../data/processed/Base_test.parquet\", header=True, inferSchema=True)\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6270ff",
   "metadata": {},
   "source": [
    "### Test df processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea05055d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+------------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |label|id          |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+------------+\n",
      "|[0.8660549446450884,0.013665223266932518,-0.09281583118660199,-0.8244650692231809,-0.30308984333870304,-0.1922377713867312,-0.46068941335779023,-0.6471775332989607,-0.4186775380660887,-0.9867393157753667,-1.2330297731036983,-0.4092648482440621,0.7753815793247844,1.5454487426000492,-1.0877309239494988,-0.8247492076967737,0.3602431922692519,-0.9397279320102967,-0.5376386529408637,0.9507324030303359,-0.16335577548236535,-0.3056985475982783,0.8701719642398018,-0.11570015461562284,-0.7540703245426154,-0.5970564032786111,1.733027784401921,-0.3772822917773208,0.6252613214080895,-0.4121824554687982,-0.21851219834830177,-0.20226957105954824,-0.16872333885085122,-0.15282232765796566,1.2780715880626958,-0.5886662287113965,-0.4607382835355354,-0.4416869371792718,-0.15201442235323412,-0.04136569258472746,0.08525783619447261,-0.712483968418749,1.412490659076138,-0.6048221463850704,-0.23915911042074695]|0    |240518168576|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "pipe = PipelineModel.load(\"../models/processing_pipeline\")\n",
    "test_df_processed = test_df.drop(\"month\")\n",
    "\n",
    "test_df_processed = pipe.transform(test_df_processed)\n",
    "\n",
    "test_df_processed = test_df_processed.select(col('scaled_features').alias('features'), col('fraud_bool').alias('label'), col(\"id\"))\n",
    "test_df_processed.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f38e30",
   "metadata": {},
   "source": [
    "### Fairness DF Creation\n",
    "\n",
    "To evaluate fariness we will create a dataframe with some sensitive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d23906e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud_bool</th>\n",
       "      <th>income</th>\n",
       "      <th>name_email_similarity</th>\n",
       "      <th>prev_address_months_count</th>\n",
       "      <th>current_address_months_count</th>\n",
       "      <th>customer_age</th>\n",
       "      <th>days_since_request</th>\n",
       "      <th>intended_balcon_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>zip_count_4w</th>\n",
       "      <th>...</th>\n",
       "      <th>has_other_cards</th>\n",
       "      <th>proposed_credit_limit</th>\n",
       "      <th>foreign_request</th>\n",
       "      <th>source</th>\n",
       "      <th>session_length_in_minutes</th>\n",
       "      <th>device_os</th>\n",
       "      <th>keep_alive_session</th>\n",
       "      <th>device_distinct_emails_8w</th>\n",
       "      <th>month</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.502961</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "      <td>0.031699</td>\n",
       "      <td>-0.504518</td>\n",
       "      <td>AC</td>\n",
       "      <td>1011</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>INTERNET</td>\n",
       "      <td>5.245903</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>240518168576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.231473</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>40</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>-1.755836</td>\n",
       "      <td>AB</td>\n",
       "      <td>1012</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>INTERNET</td>\n",
       "      <td>56.568777</td>\n",
       "      <td>macintosh</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>240518168577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.890015</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>60</td>\n",
       "      <td>0.039205</td>\n",
       "      <td>-1.236018</td>\n",
       "      <td>AA</td>\n",
       "      <td>1159</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>INTERNET</td>\n",
       "      <td>28.016203</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>240518168578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.052898</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>0.039805</td>\n",
       "      <td>-1.435753</td>\n",
       "      <td>AD</td>\n",
       "      <td>289</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>INTERNET</td>\n",
       "      <td>3.605017</td>\n",
       "      <td>windows</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>240518168579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.752483</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>20</td>\n",
       "      <td>0.006882</td>\n",
       "      <td>49.195335</td>\n",
       "      <td>AA</td>\n",
       "      <td>779</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>INTERNET</td>\n",
       "      <td>1.350371</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>240518168580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205006</th>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.175288</td>\n",
       "      <td>331</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>-0.458843</td>\n",
       "      <td>AA</td>\n",
       "      <td>318</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>INTERNET</td>\n",
       "      <td>2.828575</td>\n",
       "      <td>windows</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>206158431829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205007</th>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.999338</td>\n",
       "      <td>0</td>\n",
       "      <td>285</td>\n",
       "      <td>20</td>\n",
       "      <td>0.004945</td>\n",
       "      <td>-0.540756</td>\n",
       "      <td>AC</td>\n",
       "      <td>2588</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>INTERNET</td>\n",
       "      <td>4.869606</td>\n",
       "      <td>linux</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>206158431830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205008</th>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.823836</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>20</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>48.718399</td>\n",
       "      <td>AA</td>\n",
       "      <td>952</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>INTERNET</td>\n",
       "      <td>1.301001</td>\n",
       "      <td>linux</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>206158431831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205009</th>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.331495</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>30</td>\n",
       "      <td>0.013632</td>\n",
       "      <td>17.611195</td>\n",
       "      <td>AA</td>\n",
       "      <td>999</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>INTERNET</td>\n",
       "      <td>4.116756</td>\n",
       "      <td>windows</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>206158431832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205010</th>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.872910</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>50</td>\n",
       "      <td>0.025151</td>\n",
       "      <td>-0.697338</td>\n",
       "      <td>AB</td>\n",
       "      <td>1268</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>INTERNET</td>\n",
       "      <td>10.922564</td>\n",
       "      <td>windows</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>206158431833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205011 rows  32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fraud_bool  income  name_email_similarity  prev_address_months_count  \\\n",
       "0                0     0.8               0.502961                         13   \n",
       "1                0     0.1               0.231473                          0   \n",
       "2                0     0.9               0.890015                          0   \n",
       "3                0     0.8               0.052898                         30   \n",
       "4                0     0.9               0.752483                          0   \n",
       "...            ...     ...                    ...                        ...   \n",
       "205006           0     0.8               0.175288                        331   \n",
       "205007           0     0.8               0.999338                          0   \n",
       "205008           0     0.4               0.823836                          0   \n",
       "205009           0     0.6               0.331495                          0   \n",
       "205010           0     0.3               0.872910                          0   \n",
       "\n",
       "        current_address_months_count  customer_age  days_since_request  \\\n",
       "0                                 14            30            0.031699   \n",
       "1                                112            40            0.002116   \n",
       "2                                 47            60            0.039205   \n",
       "3                                  9            20            0.039805   \n",
       "4                                 46            20            0.006882   \n",
       "...                              ...           ...                 ...   \n",
       "205006                             1            40            0.002657   \n",
       "205007                           285            20            0.004945   \n",
       "205008                            46            20            0.001346   \n",
       "205009                            85            30            0.013632   \n",
       "205010                            69            50            0.025151   \n",
       "\n",
       "        intended_balcon_amount payment_type  zip_count_4w  ...  \\\n",
       "0                    -0.504518           AC          1011  ...   \n",
       "1                    -1.755836           AB          1012  ...   \n",
       "2                    -1.236018           AA          1159  ...   \n",
       "3                    -1.435753           AD           289  ...   \n",
       "4                    49.195335           AA           779  ...   \n",
       "...                        ...          ...           ...  ...   \n",
       "205006               -0.458843           AA           318  ...   \n",
       "205007               -0.540756           AC          2588  ...   \n",
       "205008               48.718399           AA           952  ...   \n",
       "205009               17.611195           AA           999  ...   \n",
       "205010               -0.697338           AB          1268  ...   \n",
       "\n",
       "        has_other_cards  proposed_credit_limit  foreign_request    source  \\\n",
       "0                     0                 1000.0                0  INTERNET   \n",
       "1                     0                  200.0                0  INTERNET   \n",
       "2                     0                  200.0                0  INTERNET   \n",
       "3                     0                  500.0                0  INTERNET   \n",
       "4                     0                  200.0                0  INTERNET   \n",
       "...                 ...                    ...              ...       ...   \n",
       "205006                0                 1500.0                0  INTERNET   \n",
       "205007                1                  200.0                0  INTERNET   \n",
       "205008                1                  200.0                0  INTERNET   \n",
       "205009                1                  200.0                0  INTERNET   \n",
       "205010                0                  200.0                0  INTERNET   \n",
       "\n",
       "        session_length_in_minutes  device_os  keep_alive_session  \\\n",
       "0                        5.245903      other                   1   \n",
       "1                       56.568777  macintosh                   0   \n",
       "2                       28.016203      other                   1   \n",
       "3                        3.605017    windows                   0   \n",
       "4                        1.350371      other                   1   \n",
       "...                           ...        ...                 ...   \n",
       "205006                   2.828575    windows                   0   \n",
       "205007                   4.869606      linux                   0   \n",
       "205008                   1.301001      linux                   1   \n",
       "205009                   4.116756    windows                   1   \n",
       "205010                  10.922564    windows                   1   \n",
       "\n",
       "        device_distinct_emails_8w month            id  \n",
       "0                               1     6  240518168576  \n",
       "1                               1     6  240518168577  \n",
       "2                               1     6  240518168578  \n",
       "3                               1     6  240518168579  \n",
       "4                               1     6  240518168580  \n",
       "...                           ...   ...           ...  \n",
       "205006                          1     6  206158431829  \n",
       "205007                          1     6  206158431830  \n",
       "205008                          1     6  206158431831  \n",
       "205009                          1     6  206158431832  \n",
       "205010                          1     6  206158431833  \n",
       "\n",
       "[205011 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pdf = test_df.toPandas()\n",
    "test_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a26c22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>customer_age</th>\n",
       "      <th>foreign_request</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>240518168576</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>240518168577</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>240518168578</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>240518168579</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>240518168580</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205006</th>\n",
       "      <td>206158431829</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205007</th>\n",
       "      <td>206158431830</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205008</th>\n",
       "      <td>206158431831</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205009</th>\n",
       "      <td>206158431832</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205010</th>\n",
       "      <td>206158431833</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205011 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  customer_age  foreign_request\n",
       "0       240518168576            30                0\n",
       "1       240518168577            40                0\n",
       "2       240518168578            60                0\n",
       "3       240518168579            20                0\n",
       "4       240518168580            20                0\n",
       "...              ...           ...              ...\n",
       "205006  206158431829            40                0\n",
       "205007  206158431830            20                0\n",
       "205008  206158431831            20                0\n",
       "205009  206158431832            30                0\n",
       "205010  206158431833            50                0\n",
       "\n",
       "[205011 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_fairness = test_pdf[[\"id\", \"customer_age\", \"foreign_request\"]]\n",
    "pdf_fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82730729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>foreign_request</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>240518168576</td>\n",
       "      <td>Not Foreign</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>240518168577</td>\n",
       "      <td>Not Foreign</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>240518168578</td>\n",
       "      <td>Not Foreign</td>\n",
       "      <td>Elderly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>240518168579</td>\n",
       "      <td>Not Foreign</td>\n",
       "      <td>Young Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>240518168580</td>\n",
       "      <td>Not Foreign</td>\n",
       "      <td>Young Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205006</th>\n",
       "      <td>206158431829</td>\n",
       "      <td>Not Foreign</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205007</th>\n",
       "      <td>206158431830</td>\n",
       "      <td>Not Foreign</td>\n",
       "      <td>Young Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205008</th>\n",
       "      <td>206158431831</td>\n",
       "      <td>Not Foreign</td>\n",
       "      <td>Young Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205009</th>\n",
       "      <td>206158431832</td>\n",
       "      <td>Not Foreign</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205010</th>\n",
       "      <td>206158431833</td>\n",
       "      <td>Not Foreign</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205011 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id foreign_request    age_group\n",
       "0       240518168576     Not Foreign        Adult\n",
       "1       240518168577     Not Foreign        Adult\n",
       "2       240518168578     Not Foreign      Elderly\n",
       "3       240518168579     Not Foreign  Young Adult\n",
       "4       240518168580     Not Foreign  Young Adult\n",
       "...              ...             ...          ...\n",
       "205006  206158431829     Not Foreign        Adult\n",
       "205007  206158431830     Not Foreign  Young Adult\n",
       "205008  206158431831     Not Foreign  Young Adult\n",
       "205009  206158431832     Not Foreign        Adult\n",
       "205010  206158431833     Not Foreign        Adult\n",
       "\n",
       "[205011 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def age_group(x):\n",
    "    if x < 18:\n",
    "        return \"Adolescent\"\n",
    "    if x < 25:\n",
    "        return \"Young Adult\"\n",
    "    if x < 60:\n",
    "        return \"Adult\"\n",
    "    return \"Elderly\"\n",
    "\n",
    "pdf_fairness = pdf_fairness.copy()\n",
    "pdf_fairness[\"age_group\"] = pdf_fairness[\"customer_age\"].map(age_group).astype(\"category\")\n",
    "pdf_fairness['foreign_request'] = pdf_fairness['foreign_request'].replace({0: \"Not Foreign\", 1: \"Foreign\"}).astype(\"category\")\n",
    "pdf_fairness = pdf_fairness.drop(\"customer_age\", axis=1)\n",
    "pdf_fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01297cc1",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Let's first experiment with a vanilla xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2835b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"xgb_fraud_detection_v1_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "800fff82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus/Documents/Projects/PersonalProjects/fraud-detection/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from xgboost.spark import SparkXGBClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql import DataFrame\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def calculate_scale_pos_weight(train_df: DataFrame) -> float:\n",
    "    \"\"\"Calculate class imbalance ratio for scale_pos_weight\"\"\"\n",
    "    class_counts = train_df.groupBy('label').count().collect()\n",
    "    count_dict = {row['label']: row['count'] for row in class_counts}\n",
    "    return count_dict[0] / count_dict[1]  # non-fraud / fraud\n",
    "\n",
    "def objective(trial: Trial, train_df: DataFrame, test_df: DataFrame, scale_pos_weight: float) -> float:\n",
    "    \"\"\"Optuna objective function to maximize AUPRC\"\"\"\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1)\n",
    "    }\n",
    "    \n",
    "    xgb_classifier = SparkXGBClassifier(\n",
    "        features_col='features',\n",
    "        label_col='label',\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        eval_metric='aucpr',\n",
    "        num_workers=4,\n",
    "        verbosity=0,\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    model = xgb_classifier.fit(train_df)\n",
    "    predictions = model.transform(test_df)\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol='label',\n",
    "        rawPredictionCol='rawPrediction',\n",
    "        metricName='areaUnderPR'\n",
    "    )\n",
    "    \n",
    "    auprc = evaluator.evaluate(predictions)\n",
    "    return auprc\n",
    "\n",
    "def train_model(train_df: DataFrame, test_df: DataFrame, model_name: str, n_trials: int = 50) -> tuple[SparkXGBClassifier, dict, float]:\n",
    "    \"\"\"\n",
    "    Train XGBoost model with Optuna hyperparameter optimization\n",
    "    Returns: (best_model, best_params, best_auprc)\n",
    "    \"\"\"\n",
    "    scale_pos_weight = calculate_scale_pos_weight(train_df)\n",
    "    \n",
    "    best_params_path = f\"../models/{model_name}_best_params.json\"\n",
    "    if not os.path.exists(best_params_path):\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=TPESampler(seed=42)\n",
    "        )\n",
    "        \n",
    "        # wrap the objective function with fixed arguments\n",
    "        func = lambda trial: objective(trial, train_df, test_df, scale_pos_weight)\n",
    "        \n",
    "        study.optimize(func, n_trials=n_trials)\n",
    "        \n",
    "        # Train final model with best parameters\n",
    "        best_params = study.best_params\n",
    "        with open(best_params_path, \"w\") as f:\n",
    "            json.dump(best_params,f, indent=4)\n",
    "    \n",
    "    with open(best_params_path) as f:\n",
    "        best_params = json.load(f)\n",
    "\n",
    "    xgb_classifier = SparkXGBClassifier(\n",
    "        features_col='features',\n",
    "        label_col='label',\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        eval_metric='aucpr',\n",
    "        num_workers=4,\n",
    "        verbosity=1,\n",
    "        **best_params\n",
    "    )\n",
    "    \n",
    "    best_model = xgb_classifier.fit(train_df)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    predictions = best_model.transform(test_df)\n",
    "    evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol='label',\n",
    "        rawPredictionCol='rawPrediction',\n",
    "        metricName='areaUnderPR'\n",
    "    )\n",
    "    best_auprc = evaluator.evaluate(predictions)\n",
    "    \n",
    "    return best_model, best_params, best_auprc\n",
    "\n",
    "def make_predictions(xgb_model: SparkXGBClassifier, test_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Make predictions using the trained model\"\"\"\n",
    "    predictions = xgb_model.transform(test_df)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def evaluate_model(predictions):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a trained XGBoost model and display metrics.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Spark DataFrame containing model predictions\n",
    "        xgb_model: Trained XGBoost model object\n",
    "        model_save_path: (optional) Path to save the model. If None, model won't be saved.\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    roc_evaluator = BinaryClassificationEvaluator(\n",
    "        rawPredictionCol=\"probability\",\n",
    "        labelCol=\"label\",\n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    auc = roc_evaluator.evaluate(predictions)\n",
    "\n",
    "    pr_evaluator = BinaryClassificationEvaluator(\n",
    "        rawPredictionCol=\"probability\",\n",
    "        labelCol=\"label\",\n",
    "        metricName=\"areaUnderPR\"\n",
    "    )\n",
    "    aupr = pr_evaluator.evaluate(predictions)\n",
    "\n",
    "    multi_evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"f1\"\n",
    "    )\n",
    "    f1 = multi_evaluator.evaluate(predictions)\n",
    "\n",
    "    print(f\"Test AUC: {auc:.4f}\")\n",
    "    print(f\"Test AUPR: {aupr:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    metrics = {\n",
    "        'auc': auc,\n",
    "        'aupr': aupr,\n",
    "        'f1': f1,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2394104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-28 16:26:41,297] A new study created in memory with name: no-name-720bc047-5388-4e39-95a3-fc01a5d9c79a\n",
      "2025-07-28 16:26:41,819 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.5290418060840998, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.15601864044243652, 'learning_rate': 0.2536999076681772, 'max_depth': 5, 'min_child_weight': 6, 'reg_alpha': 0.8661761457749352, 'reg_lambda': 0.6011150117432088, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.5779972601681014, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 233}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:26:44,855 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:26:46] Task 0 got rank 0\n",
      "[16:26:46] Task 2 got rank 2\n",
      "[16:26:46] Task 1 got rank 1\n",
      "[16:26:46] Task 3 got rank 3\n",
      "[16:26:46] [0]\ttraining-aucpr:0.06377\n",
      "[16:26:46] [1]\ttraining-aucpr:0.07758\n",
      "[16:26:46] [2]\ttraining-aucpr:0.10011\n",
      "[16:26:46] [3]\ttraining-aucpr:0.10071\n",
      "[16:26:46] [4]\ttraining-aucpr:0.10131\n",
      "[16:26:46] [5]\ttraining-aucpr:0.10998\n",
      "[16:26:46] [6]\ttraining-aucpr:0.11693\n",
      "[16:26:46] [7]\ttraining-aucpr:0.11793\n",
      "[16:26:46] [8]\ttraining-aucpr:0.12536\n",
      "[16:26:46] [9]\ttraining-aucpr:0.13193\n",
      "[16:26:46] [10]\ttraining-aucpr:0.13613\n",
      "[16:26:46] [11]\ttraining-aucpr:0.13694\n",
      "[16:26:47] [12]\ttraining-aucpr:0.13825\n",
      "[16:26:47] [13]\ttraining-aucpr:0.14122\n",
      "[16:26:47] [14]\ttraining-aucpr:0.14419\n",
      "[16:26:47] [15]\ttraining-aucpr:0.14836\n",
      "[16:26:47] [16]\ttraining-aucpr:0.15060\n",
      "[16:26:47] [17]\ttraining-aucpr:0.15361\n",
      "[16:26:47] [18]\ttraining-aucpr:0.15481\n",
      "[16:26:47] [19]\ttraining-aucpr:0.15711\n",
      "[16:26:47] [20]\ttraining-aucpr:0.15791\n",
      "[16:26:47] [21]\ttraining-aucpr:0.15946\n",
      "[16:26:47] [22]\ttraining-aucpr:0.16114\n",
      "[16:26:47] [23]\ttraining-aucpr:0.16111\n",
      "[16:26:47] [24]\ttraining-aucpr:0.16219\n",
      "[16:26:47] [25]\ttraining-aucpr:0.16305\n",
      "[16:26:47] [26]\ttraining-aucpr:0.16379\n",
      "[16:26:47] [27]\ttraining-aucpr:0.16482\n",
      "[16:26:47] [28]\ttraining-aucpr:0.16547\n",
      "[16:26:47] [29]\ttraining-aucpr:0.16630\n",
      "[16:26:47] [30]\ttraining-aucpr:0.16704\n",
      "[16:26:47] [31]\ttraining-aucpr:0.16768\n",
      "[16:26:47] [32]\ttraining-aucpr:0.16803\n",
      "[16:26:47] [33]\ttraining-aucpr:0.16847\n",
      "[16:26:47] [34]\ttraining-aucpr:0.16870\n",
      "[16:26:47] [35]\ttraining-aucpr:0.16970\n",
      "[16:26:47] [36]\ttraining-aucpr:0.16998\n",
      "[16:26:47] [37]\ttraining-aucpr:0.17027\n",
      "[16:26:47] [38]\ttraining-aucpr:0.17034\n",
      "[16:26:47] [39]\ttraining-aucpr:0.17043\n",
      "[16:26:47] [40]\ttraining-aucpr:0.17063\n",
      "[16:26:47] [41]\ttraining-aucpr:0.17162\n",
      "[16:26:47] [42]\ttraining-aucpr:0.17186\n",
      "[16:26:47] [43]\ttraining-aucpr:0.17238\n",
      "[16:26:47] [44]\ttraining-aucpr:0.17312\n",
      "[16:26:47] [45]\ttraining-aucpr:0.17491\n",
      "[16:26:48] [46]\ttraining-aucpr:0.17530\n",
      "[16:26:48] [47]\ttraining-aucpr:0.17593\n",
      "[16:26:48] [48]\ttraining-aucpr:0.17668\n",
      "[16:26:48] [49]\ttraining-aucpr:0.17733\n",
      "[16:26:48] [50]\ttraining-aucpr:0.17731\n",
      "[16:26:48] [51]\ttraining-aucpr:0.17804\n",
      "[16:26:48] [52]\ttraining-aucpr:0.17832\n",
      "[16:26:48] [53]\ttraining-aucpr:0.17831\n",
      "[16:26:48] [54]\ttraining-aucpr:0.17805\n",
      "[16:26:48] [55]\ttraining-aucpr:0.17879\n",
      "[16:26:48] [56]\ttraining-aucpr:0.17873\n",
      "[16:26:48] [57]\ttraining-aucpr:0.17897\n",
      "[16:26:48] [58]\ttraining-aucpr:0.17970\n",
      "[16:26:48] [59]\ttraining-aucpr:0.17999\n",
      "[16:26:48] [60]\ttraining-aucpr:0.18001\n",
      "[16:26:48] [61]\ttraining-aucpr:0.18045\n",
      "[16:26:48] [62]\ttraining-aucpr:0.18169\n",
      "[16:26:48] [63]\ttraining-aucpr:0.18171\n",
      "[16:26:48] [64]\ttraining-aucpr:0.18183\n",
      "[16:26:48] [65]\ttraining-aucpr:0.18196\n",
      "[16:26:48] [66]\ttraining-aucpr:0.18193\n",
      "[16:26:48] [67]\ttraining-aucpr:0.18217\n",
      "[16:26:48] [68]\ttraining-aucpr:0.18232\n",
      "[16:26:48] [69]\ttraining-aucpr:0.18266\n",
      "[16:26:48] [70]\ttraining-aucpr:0.18264\n",
      "[16:26:48] [71]\ttraining-aucpr:0.18261\n",
      "[16:26:48] [72]\ttraining-aucpr:0.18293\n",
      "[16:26:48] [73]\ttraining-aucpr:0.18318\n",
      "[16:26:48] [74]\ttraining-aucpr:0.18333\n",
      "[16:26:48] [75]\ttraining-aucpr:0.18381\n",
      "[16:26:48] [76]\ttraining-aucpr:0.18366\n",
      "[16:26:48] [77]\ttraining-aucpr:0.18448\n",
      "[16:26:48] [78]\ttraining-aucpr:0.18474\n",
      "[16:26:48] [79]\ttraining-aucpr:0.18492\n",
      "[16:26:48] [80]\ttraining-aucpr:0.18482\n",
      "[16:26:48] [81]\ttraining-aucpr:0.18517\n",
      "[16:26:49] [82]\ttraining-aucpr:0.18523\n",
      "[16:26:49] [83]\ttraining-aucpr:0.18560\n",
      "[16:26:49] [84]\ttraining-aucpr:0.18575\n",
      "[16:26:49] [85]\ttraining-aucpr:0.18607\n",
      "[16:26:49] [86]\ttraining-aucpr:0.18637\n",
      "[16:26:49] [87]\ttraining-aucpr:0.18647\n",
      "[16:26:49] [88]\ttraining-aucpr:0.18654\n",
      "[16:26:49] [89]\ttraining-aucpr:0.18649\n",
      "[16:26:49] [90]\ttraining-aucpr:0.18681\n",
      "[16:26:49] [91]\ttraining-aucpr:0.18709\n",
      "[16:26:49] [92]\ttraining-aucpr:0.18709\n",
      "[16:26:49] [93]\ttraining-aucpr:0.18746\n",
      "[16:26:49] [94]\ttraining-aucpr:0.18802\n",
      "[16:26:49] [95]\ttraining-aucpr:0.18807\n",
      "[16:26:49] [96]\ttraining-aucpr:0.18826\n",
      "[16:26:49] [97]\ttraining-aucpr:0.18847\n",
      "[16:26:49] [98]\ttraining-aucpr:0.18858\n",
      "[16:26:49] [99]\ttraining-aucpr:0.18867\n",
      "[16:26:49] [100]\ttraining-aucpr:0.18882\n",
      "[16:26:49] [101]\ttraining-aucpr:0.18924\n",
      "[16:26:49] [102]\ttraining-aucpr:0.18980\n",
      "[16:26:49] [103]\ttraining-aucpr:0.18980\n",
      "[16:26:49] [104]\ttraining-aucpr:0.19042\n",
      "[16:26:49] [105]\ttraining-aucpr:0.19038\n",
      "[16:26:49] [106]\ttraining-aucpr:0.19066\n",
      "[16:26:49] [107]\ttraining-aucpr:0.19056\n",
      "[16:26:49] [108]\ttraining-aucpr:0.19082\n",
      "[16:26:49] [109]\ttraining-aucpr:0.19094\n",
      "[16:26:49] [110]\ttraining-aucpr:0.19097\n",
      "[16:26:49] [111]\ttraining-aucpr:0.19127\n",
      "[16:26:49] [112]\ttraining-aucpr:0.19140\n",
      "[16:26:49] [113]\ttraining-aucpr:0.19195\n",
      "[16:26:49] [114]\ttraining-aucpr:0.19216\n",
      "[16:26:49] [115]\ttraining-aucpr:0.19197\n",
      "[16:26:49] [116]\ttraining-aucpr:0.19227\n",
      "[16:26:49] [117]\ttraining-aucpr:0.19277\n",
      "[16:26:49] [118]\ttraining-aucpr:0.19283\n",
      "[16:26:50] [119]\ttraining-aucpr:0.19334\n",
      "[16:26:50] [120]\ttraining-aucpr:0.19364\n",
      "[16:26:50] [121]\ttraining-aucpr:0.19399\n",
      "[16:26:50] [122]\ttraining-aucpr:0.19439\n",
      "[16:26:50] [123]\ttraining-aucpr:0.19489\n",
      "[16:26:50] [124]\ttraining-aucpr:0.19494\n",
      "[16:26:50] [125]\ttraining-aucpr:0.19512\n",
      "[16:26:50] [126]\ttraining-aucpr:0.19547\n",
      "[16:26:50] [127]\ttraining-aucpr:0.19556\n",
      "[16:26:50] [128]\ttraining-aucpr:0.19571\n",
      "[16:26:50] [129]\ttraining-aucpr:0.19600\n",
      "[16:26:50] [130]\ttraining-aucpr:0.19623\n",
      "[16:26:50] [131]\ttraining-aucpr:0.19650\n",
      "[16:26:50] [132]\ttraining-aucpr:0.19671\n",
      "[16:26:50] [133]\ttraining-aucpr:0.19697\n",
      "[16:26:50] [134]\ttraining-aucpr:0.19695\n",
      "[16:26:50] [135]\ttraining-aucpr:0.19723\n",
      "[16:26:50] [136]\ttraining-aucpr:0.19732\n",
      "[16:26:50] [137]\ttraining-aucpr:0.19691\n",
      "[16:26:50] [138]\ttraining-aucpr:0.19693\n",
      "[16:26:50] [139]\ttraining-aucpr:0.19698\n",
      "[16:26:50] [140]\ttraining-aucpr:0.19678\n",
      "[16:26:50] [141]\ttraining-aucpr:0.19686\n",
      "[16:26:50] [142]\ttraining-aucpr:0.19695\n",
      "[16:26:50] [143]\ttraining-aucpr:0.19753\n",
      "[16:26:50] [144]\ttraining-aucpr:0.19762\n",
      "[16:26:50] [145]\ttraining-aucpr:0.19774\n",
      "[16:26:50] [146]\ttraining-aucpr:0.19789\n",
      "[16:26:50] [147]\ttraining-aucpr:0.19786\n",
      "[16:26:50] [148]\ttraining-aucpr:0.19805\n",
      "[16:26:50] [149]\ttraining-aucpr:0.19819\n",
      "[16:26:50] [150]\ttraining-aucpr:0.19818\n",
      "[16:26:50] [151]\ttraining-aucpr:0.19841\n",
      "[16:26:50] [152]\ttraining-aucpr:0.19891\n",
      "[16:26:50] [153]\ttraining-aucpr:0.19920\n",
      "[16:26:50] [154]\ttraining-aucpr:0.19931\n",
      "[16:26:50] [155]\ttraining-aucpr:0.19966\n",
      "[16:26:50] [156]\ttraining-aucpr:0.19986\n",
      "[16:26:51] [157]\ttraining-aucpr:0.19978\n",
      "[16:26:51] [158]\ttraining-aucpr:0.19993\n",
      "[16:26:51] [159]\ttraining-aucpr:0.20011\n",
      "[16:26:51] [160]\ttraining-aucpr:0.20100\n",
      "[16:26:51] [161]\ttraining-aucpr:0.20141\n",
      "[16:26:51] [162]\ttraining-aucpr:0.20140\n",
      "[16:26:51] [163]\ttraining-aucpr:0.20162\n",
      "[16:26:51] [164]\ttraining-aucpr:0.20176\n",
      "[16:26:51] [165]\ttraining-aucpr:0.20214\n",
      "[16:26:51] [166]\ttraining-aucpr:0.20236\n",
      "[16:26:51] [167]\ttraining-aucpr:0.20256\n",
      "[16:26:51] [168]\ttraining-aucpr:0.20268\n",
      "[16:26:51] [169]\ttraining-aucpr:0.20278\n",
      "[16:26:51] [170]\ttraining-aucpr:0.20295\n",
      "[16:26:51] [171]\ttraining-aucpr:0.20376\n",
      "[16:26:51] [172]\ttraining-aucpr:0.20380\n",
      "[16:26:51] [173]\ttraining-aucpr:0.20400\n",
      "[16:26:51] [174]\ttraining-aucpr:0.20464\n",
      "[16:26:51] [175]\ttraining-aucpr:0.20474\n",
      "[16:26:51] [176]\ttraining-aucpr:0.20497\n",
      "[16:26:51] [177]\ttraining-aucpr:0.20535\n",
      "[16:26:51] [178]\ttraining-aucpr:0.20576\n",
      "[16:26:51] [179]\ttraining-aucpr:0.20580\n",
      "[16:26:51] [180]\ttraining-aucpr:0.20623\n",
      "[16:26:51] [181]\ttraining-aucpr:0.20629\n",
      "[16:26:51] [182]\ttraining-aucpr:0.20626\n",
      "[16:26:51] [183]\ttraining-aucpr:0.20640\n",
      "[16:26:51] [184]\ttraining-aucpr:0.20656\n",
      "[16:26:51] [185]\ttraining-aucpr:0.20692\n",
      "[16:26:51] [186]\ttraining-aucpr:0.20686\n",
      "[16:26:51] [187]\ttraining-aucpr:0.20695\n",
      "[16:26:51] [188]\ttraining-aucpr:0.20687\n",
      "[16:26:51] [189]\ttraining-aucpr:0.20680\n",
      "[16:26:51] [190]\ttraining-aucpr:0.20721\n",
      "[16:26:51] [191]\ttraining-aucpr:0.20762\n",
      "[16:26:52] [192]\ttraining-aucpr:0.20766\n",
      "[16:26:52] [193]\ttraining-aucpr:0.20792\n",
      "[16:26:52] [194]\ttraining-aucpr:0.20784\n",
      "[16:26:52] [195]\ttraining-aucpr:0.20819\n",
      "[16:26:52] [196]\ttraining-aucpr:0.20873\n",
      "[16:26:52] [197]\ttraining-aucpr:0.20904\n",
      "[16:26:52] [198]\ttraining-aucpr:0.20932\n",
      "[16:26:52] [199]\ttraining-aucpr:0.20960\n",
      "[16:26:52] [200]\ttraining-aucpr:0.21010\n",
      "[16:26:52] [201]\ttraining-aucpr:0.21028\n",
      "[16:26:52] [202]\ttraining-aucpr:0.21063\n",
      "[16:26:52] [203]\ttraining-aucpr:0.21078\n",
      "[16:26:52] [204]\ttraining-aucpr:0.21115\n",
      "[16:26:52] [205]\ttraining-aucpr:0.21151\n",
      "[16:26:52] [206]\ttraining-aucpr:0.21151\n",
      "[16:26:52] [207]\ttraining-aucpr:0.21144\n",
      "[16:26:52] [208]\ttraining-aucpr:0.21187\n",
      "[16:26:52] [209]\ttraining-aucpr:0.21209\n",
      "[16:26:52] [210]\ttraining-aucpr:0.21211\n",
      "[16:26:52] [211]\ttraining-aucpr:0.21213\n",
      "[16:26:52] [212]\ttraining-aucpr:0.21249\n",
      "[16:26:52] [213]\ttraining-aucpr:0.21267\n",
      "[16:26:52] [214]\ttraining-aucpr:0.21312\n",
      "[16:26:52] [215]\ttraining-aucpr:0.21319\n",
      "[16:26:52] [216]\ttraining-aucpr:0.21374\n",
      "[16:26:52] [217]\ttraining-aucpr:0.21371\n",
      "[16:26:52] [218]\ttraining-aucpr:0.21388\n",
      "[16:26:52] [219]\ttraining-aucpr:0.21442\n",
      "[16:26:52] [220]\ttraining-aucpr:0.21472\n",
      "[16:26:52] [221]\ttraining-aucpr:0.21507\n",
      "[16:26:52] [222]\ttraining-aucpr:0.21534\n",
      "[16:26:52] [223]\ttraining-aucpr:0.21550\n",
      "[16:26:52] [224]\ttraining-aucpr:0.21601\n",
      "[16:26:52] [225]\ttraining-aucpr:0.21644\n",
      "[16:26:52] [226]\ttraining-aucpr:0.21681\n",
      "[16:26:52] [227]\ttraining-aucpr:0.21703\n",
      "[16:26:52] [228]\ttraining-aucpr:0.21722\n",
      "[16:26:52] [229]\ttraining-aucpr:0.21761\n",
      "[16:26:53] [230]\ttraining-aucpr:0.21766\n",
      "[16:26:53] [231]\ttraining-aucpr:0.21814\n",
      "[16:26:53] [232]\ttraining-aucpr:0.21803\n",
      "2025-07-28 16:26:54,112 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:26:55,672 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:26:56,384] Trial 0 finished with value: 0.15308040898604974 and parameters: {'max_depth': 5, 'learning_rate': 0.2536999076681772, 'n_estimators': 233, 'min_child_weight': 6, 'gamma': 0.15601864044243652, 'subsample': 0.5779972601681014, 'colsample_bytree': 0.5290418060840998, 'reg_alpha': 0.8661761457749352, 'reg_lambda': 0.6011150117432088}. Best is trial 0 with value: 0.15308040898604974.\n",
      "2025-07-28 16:26:56,566 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.5917022549267169, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.21233911067827616, 'learning_rate': 0.010725209743171997, 'max_depth': 8, 'min_child_weight': 9, 'reg_alpha': 0.3042422429595377, 'reg_lambda': 0.5247564316322378, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.5909124836035503, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 293}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:26:57,997 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:26:59] Task 1 got rank 1\n",
      "[16:26:59] Task 0 got rank 0\n",
      "[16:26:59] Task 2 got rank 2\n",
      "[16:26:59] Task 3 got rank 3\n",
      "[16:26:59] [0]\ttraining-aucpr:0.08129\n",
      "[16:26:59] [1]\ttraining-aucpr:0.10162\n",
      "[16:26:59] [2]\ttraining-aucpr:0.11936\n",
      "[16:26:59] [3]\ttraining-aucpr:0.12235\n",
      "[16:26:59] [4]\ttraining-aucpr:0.12265\n",
      "[16:26:59] [5]\ttraining-aucpr:0.12893\n",
      "[16:26:59] [6]\ttraining-aucpr:0.13116\n",
      "[16:26:59] [7]\ttraining-aucpr:0.12989\n",
      "[16:26:59] [8]\ttraining-aucpr:0.13363\n",
      "[16:27:00] [9]\ttraining-aucpr:0.13428\n",
      "[16:27:00] [10]\ttraining-aucpr:0.13454\n",
      "[16:27:00] [11]\ttraining-aucpr:0.13462\n",
      "[16:27:00] [12]\ttraining-aucpr:0.13439\n",
      "[16:27:00] [13]\ttraining-aucpr:0.13775\n",
      "[16:27:00] [14]\ttraining-aucpr:0.13855\n",
      "[16:27:00] [15]\ttraining-aucpr:0.13768\n",
      "[16:27:00] [16]\ttraining-aucpr:0.13752\n",
      "[16:27:00] [17]\ttraining-aucpr:0.13856\n",
      "[16:27:00] [18]\ttraining-aucpr:0.13890\n",
      "[16:27:00] [19]\ttraining-aucpr:0.14015\n",
      "[16:27:00] [20]\ttraining-aucpr:0.14070\n",
      "[16:27:00] [21]\ttraining-aucpr:0.14253\n",
      "[16:27:00] [22]\ttraining-aucpr:0.14285\n",
      "[16:27:00] [23]\ttraining-aucpr:0.14218\n",
      "[16:27:00] [24]\ttraining-aucpr:0.14266\n",
      "[16:27:00] [25]\ttraining-aucpr:0.14371\n",
      "[16:27:00] [26]\ttraining-aucpr:0.14415\n",
      "[16:27:00] [27]\ttraining-aucpr:0.14565\n",
      "[16:27:00] [28]\ttraining-aucpr:0.14695\n",
      "[16:27:00] [29]\ttraining-aucpr:0.14784\n",
      "[16:27:01] [30]\ttraining-aucpr:0.14855\n",
      "[16:27:01] [31]\ttraining-aucpr:0.14802\n",
      "[16:27:01] [32]\ttraining-aucpr:0.14792\n",
      "[16:27:01] [33]\ttraining-aucpr:0.14785\n",
      "[16:27:01] [34]\ttraining-aucpr:0.14855\n",
      "[16:27:01] [35]\ttraining-aucpr:0.14876\n",
      "[16:27:01] [36]\ttraining-aucpr:0.14882\n",
      "[16:27:01] [37]\ttraining-aucpr:0.15007\n",
      "[16:27:01] [38]\ttraining-aucpr:0.15142\n",
      "[16:27:01] [39]\ttraining-aucpr:0.15154\n",
      "[16:27:01] [40]\ttraining-aucpr:0.15193\n",
      "[16:27:01] [41]\ttraining-aucpr:0.15201\n",
      "[16:27:01] [42]\ttraining-aucpr:0.15236\n",
      "[16:27:01] [43]\ttraining-aucpr:0.15261\n",
      "[16:27:01] [44]\ttraining-aucpr:0.15300\n",
      "[16:27:01] [45]\ttraining-aucpr:0.15351\n",
      "[16:27:01] [46]\ttraining-aucpr:0.15312\n",
      "[16:27:01] [47]\ttraining-aucpr:0.15325\n",
      "[16:27:01] [48]\ttraining-aucpr:0.15333\n",
      "[16:27:01] [49]\ttraining-aucpr:0.15341\n",
      "[16:27:01] [50]\ttraining-aucpr:0.15398\n",
      "[16:27:01] [51]\ttraining-aucpr:0.15429\n",
      "[16:27:02] [52]\ttraining-aucpr:0.15451\n",
      "[16:27:02] [53]\ttraining-aucpr:0.15418\n",
      "[16:27:02] [54]\ttraining-aucpr:0.15452\n",
      "[16:27:02] [55]\ttraining-aucpr:0.15462\n",
      "[16:27:02] [56]\ttraining-aucpr:0.15452\n",
      "[16:27:02] [57]\ttraining-aucpr:0.15486\n",
      "[16:27:02] [58]\ttraining-aucpr:0.15484\n",
      "[16:27:02] [59]\ttraining-aucpr:0.15444\n",
      "[16:27:02] [60]\ttraining-aucpr:0.15446\n",
      "[16:27:02] [61]\ttraining-aucpr:0.15516\n",
      "[16:27:02] [62]\ttraining-aucpr:0.15609\n",
      "[16:27:02] [63]\ttraining-aucpr:0.15648\n",
      "[16:27:02] [64]\ttraining-aucpr:0.15666\n",
      "[16:27:02] [65]\ttraining-aucpr:0.15722\n",
      "[16:27:02] [66]\ttraining-aucpr:0.15753\n",
      "[16:27:02] [67]\ttraining-aucpr:0.15756\n",
      "[16:27:02] [68]\ttraining-aucpr:0.15779\n",
      "[16:27:02] [69]\ttraining-aucpr:0.15806\n",
      "[16:27:02] [70]\ttraining-aucpr:0.15785\n",
      "[16:27:02] [71]\ttraining-aucpr:0.15783\n",
      "[16:27:03] [72]\ttraining-aucpr:0.15801\n",
      "[16:27:03] [73]\ttraining-aucpr:0.15884\n",
      "[16:27:03] [74]\ttraining-aucpr:0.15972\n",
      "[16:27:03] [75]\ttraining-aucpr:0.16002\n",
      "[16:27:03] [76]\ttraining-aucpr:0.16014\n",
      "[16:27:03] [77]\ttraining-aucpr:0.16012\n",
      "[16:27:03] [78]\ttraining-aucpr:0.16062\n",
      "[16:27:03] [79]\ttraining-aucpr:0.16106\n",
      "[16:27:03] [80]\ttraining-aucpr:0.16121\n",
      "[16:27:03] [81]\ttraining-aucpr:0.16093\n",
      "[16:27:03] [82]\ttraining-aucpr:0.16123\n",
      "[16:27:03] [83]\ttraining-aucpr:0.16105\n",
      "[16:27:03] [84]\ttraining-aucpr:0.16107\n",
      "[16:27:03] [85]\ttraining-aucpr:0.16127\n",
      "[16:27:03] [86]\ttraining-aucpr:0.16132\n",
      "[16:27:03] [87]\ttraining-aucpr:0.16184\n",
      "[16:27:03] [88]\ttraining-aucpr:0.16210\n",
      "[16:27:03] [89]\ttraining-aucpr:0.16249\n",
      "[16:27:03] [90]\ttraining-aucpr:0.16243\n",
      "[16:27:03] [91]\ttraining-aucpr:0.16264\n",
      "[16:27:03] [92]\ttraining-aucpr:0.16286\n",
      "[16:27:03] [93]\ttraining-aucpr:0.16285\n",
      "[16:27:04] [94]\ttraining-aucpr:0.16300\n",
      "[16:27:04] [95]\ttraining-aucpr:0.16323\n",
      "[16:27:04] [96]\ttraining-aucpr:0.16405\n",
      "[16:27:04] [97]\ttraining-aucpr:0.16452\n",
      "[16:27:04] [98]\ttraining-aucpr:0.16449\n",
      "[16:27:04] [99]\ttraining-aucpr:0.16477\n",
      "[16:27:04] [100]\ttraining-aucpr:0.16512\n",
      "[16:27:04] [101]\ttraining-aucpr:0.16526\n",
      "[16:27:04] [102]\ttraining-aucpr:0.16548\n",
      "[16:27:04] [103]\ttraining-aucpr:0.16577\n",
      "[16:27:04] [104]\ttraining-aucpr:0.16601\n",
      "[16:27:04] [105]\ttraining-aucpr:0.16623\n",
      "[16:27:04] [106]\ttraining-aucpr:0.16638\n",
      "[16:27:04] [107]\ttraining-aucpr:0.16663\n",
      "[16:27:04] [108]\ttraining-aucpr:0.16695\n",
      "[16:27:04] [109]\ttraining-aucpr:0.16734\n",
      "[16:27:04] [110]\ttraining-aucpr:0.16764\n",
      "[16:27:04] [111]\ttraining-aucpr:0.16782\n",
      "[16:27:04] [112]\ttraining-aucpr:0.16806\n",
      "[16:27:04] [113]\ttraining-aucpr:0.16796\n",
      "[16:27:04] [114]\ttraining-aucpr:0.16810\n",
      "[16:27:04] [115]\ttraining-aucpr:0.16856\n",
      "[16:27:05] [116]\ttraining-aucpr:0.16875\n",
      "[16:27:05] [117]\ttraining-aucpr:0.16874\n",
      "[16:27:05] [118]\ttraining-aucpr:0.16883\n",
      "[16:27:05] [119]\ttraining-aucpr:0.16914\n",
      "[16:27:05] [120]\ttraining-aucpr:0.16945\n",
      "[16:27:05] [121]\ttraining-aucpr:0.16960\n",
      "[16:27:05] [122]\ttraining-aucpr:0.16988\n",
      "[16:27:05] [123]\ttraining-aucpr:0.17011\n",
      "[16:27:05] [124]\ttraining-aucpr:0.17034\n",
      "[16:27:05] [125]\ttraining-aucpr:0.17078\n",
      "[16:27:05] [126]\ttraining-aucpr:0.17092\n",
      "[16:27:05] [127]\ttraining-aucpr:0.17094\n",
      "[16:27:05] [128]\ttraining-aucpr:0.17092\n",
      "[16:27:05] [129]\ttraining-aucpr:0.17129\n",
      "[16:27:05] [130]\ttraining-aucpr:0.17126\n",
      "[16:27:05] [131]\ttraining-aucpr:0.17159\n",
      "[16:27:05] [132]\ttraining-aucpr:0.17167\n",
      "[16:27:05] [133]\ttraining-aucpr:0.17184\n",
      "[16:27:05] [134]\ttraining-aucpr:0.17173\n",
      "[16:27:05] [135]\ttraining-aucpr:0.17196\n",
      "[16:27:05] [136]\ttraining-aucpr:0.17190\n",
      "[16:27:06] [137]\ttraining-aucpr:0.17242\n",
      "[16:27:06] [138]\ttraining-aucpr:0.17261\n",
      "[16:27:06] [139]\ttraining-aucpr:0.17272\n",
      "[16:27:06] [140]\ttraining-aucpr:0.17301\n",
      "[16:27:06] [141]\ttraining-aucpr:0.17327\n",
      "[16:27:06] [142]\ttraining-aucpr:0.17367\n",
      "[16:27:06] [143]\ttraining-aucpr:0.17374\n",
      "[16:27:06] [144]\ttraining-aucpr:0.17391\n",
      "[16:27:06] [145]\ttraining-aucpr:0.17406\n",
      "[16:27:06] [146]\ttraining-aucpr:0.17422\n",
      "[16:27:06] [147]\ttraining-aucpr:0.17453\n",
      "[16:27:06] [148]\ttraining-aucpr:0.17453\n",
      "[16:27:06] [149]\ttraining-aucpr:0.17461\n",
      "[16:27:06] [150]\ttraining-aucpr:0.17493\n",
      "[16:27:06] [151]\ttraining-aucpr:0.17516\n",
      "[16:27:06] [152]\ttraining-aucpr:0.17583\n",
      "[16:27:06] [153]\ttraining-aucpr:0.17635\n",
      "[16:27:06] [154]\ttraining-aucpr:0.17654\n",
      "[16:27:06] [155]\ttraining-aucpr:0.17687\n",
      "[16:27:06] [156]\ttraining-aucpr:0.17708\n",
      "[16:27:06] [157]\ttraining-aucpr:0.17766\n",
      "[16:27:06] [158]\ttraining-aucpr:0.17791\n",
      "[16:27:06] [159]\ttraining-aucpr:0.17812\n",
      "[16:27:07] [160]\ttraining-aucpr:0.17846\n",
      "[16:27:07] [161]\ttraining-aucpr:0.17890\n",
      "[16:27:07] [162]\ttraining-aucpr:0.17926\n",
      "[16:27:07] [163]\ttraining-aucpr:0.17955\n",
      "[16:27:07] [164]\ttraining-aucpr:0.17963\n",
      "[16:27:07] [165]\ttraining-aucpr:0.17994\n",
      "[16:27:07] [166]\ttraining-aucpr:0.17992\n",
      "[16:27:07] [167]\ttraining-aucpr:0.18023\n",
      "[16:27:07] [168]\ttraining-aucpr:0.18055\n",
      "[16:27:07] [169]\ttraining-aucpr:0.18081\n",
      "[16:27:07] [170]\ttraining-aucpr:0.18123\n",
      "[16:27:07] [171]\ttraining-aucpr:0.18165\n",
      "[16:27:07] [172]\ttraining-aucpr:0.18195\n",
      "[16:27:07] [173]\ttraining-aucpr:0.18250\n",
      "[16:27:07] [174]\ttraining-aucpr:0.18270\n",
      "[16:27:07] [175]\ttraining-aucpr:0.18308\n",
      "[16:27:07] [176]\ttraining-aucpr:0.18334\n",
      "[16:27:07] [177]\ttraining-aucpr:0.18342\n",
      "[16:27:07] [178]\ttraining-aucpr:0.18354\n",
      "[16:27:07] [179]\ttraining-aucpr:0.18379\n",
      "[16:27:07] [180]\ttraining-aucpr:0.18385\n",
      "[16:27:07] [181]\ttraining-aucpr:0.18419\n",
      "[16:27:07] [182]\ttraining-aucpr:0.18450\n",
      "[16:27:08] [183]\ttraining-aucpr:0.18495\n",
      "[16:27:08] [184]\ttraining-aucpr:0.18508\n",
      "[16:27:08] [185]\ttraining-aucpr:0.18515\n",
      "[16:27:08] [186]\ttraining-aucpr:0.18528\n",
      "[16:27:08] [187]\ttraining-aucpr:0.18564\n",
      "[16:27:08] [188]\ttraining-aucpr:0.18607\n",
      "[16:27:08] [189]\ttraining-aucpr:0.18627\n",
      "[16:27:08] [190]\ttraining-aucpr:0.18628\n",
      "[16:27:08] [191]\ttraining-aucpr:0.18677\n",
      "[16:27:08] [192]\ttraining-aucpr:0.18719\n",
      "[16:27:08] [193]\ttraining-aucpr:0.18738\n",
      "[16:27:08] [194]\ttraining-aucpr:0.18772\n",
      "[16:27:08] [195]\ttraining-aucpr:0.18787\n",
      "[16:27:08] [196]\ttraining-aucpr:0.18802\n",
      "[16:27:08] [197]\ttraining-aucpr:0.18816\n",
      "[16:27:08] [198]\ttraining-aucpr:0.18855\n",
      "[16:27:08] [199]\ttraining-aucpr:0.18886\n",
      "[16:27:08] [200]\ttraining-aucpr:0.18915\n",
      "[16:27:08] [201]\ttraining-aucpr:0.18938\n",
      "[16:27:08] [202]\ttraining-aucpr:0.18949\n",
      "[16:27:08] [203]\ttraining-aucpr:0.18970\n",
      "[16:27:08] [204]\ttraining-aucpr:0.18979\n",
      "[16:27:09] [205]\ttraining-aucpr:0.19042\n",
      "[16:27:09] [206]\ttraining-aucpr:0.19063\n",
      "[16:27:09] [207]\ttraining-aucpr:0.19090\n",
      "[16:27:09] [208]\ttraining-aucpr:0.19114\n",
      "[16:27:09] [209]\ttraining-aucpr:0.19131\n",
      "[16:27:09] [210]\ttraining-aucpr:0.19155\n",
      "[16:27:09] [211]\ttraining-aucpr:0.19196\n",
      "[16:27:09] [212]\ttraining-aucpr:0.19218\n",
      "[16:27:09] [213]\ttraining-aucpr:0.19238\n",
      "[16:27:09] [214]\ttraining-aucpr:0.19243\n",
      "[16:27:09] [215]\ttraining-aucpr:0.19258\n",
      "[16:27:09] [216]\ttraining-aucpr:0.19275\n",
      "[16:27:09] [217]\ttraining-aucpr:0.19298\n",
      "[16:27:09] [218]\ttraining-aucpr:0.19332\n",
      "[16:27:09] [219]\ttraining-aucpr:0.19350\n",
      "[16:27:09] [220]\ttraining-aucpr:0.19391\n",
      "[16:27:09] [221]\ttraining-aucpr:0.19418\n",
      "[16:27:09] [222]\ttraining-aucpr:0.19438\n",
      "[16:27:09] [223]\ttraining-aucpr:0.19477\n",
      "[16:27:09] [224]\ttraining-aucpr:0.19498\n",
      "[16:27:09] [225]\ttraining-aucpr:0.19529\n",
      "[16:27:09] [226]\ttraining-aucpr:0.19563\n",
      "[16:27:10] [227]\ttraining-aucpr:0.19589\n",
      "[16:27:10] [228]\ttraining-aucpr:0.19627\n",
      "[16:27:10] [229]\ttraining-aucpr:0.19671\n",
      "[16:27:10] [230]\ttraining-aucpr:0.19701\n",
      "[16:27:10] [231]\ttraining-aucpr:0.19732\n",
      "[16:27:10] [232]\ttraining-aucpr:0.19743\n",
      "[16:27:10] [233]\ttraining-aucpr:0.19751\n",
      "[16:27:10] [234]\ttraining-aucpr:0.19777\n",
      "[16:27:10] [235]\ttraining-aucpr:0.19804\n",
      "[16:27:10] [236]\ttraining-aucpr:0.19842\n",
      "[16:27:10] [237]\ttraining-aucpr:0.19869\n",
      "[16:27:10] [238]\ttraining-aucpr:0.19888\n",
      "[16:27:10] [239]\ttraining-aucpr:0.19906\n",
      "[16:27:10] [240]\ttraining-aucpr:0.19954\n",
      "[16:27:10] [241]\ttraining-aucpr:0.19972\n",
      "[16:27:10] [242]\ttraining-aucpr:0.19999\n",
      "[16:27:10] [243]\ttraining-aucpr:0.20030\n",
      "[16:27:10] [244]\ttraining-aucpr:0.20065\n",
      "[16:27:10] [245]\ttraining-aucpr:0.20082\n",
      "[16:27:10] [246]\ttraining-aucpr:0.20108\n",
      "[16:27:10] [247]\ttraining-aucpr:0.20140\n",
      "[16:27:10] [248]\ttraining-aucpr:0.20148\n",
      "[16:27:10] [249]\ttraining-aucpr:0.20187\n",
      "[16:27:11] [250]\ttraining-aucpr:0.20229\n",
      "[16:27:11] [251]\ttraining-aucpr:0.20257\n",
      "[16:27:11] [252]\ttraining-aucpr:0.20281\n",
      "[16:27:11] [253]\ttraining-aucpr:0.20300\n",
      "[16:27:11] [254]\ttraining-aucpr:0.20326\n",
      "[16:27:11] [255]\ttraining-aucpr:0.20367\n",
      "[16:27:11] [256]\ttraining-aucpr:0.20390\n",
      "[16:27:11] [257]\ttraining-aucpr:0.20408\n",
      "[16:27:11] [258]\ttraining-aucpr:0.20423\n",
      "[16:27:11] [259]\ttraining-aucpr:0.20463\n",
      "[16:27:11] [260]\ttraining-aucpr:0.20487\n",
      "[16:27:11] [261]\ttraining-aucpr:0.20531\n",
      "[16:27:11] [262]\ttraining-aucpr:0.20554\n",
      "[16:27:11] [263]\ttraining-aucpr:0.20575\n",
      "[16:27:11] [264]\ttraining-aucpr:0.20586\n",
      "[16:27:11] [265]\ttraining-aucpr:0.20607\n",
      "[16:27:11] [266]\ttraining-aucpr:0.20633\n",
      "[16:27:11] [267]\ttraining-aucpr:0.20643\n",
      "[16:27:11] [268]\ttraining-aucpr:0.20673\n",
      "[16:27:11] [269]\ttraining-aucpr:0.20689\n",
      "[16:27:11] [270]\ttraining-aucpr:0.20693\n",
      "[16:27:11] [271]\ttraining-aucpr:0.20723\n",
      "[16:27:11] [272]\ttraining-aucpr:0.20758\n",
      "[16:27:12] [273]\ttraining-aucpr:0.20779\n",
      "[16:27:12] [274]\ttraining-aucpr:0.20819\n",
      "[16:27:12] [275]\ttraining-aucpr:0.20837\n",
      "[16:27:12] [276]\ttraining-aucpr:0.20864\n",
      "[16:27:12] [277]\ttraining-aucpr:0.20876\n",
      "[16:27:12] [278]\ttraining-aucpr:0.20917\n",
      "[16:27:12] [279]\ttraining-aucpr:0.20945\n",
      "[16:27:12] [280]\ttraining-aucpr:0.20970\n",
      "[16:27:12] [281]\ttraining-aucpr:0.20979\n",
      "[16:27:12] [282]\ttraining-aucpr:0.20997\n",
      "[16:27:12] [283]\ttraining-aucpr:0.21019\n",
      "[16:27:12] [284]\ttraining-aucpr:0.21048\n",
      "[16:27:12] [285]\ttraining-aucpr:0.21075\n",
      "[16:27:12] [286]\ttraining-aucpr:0.21095\n",
      "[16:27:12] [287]\ttraining-aucpr:0.21131\n",
      "[16:27:12] [288]\ttraining-aucpr:0.21149\n",
      "[16:27:12] [289]\ttraining-aucpr:0.21158\n",
      "[16:27:12] [290]\ttraining-aucpr:0.21167\n",
      "[16:27:12] [291]\ttraining-aucpr:0.21192\n",
      "[16:27:12] [292]\ttraining-aucpr:0.21216\n",
      "2025-07-28 16:27:13,968 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:27:14,163 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:27:14,944] Trial 1 finished with value: 0.16760660731016186 and parameters: {'max_depth': 8, 'learning_rate': 0.010725209743171997, 'n_estimators': 293, 'min_child_weight': 9, 'gamma': 0.21233911067827616, 'subsample': 0.5909124836035503, 'colsample_bytree': 0.5917022549267169, 'reg_alpha': 0.3042422429595377, 'reg_lambda': 0.5247564316322378}. Best is trial 1 with value: 0.16760660731016186.\n",
      "2025-07-28 16:27:15,095 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.728034992108518, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.29214464853521815, 'learning_rate': 0.02692655251486473, 'max_depth': 6, 'min_child_weight': 2, 'reg_alpha': 0.7851759613930136, 'reg_lambda': 0.19967378215835974, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.6831809216468459, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 203}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:27:16,570 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:27:17] Task 1 got rank 1[16:27:17] Task 0 got rank 0\n",
      "\n",
      "[16:27:17] Task 2 got rank 2\n",
      "[16:27:17] Task 3 got rank 3\n",
      "[16:27:18] [0]\ttraining-aucpr:0.07213\n",
      "[16:27:18] [1]\ttraining-aucpr:0.08928\n",
      "[16:27:18] [2]\ttraining-aucpr:0.09240\n",
      "[16:27:18] [3]\ttraining-aucpr:0.09648\n",
      "[16:27:18] [4]\ttraining-aucpr:0.09552\n",
      "[16:27:18] [5]\ttraining-aucpr:0.10011\n",
      "[16:27:18] [6]\ttraining-aucpr:0.10410\n",
      "[16:27:18] [7]\ttraining-aucpr:0.10293\n",
      "[16:27:18] [8]\ttraining-aucpr:0.10403\n",
      "[16:27:18] [9]\ttraining-aucpr:0.10443\n",
      "[16:27:18] [10]\ttraining-aucpr:0.10702\n",
      "[16:27:18] [11]\ttraining-aucpr:0.10841\n",
      "[16:27:18] [12]\ttraining-aucpr:0.10791\n",
      "[16:27:18] [13]\ttraining-aucpr:0.11155\n",
      "[16:27:18] [14]\ttraining-aucpr:0.11235\n",
      "[16:27:18] [15]\ttraining-aucpr:0.11288\n",
      "[16:27:18] [16]\ttraining-aucpr:0.11321\n",
      "[16:27:18] [17]\ttraining-aucpr:0.11451\n",
      "[16:27:18] [18]\ttraining-aucpr:0.11496\n",
      "[16:27:18] [19]\ttraining-aucpr:0.11661\n",
      "[16:27:18] [20]\ttraining-aucpr:0.11701\n",
      "[16:27:18] [21]\ttraining-aucpr:0.11764\n",
      "[16:27:18] [22]\ttraining-aucpr:0.11827\n",
      "[16:27:18] [23]\ttraining-aucpr:0.11946\n",
      "[16:27:18] [24]\ttraining-aucpr:0.11957\n",
      "[16:27:19] [25]\ttraining-aucpr:0.11995\n",
      "[16:27:19] [26]\ttraining-aucpr:0.12009\n",
      "[16:27:19] [27]\ttraining-aucpr:0.12105\n",
      "[16:27:19] [28]\ttraining-aucpr:0.12112\n",
      "[16:27:19] [29]\ttraining-aucpr:0.12155\n",
      "[16:27:19] [30]\ttraining-aucpr:0.12245\n",
      "[16:27:19] [31]\ttraining-aucpr:0.12265\n",
      "[16:27:19] [32]\ttraining-aucpr:0.12256\n",
      "[16:27:19] [33]\ttraining-aucpr:0.12273\n",
      "[16:27:19] [34]\ttraining-aucpr:0.12262\n",
      "[16:27:19] [35]\ttraining-aucpr:0.12357\n",
      "[16:27:19] [36]\ttraining-aucpr:0.12449\n",
      "[16:27:19] [37]\ttraining-aucpr:0.12701\n",
      "[16:27:19] [38]\ttraining-aucpr:0.12821\n",
      "[16:27:19] [39]\ttraining-aucpr:0.12915\n",
      "[16:27:19] [40]\ttraining-aucpr:0.13009\n",
      "[16:27:19] [41]\ttraining-aucpr:0.13003\n",
      "[16:27:19] [42]\ttraining-aucpr:0.13063\n",
      "[16:27:19] [43]\ttraining-aucpr:0.13131\n",
      "[16:27:19] [44]\ttraining-aucpr:0.13164\n",
      "[16:27:19] [45]\ttraining-aucpr:0.13174\n",
      "[16:27:19] [46]\ttraining-aucpr:0.13144\n",
      "[16:27:19] [47]\ttraining-aucpr:0.13173\n",
      "[16:27:19] [48]\ttraining-aucpr:0.13191\n",
      "[16:27:19] [49]\ttraining-aucpr:0.13238\n",
      "[16:27:19] [50]\ttraining-aucpr:0.13354\n",
      "[16:27:19] [51]\ttraining-aucpr:0.13408\n",
      "[16:27:19] [52]\ttraining-aucpr:0.13444\n",
      "[16:27:20] [53]\ttraining-aucpr:0.13502\n",
      "[16:27:20] [54]\ttraining-aucpr:0.13614\n",
      "[16:27:20] [55]\ttraining-aucpr:0.13694\n",
      "[16:27:20] [56]\ttraining-aucpr:0.13734\n",
      "[16:27:20] [57]\ttraining-aucpr:0.13776\n",
      "[16:27:20] [58]\ttraining-aucpr:0.13822\n",
      "[16:27:20] [59]\ttraining-aucpr:0.13824\n",
      "[16:27:20] [60]\ttraining-aucpr:0.13902\n",
      "[16:27:20] [61]\ttraining-aucpr:0.13926\n",
      "[16:27:20] [62]\ttraining-aucpr:0.13973\n",
      "[16:27:20] [63]\ttraining-aucpr:0.14008\n",
      "[16:27:20] [64]\ttraining-aucpr:0.14040\n",
      "[16:27:20] [65]\ttraining-aucpr:0.14099\n",
      "[16:27:20] [66]\ttraining-aucpr:0.14219\n",
      "[16:27:20] [67]\ttraining-aucpr:0.14257\n",
      "[16:27:20] [68]\ttraining-aucpr:0.14283\n",
      "[16:27:20] [69]\ttraining-aucpr:0.14321\n",
      "[16:27:20] [70]\ttraining-aucpr:0.14381\n",
      "[16:27:20] [71]\ttraining-aucpr:0.14434\n",
      "[16:27:20] [72]\ttraining-aucpr:0.14496\n",
      "[16:27:20] [73]\ttraining-aucpr:0.14536\n",
      "[16:27:20] [74]\ttraining-aucpr:0.14623\n",
      "[16:27:20] [75]\ttraining-aucpr:0.14637\n",
      "[16:27:20] [76]\ttraining-aucpr:0.14711\n",
      "[16:27:20] [77]\ttraining-aucpr:0.14765\n",
      "[16:27:20] [78]\ttraining-aucpr:0.14836\n",
      "[16:27:20] [79]\ttraining-aucpr:0.14859\n",
      "[16:27:20] [80]\ttraining-aucpr:0.14916\n",
      "[16:27:20] [81]\ttraining-aucpr:0.14947\n",
      "[16:27:20] [82]\ttraining-aucpr:0.15003\n",
      "[16:27:21] [83]\ttraining-aucpr:0.15076\n",
      "[16:27:21] [84]\ttraining-aucpr:0.15147\n",
      "[16:27:21] [85]\ttraining-aucpr:0.15218\n",
      "[16:27:21] [86]\ttraining-aucpr:0.15295\n",
      "[16:27:21] [87]\ttraining-aucpr:0.15347\n",
      "[16:27:21] [88]\ttraining-aucpr:0.15391\n",
      "[16:27:21] [89]\ttraining-aucpr:0.15443\n",
      "[16:27:21] [90]\ttraining-aucpr:0.15453\n",
      "[16:27:21] [91]\ttraining-aucpr:0.15520\n",
      "[16:27:21] [92]\ttraining-aucpr:0.15567\n",
      "[16:27:21] [93]\ttraining-aucpr:0.15604\n",
      "[16:27:21] [94]\ttraining-aucpr:0.15628\n",
      "[16:27:21] [95]\ttraining-aucpr:0.15650\n",
      "[16:27:21] [96]\ttraining-aucpr:0.15704\n",
      "[16:27:21] [97]\ttraining-aucpr:0.15750\n",
      "[16:27:21] [98]\ttraining-aucpr:0.15772\n",
      "[16:27:21] [99]\ttraining-aucpr:0.15815\n",
      "[16:27:21] [100]\ttraining-aucpr:0.15838\n",
      "[16:27:21] [101]\ttraining-aucpr:0.15882\n",
      "[16:27:21] [102]\ttraining-aucpr:0.15913\n",
      "[16:27:21] [103]\ttraining-aucpr:0.15961\n",
      "[16:27:21] [104]\ttraining-aucpr:0.16019\n",
      "[16:27:21] [105]\ttraining-aucpr:0.16055\n",
      "[16:27:21] [106]\ttraining-aucpr:0.16101\n",
      "[16:27:21] [107]\ttraining-aucpr:0.16130\n",
      "[16:27:21] [108]\ttraining-aucpr:0.16167\n",
      "[16:27:21] [109]\ttraining-aucpr:0.16226\n",
      "[16:27:21] [110]\ttraining-aucpr:0.16269\n",
      "[16:27:21] [111]\ttraining-aucpr:0.16318\n",
      "[16:27:22] [112]\ttraining-aucpr:0.16380\n",
      "[16:27:22] [113]\ttraining-aucpr:0.16415\n",
      "[16:27:22] [114]\ttraining-aucpr:0.16445\n",
      "[16:27:22] [115]\ttraining-aucpr:0.16486\n",
      "[16:27:22] [116]\ttraining-aucpr:0.16535\n",
      "[16:27:22] [117]\ttraining-aucpr:0.16550\n",
      "[16:27:22] [118]\ttraining-aucpr:0.16578\n",
      "[16:27:22] [119]\ttraining-aucpr:0.16594\n",
      "[16:27:22] [120]\ttraining-aucpr:0.16625\n",
      "[16:27:22] [121]\ttraining-aucpr:0.16662\n",
      "[16:27:22] [122]\ttraining-aucpr:0.16701\n",
      "[16:27:22] [123]\ttraining-aucpr:0.16747\n",
      "[16:27:22] [124]\ttraining-aucpr:0.16807\n",
      "[16:27:22] [125]\ttraining-aucpr:0.16853\n",
      "[16:27:22] [126]\ttraining-aucpr:0.16910\n",
      "[16:27:22] [127]\ttraining-aucpr:0.16928\n",
      "[16:27:22] [128]\ttraining-aucpr:0.16960\n",
      "[16:27:22] [129]\ttraining-aucpr:0.17002\n",
      "[16:27:22] [130]\ttraining-aucpr:0.17029\n",
      "[16:27:22] [131]\ttraining-aucpr:0.17049\n",
      "[16:27:22] [132]\ttraining-aucpr:0.17067\n",
      "[16:27:22] [133]\ttraining-aucpr:0.17089\n",
      "[16:27:22] [134]\ttraining-aucpr:0.17103\n",
      "[16:27:22] [135]\ttraining-aucpr:0.17124\n",
      "[16:27:22] [136]\ttraining-aucpr:0.17136\n",
      "[16:27:22] [137]\ttraining-aucpr:0.17156\n",
      "[16:27:22] [138]\ttraining-aucpr:0.17187\n",
      "[16:27:22] [139]\ttraining-aucpr:0.17216\n",
      "[16:27:22] [140]\ttraining-aucpr:0.17239\n",
      "[16:27:22] [141]\ttraining-aucpr:0.17264\n",
      "[16:27:23] [142]\ttraining-aucpr:0.17335\n",
      "[16:27:23] [143]\ttraining-aucpr:0.17365\n",
      "[16:27:23] [144]\ttraining-aucpr:0.17396\n",
      "[16:27:23] [145]\ttraining-aucpr:0.17418\n",
      "[16:27:23] [146]\ttraining-aucpr:0.17445\n",
      "[16:27:23] [147]\ttraining-aucpr:0.17478\n",
      "[16:27:23] [148]\ttraining-aucpr:0.17510\n",
      "[16:27:23] [149]\ttraining-aucpr:0.17539\n",
      "[16:27:23] [150]\ttraining-aucpr:0.17573\n",
      "[16:27:23] [151]\ttraining-aucpr:0.17610\n",
      "[16:27:23] [152]\ttraining-aucpr:0.17628\n",
      "[16:27:23] [153]\ttraining-aucpr:0.17668\n",
      "[16:27:23] [154]\ttraining-aucpr:0.17697\n",
      "[16:27:23] [155]\ttraining-aucpr:0.17720\n",
      "[16:27:23] [156]\ttraining-aucpr:0.17753\n",
      "[16:27:23] [157]\ttraining-aucpr:0.17800\n",
      "[16:27:23] [158]\ttraining-aucpr:0.17815\n",
      "[16:27:23] [159]\ttraining-aucpr:0.17839\n",
      "[16:27:23] [160]\ttraining-aucpr:0.17879\n",
      "[16:27:23] [161]\ttraining-aucpr:0.17900\n",
      "[16:27:23] [162]\ttraining-aucpr:0.17921\n",
      "[16:27:23] [163]\ttraining-aucpr:0.17943\n",
      "[16:27:23] [164]\ttraining-aucpr:0.17951\n",
      "[16:27:23] [165]\ttraining-aucpr:0.17980\n",
      "[16:27:23] [166]\ttraining-aucpr:0.17997\n",
      "[16:27:23] [167]\ttraining-aucpr:0.18022\n",
      "[16:27:23] [168]\ttraining-aucpr:0.18038\n",
      "[16:27:23] [169]\ttraining-aucpr:0.18075\n",
      "[16:27:23] [170]\ttraining-aucpr:0.18121\n",
      "[16:27:23] [171]\ttraining-aucpr:0.18169\n",
      "[16:27:24] [172]\ttraining-aucpr:0.18202\n",
      "[16:27:24] [173]\ttraining-aucpr:0.18245\n",
      "[16:27:24] [174]\ttraining-aucpr:0.18283\n",
      "[16:27:24] [175]\ttraining-aucpr:0.18316\n",
      "[16:27:24] [176]\ttraining-aucpr:0.18394\n",
      "[16:27:24] [177]\ttraining-aucpr:0.18397\n",
      "[16:27:24] [178]\ttraining-aucpr:0.18441\n",
      "[16:27:24] [179]\ttraining-aucpr:0.18463\n",
      "[16:27:24] [180]\ttraining-aucpr:0.18489\n",
      "[16:27:24] [181]\ttraining-aucpr:0.18514\n",
      "[16:27:24] [182]\ttraining-aucpr:0.18540\n",
      "[16:27:24] [183]\ttraining-aucpr:0.18554\n",
      "[16:27:24] [184]\ttraining-aucpr:0.18609\n",
      "[16:27:24] [185]\ttraining-aucpr:0.18627\n",
      "[16:27:24] [186]\ttraining-aucpr:0.18648\n",
      "[16:27:24] [187]\ttraining-aucpr:0.18663\n",
      "[16:27:24] [188]\ttraining-aucpr:0.18667\n",
      "[16:27:24] [189]\ttraining-aucpr:0.18678\n",
      "[16:27:24] [190]\ttraining-aucpr:0.18717\n",
      "[16:27:24] [191]\ttraining-aucpr:0.18734\n",
      "[16:27:24] [192]\ttraining-aucpr:0.18763\n",
      "[16:27:24] [193]\ttraining-aucpr:0.18778\n",
      "[16:27:24] [194]\ttraining-aucpr:0.18795\n",
      "[16:27:24] [195]\ttraining-aucpr:0.18835\n",
      "[16:27:24] [196]\ttraining-aucpr:0.18853\n",
      "[16:27:24] [197]\ttraining-aucpr:0.18858\n",
      "[16:27:24] [198]\ttraining-aucpr:0.18895\n",
      "[16:27:24] [199]\ttraining-aucpr:0.18902\n",
      "[16:27:24] [200]\ttraining-aucpr:0.18925\n",
      "[16:27:24] [201]\ttraining-aucpr:0.18935\n",
      "[16:27:24] [202]\ttraining-aucpr:0.18959\n",
      "2025-07-28 16:27:26,009 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "25/07/28 16:27:26 WARN DAGScheduler: Broadcasting large task binary with size 1182.2 KiB\n",
      "2025-07-28 16:27:26,111 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:27:26,806] Trial 2 finished with value: 0.18255500709762018 and parameters: {'max_depth': 6, 'learning_rate': 0.02692655251486473, 'n_estimators': 203, 'min_child_weight': 2, 'gamma': 0.29214464853521815, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518, 'reg_alpha': 0.7851759613930136, 'reg_lambda': 0.19967378215835974}. Best is trial 2 with value: 0.18255500709762018.\n",
      "2025-07-28 16:27:26,931 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.9744427686266666, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.17052412368729153, 'learning_rate': 0.07500118950416987, 'max_depth': 7, 'min_child_weight': 7, 'reg_alpha': 0.9656320330745594, 'reg_lambda': 0.8083973481164611, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.5325257964926398, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 61}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:27:28,401 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:27:29] Task 0 got rank 0\n",
      "[16:27:29] Task 1 got rank 1[16:27:29] Task 3 got rank 3\n",
      "\n",
      "[16:27:29] Task 2 got rank 2\n",
      "[16:27:29] [0]\ttraining-aucpr:0.07473\n",
      "[16:27:30] [1]\ttraining-aucpr:0.09324\n",
      "[16:27:30] [2]\ttraining-aucpr:0.10181\n",
      "[16:27:30] [3]\ttraining-aucpr:0.10652\n",
      "[16:27:30] [4]\ttraining-aucpr:0.10732\n",
      "[16:27:30] [5]\ttraining-aucpr:0.11269\n",
      "[16:27:30] [6]\ttraining-aucpr:0.11931\n",
      "[16:27:30] [7]\ttraining-aucpr:0.11980\n",
      "[16:27:30] [8]\ttraining-aucpr:0.12255\n",
      "[16:27:30] [9]\ttraining-aucpr:0.12515\n",
      "[16:27:30] [10]\ttraining-aucpr:0.12595\n",
      "[16:27:30] [11]\ttraining-aucpr:0.12688\n",
      "[16:27:30] [12]\ttraining-aucpr:0.12754\n",
      "[16:27:30] [13]\ttraining-aucpr:0.12963\n",
      "[16:27:30] [14]\ttraining-aucpr:0.13289\n",
      "[16:27:30] [15]\ttraining-aucpr:0.13466\n",
      "[16:27:30] [16]\ttraining-aucpr:0.13724\n",
      "[16:27:30] [17]\ttraining-aucpr:0.13910\n",
      "[16:27:30] [18]\ttraining-aucpr:0.14189\n",
      "[16:27:30] [19]\ttraining-aucpr:0.14467\n",
      "[16:27:30] [20]\ttraining-aucpr:0.14722\n",
      "[16:27:30] [21]\ttraining-aucpr:0.14916\n",
      "[16:27:30] [22]\ttraining-aucpr:0.14962\n",
      "[16:27:30] [23]\ttraining-aucpr:0.15077\n",
      "[16:27:30] [24]\ttraining-aucpr:0.15220\n",
      "[16:27:31] [25]\ttraining-aucpr:0.15399\n",
      "[16:27:31] [26]\ttraining-aucpr:0.15565\n",
      "[16:27:31] [27]\ttraining-aucpr:0.15754\n",
      "[16:27:31] [28]\ttraining-aucpr:0.15888\n",
      "[16:27:31] [29]\ttraining-aucpr:0.16061\n",
      "[16:27:31] [30]\ttraining-aucpr:0.16166\n",
      "[16:27:31] [31]\ttraining-aucpr:0.16338\n",
      "[16:27:31] [32]\ttraining-aucpr:0.16432\n",
      "[16:27:31] [33]\ttraining-aucpr:0.16566\n",
      "[16:27:31] [34]\ttraining-aucpr:0.16673\n",
      "[16:27:31] [35]\ttraining-aucpr:0.16839\n",
      "[16:27:31] [36]\ttraining-aucpr:0.17023\n",
      "[16:27:31] [37]\ttraining-aucpr:0.17144\n",
      "[16:27:31] [38]\ttraining-aucpr:0.17249\n",
      "[16:27:31] [39]\ttraining-aucpr:0.17394\n",
      "[16:27:31] [40]\ttraining-aucpr:0.17474\n",
      "[16:27:31] [41]\ttraining-aucpr:0.17661\n",
      "[16:27:31] [42]\ttraining-aucpr:0.17774\n",
      "[16:27:31] [43]\ttraining-aucpr:0.17840\n",
      "[16:27:31] [44]\ttraining-aucpr:0.17915\n",
      "[16:27:31] [45]\ttraining-aucpr:0.18110\n",
      "[16:27:31] [46]\ttraining-aucpr:0.18225\n",
      "[16:27:31] [47]\ttraining-aucpr:0.18285\n",
      "[16:27:31] [48]\ttraining-aucpr:0.18357\n",
      "[16:27:31] [49]\ttraining-aucpr:0.18441\n",
      "[16:27:31] [50]\ttraining-aucpr:0.18514\n",
      "[16:27:32] [51]\ttraining-aucpr:0.18571\n",
      "[16:27:32] [52]\ttraining-aucpr:0.18721\n",
      "[16:27:32] [53]\ttraining-aucpr:0.18795\n",
      "[16:27:32] [54]\ttraining-aucpr:0.18890\n",
      "[16:27:32] [55]\ttraining-aucpr:0.18989\n",
      "[16:27:32] [56]\ttraining-aucpr:0.19077\n",
      "[16:27:32] [57]\ttraining-aucpr:0.19185\n",
      "[16:27:32] [58]\ttraining-aucpr:0.19256\n",
      "[16:27:32] [59]\ttraining-aucpr:0.19309\n",
      "[16:27:32] [60]\ttraining-aucpr:0.19436\n",
      "2025-07-28 16:27:33,354 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:27:33,451 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:27:34,132] Trial 3 finished with value: 0.1740410309855796 and parameters: {'max_depth': 7, 'learning_rate': 0.07500118950416987, 'n_estimators': 61, 'min_child_weight': 7, 'gamma': 0.17052412368729153, 'subsample': 0.5325257964926398, 'colsample_bytree': 0.9744427686266666, 'reg_alpha': 0.9656320330745594, 'reg_lambda': 0.8083973481164611}. Best is trial 2 with value: 0.18255500709762018.\n",
      "2025-07-28 16:27:34,237 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.5171942605576092, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.12203823484477883, 'learning_rate': 0.013940346079873234, 'max_depth': 5, 'min_child_weight': 5, 'reg_alpha': 0.9093204020787821, 'reg_lambda': 0.2587799816000169, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.7475884550556351, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 221}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:27:35,701 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:27:36] Task 0 got rank 0\n",
      "[16:27:36] Task 1 got rank 1\n",
      "[16:27:36] Task 2 got rank 2\n",
      "[16:27:36] Task 3 got rank 3\n",
      "[16:27:37] [0]\ttraining-aucpr:0.06412\n",
      "[16:27:37] [1]\ttraining-aucpr:0.07662\n",
      "[16:27:37] [2]\ttraining-aucpr:0.08724\n",
      "[16:27:37] [3]\ttraining-aucpr:0.08640\n",
      "[16:27:37] [4]\ttraining-aucpr:0.08569\n",
      "[16:27:37] [5]\ttraining-aucpr:0.09038\n",
      "[16:27:37] [6]\ttraining-aucpr:0.09054\n",
      "[16:27:37] [7]\ttraining-aucpr:0.09026\n",
      "[16:27:37] [8]\ttraining-aucpr:0.09360\n",
      "[16:27:37] [9]\ttraining-aucpr:0.09367\n",
      "[16:27:37] [10]\ttraining-aucpr:0.10055\n",
      "[16:27:37] [11]\ttraining-aucpr:0.10018\n",
      "[16:27:37] [12]\ttraining-aucpr:0.09979\n",
      "[16:27:37] [13]\ttraining-aucpr:0.10219\n",
      "[16:27:37] [14]\ttraining-aucpr:0.10185\n",
      "[16:27:37] [15]\ttraining-aucpr:0.10140\n",
      "[16:27:37] [16]\ttraining-aucpr:0.10132\n",
      "[16:27:37] [17]\ttraining-aucpr:0.10262\n",
      "[16:27:37] [18]\ttraining-aucpr:0.10361\n",
      "[16:27:37] [19]\ttraining-aucpr:0.10546\n",
      "[16:27:37] [20]\ttraining-aucpr:0.10540\n",
      "[16:27:37] [21]\ttraining-aucpr:0.10691\n",
      "[16:27:37] [22]\ttraining-aucpr:0.10707\n",
      "[16:27:37] [23]\ttraining-aucpr:0.10654\n",
      "[16:27:38] [24]\ttraining-aucpr:0.10708\n",
      "[16:27:38] [25]\ttraining-aucpr:0.10904\n",
      "[16:27:38] [26]\ttraining-aucpr:0.10910\n",
      "[16:27:38] [27]\ttraining-aucpr:0.11025\n",
      "[16:27:38] [28]\ttraining-aucpr:0.11201\n",
      "[16:27:38] [29]\ttraining-aucpr:0.11277\n",
      "[16:27:38] [30]\ttraining-aucpr:0.11316\n",
      "[16:27:38] [31]\ttraining-aucpr:0.11270\n",
      "[16:27:38] [32]\ttraining-aucpr:0.11248\n",
      "[16:27:38] [33]\ttraining-aucpr:0.11305\n",
      "[16:27:38] [34]\ttraining-aucpr:0.11359\n",
      "[16:27:38] [35]\ttraining-aucpr:0.11385\n",
      "[16:27:38] [36]\ttraining-aucpr:0.11426\n",
      "[16:27:38] [37]\ttraining-aucpr:0.11495\n",
      "[16:27:38] [38]\ttraining-aucpr:0.11565\n",
      "[16:27:38] [39]\ttraining-aucpr:0.11588\n",
      "[16:27:38] [40]\ttraining-aucpr:0.11572\n",
      "[16:27:38] [41]\ttraining-aucpr:0.11565\n",
      "[16:27:38] [42]\ttraining-aucpr:0.11568\n",
      "[16:27:38] [43]\ttraining-aucpr:0.11625\n",
      "[16:27:38] [44]\ttraining-aucpr:0.11670\n",
      "[16:27:38] [45]\ttraining-aucpr:0.11698\n",
      "[16:27:38] [46]\ttraining-aucpr:0.11663\n",
      "[16:27:38] [47]\ttraining-aucpr:0.11672\n",
      "[16:27:38] [48]\ttraining-aucpr:0.11652\n",
      "[16:27:38] [49]\ttraining-aucpr:0.11694\n",
      "[16:27:38] [50]\ttraining-aucpr:0.11754\n",
      "[16:27:38] [51]\ttraining-aucpr:0.11761\n",
      "[16:27:38] [52]\ttraining-aucpr:0.11768\n",
      "[16:27:38] [53]\ttraining-aucpr:0.11794\n",
      "[16:27:38] [54]\ttraining-aucpr:0.11858\n",
      "[16:27:38] [55]\ttraining-aucpr:0.11895\n",
      "[16:27:39] [56]\ttraining-aucpr:0.11949\n",
      "[16:27:39] [57]\ttraining-aucpr:0.11954\n",
      "[16:27:39] [58]\ttraining-aucpr:0.11937\n",
      "[16:27:39] [59]\ttraining-aucpr:0.11924\n",
      "[16:27:39] [60]\ttraining-aucpr:0.11934\n",
      "[16:27:39] [61]\ttraining-aucpr:0.12003\n",
      "[16:27:39] [62]\ttraining-aucpr:0.12079\n",
      "[16:27:39] [63]\ttraining-aucpr:0.12119\n",
      "[16:27:39] [64]\ttraining-aucpr:0.12148\n",
      "[16:27:39] [65]\ttraining-aucpr:0.12176\n",
      "[16:27:39] [66]\ttraining-aucpr:0.12185\n",
      "[16:27:39] [67]\ttraining-aucpr:0.12227\n",
      "[16:27:39] [68]\ttraining-aucpr:0.12230\n",
      "[16:27:39] [69]\ttraining-aucpr:0.12237\n",
      "[16:27:39] [70]\ttraining-aucpr:0.12224\n",
      "[16:27:39] [71]\ttraining-aucpr:0.12224\n",
      "[16:27:39] [72]\ttraining-aucpr:0.12263\n",
      "[16:27:39] [73]\ttraining-aucpr:0.12336\n",
      "[16:27:39] [74]\ttraining-aucpr:0.12377\n",
      "[16:27:39] [75]\ttraining-aucpr:0.12388\n",
      "[16:27:39] [76]\ttraining-aucpr:0.12399\n",
      "[16:27:39] [77]\ttraining-aucpr:0.12385\n",
      "[16:27:39] [78]\ttraining-aucpr:0.12422\n",
      "[16:27:39] [79]\ttraining-aucpr:0.12437\n",
      "[16:27:39] [80]\ttraining-aucpr:0.12448\n",
      "[16:27:39] [81]\ttraining-aucpr:0.12458\n",
      "[16:27:39] [82]\ttraining-aucpr:0.12468\n",
      "[16:27:39] [83]\ttraining-aucpr:0.12503\n",
      "[16:27:39] [84]\ttraining-aucpr:0.12512\n",
      "[16:27:39] [85]\ttraining-aucpr:0.12547\n",
      "[16:27:39] [86]\ttraining-aucpr:0.12595\n",
      "[16:27:39] [87]\ttraining-aucpr:0.12648\n",
      "[16:27:39] [88]\ttraining-aucpr:0.12658\n",
      "[16:27:40] [89]\ttraining-aucpr:0.12662\n",
      "[16:27:40] [90]\ttraining-aucpr:0.12692\n",
      "[16:27:40] [91]\ttraining-aucpr:0.12699\n",
      "[16:27:40] [92]\ttraining-aucpr:0.12702\n",
      "[16:27:40] [93]\ttraining-aucpr:0.12694\n",
      "[16:27:40] [94]\ttraining-aucpr:0.12693\n",
      "[16:27:40] [95]\ttraining-aucpr:0.12722\n",
      "[16:27:40] [96]\ttraining-aucpr:0.12741\n",
      "[16:27:40] [97]\ttraining-aucpr:0.12780\n",
      "[16:27:40] [98]\ttraining-aucpr:0.12774\n",
      "[16:27:40] [99]\ttraining-aucpr:0.12817\n",
      "[16:27:40] [100]\ttraining-aucpr:0.12827\n",
      "[16:27:40] [101]\ttraining-aucpr:0.12858\n",
      "[16:27:40] [102]\ttraining-aucpr:0.12900\n",
      "[16:27:40] [103]\ttraining-aucpr:0.12912\n",
      "[16:27:40] [104]\ttraining-aucpr:0.12925\n",
      "[16:27:40] [105]\ttraining-aucpr:0.12916\n",
      "[16:27:40] [106]\ttraining-aucpr:0.12939\n",
      "[16:27:40] [107]\ttraining-aucpr:0.12993\n",
      "[16:27:40] [108]\ttraining-aucpr:0.13011\n",
      "[16:27:40] [109]\ttraining-aucpr:0.13021\n",
      "[16:27:40] [110]\ttraining-aucpr:0.13067\n",
      "[16:27:40] [111]\ttraining-aucpr:0.13078\n",
      "[16:27:40] [112]\ttraining-aucpr:0.13101\n",
      "[16:27:40] [113]\ttraining-aucpr:0.13094\n",
      "[16:27:40] [114]\ttraining-aucpr:0.13093\n",
      "[16:27:40] [115]\ttraining-aucpr:0.13122\n",
      "[16:27:40] [116]\ttraining-aucpr:0.13153\n",
      "[16:27:40] [117]\ttraining-aucpr:0.13166\n",
      "[16:27:40] [118]\ttraining-aucpr:0.13218\n",
      "[16:27:40] [119]\ttraining-aucpr:0.13235\n",
      "[16:27:40] [120]\ttraining-aucpr:0.13253\n",
      "[16:27:40] [121]\ttraining-aucpr:0.13283\n",
      "[16:27:41] [122]\ttraining-aucpr:0.13302\n",
      "[16:27:41] [123]\ttraining-aucpr:0.13326\n",
      "[16:27:41] [124]\ttraining-aucpr:0.13328\n",
      "[16:27:41] [125]\ttraining-aucpr:0.13366\n",
      "[16:27:41] [126]\ttraining-aucpr:0.13365\n",
      "[16:27:41] [127]\ttraining-aucpr:0.13373\n",
      "[16:27:41] [128]\ttraining-aucpr:0.13387\n",
      "[16:27:41] [129]\ttraining-aucpr:0.13404\n",
      "[16:27:41] [130]\ttraining-aucpr:0.13403\n",
      "[16:27:41] [131]\ttraining-aucpr:0.13435\n",
      "[16:27:41] [132]\ttraining-aucpr:0.13451\n",
      "[16:27:41] [133]\ttraining-aucpr:0.13449\n",
      "[16:27:41] [134]\ttraining-aucpr:0.13466\n",
      "[16:27:41] [135]\ttraining-aucpr:0.13484\n",
      "[16:27:41] [136]\ttraining-aucpr:0.13481\n",
      "[16:27:41] [137]\ttraining-aucpr:0.13504\n",
      "[16:27:41] [138]\ttraining-aucpr:0.13515\n",
      "[16:27:41] [139]\ttraining-aucpr:0.13522\n",
      "[16:27:41] [140]\ttraining-aucpr:0.13524\n",
      "[16:27:41] [141]\ttraining-aucpr:0.13541\n",
      "[16:27:41] [142]\ttraining-aucpr:0.13558\n",
      "[16:27:41] [143]\ttraining-aucpr:0.13567\n",
      "[16:27:41] [144]\ttraining-aucpr:0.13593\n",
      "[16:27:41] [145]\ttraining-aucpr:0.13604\n",
      "[16:27:41] [146]\ttraining-aucpr:0.13611\n",
      "[16:27:41] [147]\ttraining-aucpr:0.13621\n",
      "[16:27:41] [148]\ttraining-aucpr:0.13639\n",
      "[16:27:41] [149]\ttraining-aucpr:0.13640\n",
      "[16:27:41] [150]\ttraining-aucpr:0.13656\n",
      "[16:27:41] [151]\ttraining-aucpr:0.13679\n",
      "[16:27:41] [152]\ttraining-aucpr:0.13719\n",
      "[16:27:41] [153]\ttraining-aucpr:0.13757\n",
      "[16:27:41] [154]\ttraining-aucpr:0.13780\n",
      "[16:27:42] [155]\ttraining-aucpr:0.13812\n",
      "[16:27:42] [156]\ttraining-aucpr:0.13816\n",
      "[16:27:42] [157]\ttraining-aucpr:0.13851\n",
      "[16:27:42] [158]\ttraining-aucpr:0.13865\n",
      "[16:27:42] [159]\ttraining-aucpr:0.13870\n",
      "[16:27:42] [160]\ttraining-aucpr:0.13889\n",
      "[16:27:42] [161]\ttraining-aucpr:0.13905\n",
      "[16:27:42] [162]\ttraining-aucpr:0.13917\n",
      "[16:27:42] [163]\ttraining-aucpr:0.13931\n",
      "[16:27:42] [164]\ttraining-aucpr:0.13943\n",
      "[16:27:42] [165]\ttraining-aucpr:0.13959\n",
      "[16:27:42] [166]\ttraining-aucpr:0.13968\n",
      "[16:27:42] [167]\ttraining-aucpr:0.13970\n",
      "[16:27:42] [168]\ttraining-aucpr:0.13981\n",
      "[16:27:42] [169]\ttraining-aucpr:0.13997\n",
      "[16:27:42] [170]\ttraining-aucpr:0.13998\n",
      "[16:27:42] [171]\ttraining-aucpr:0.14015\n",
      "[16:27:42] [172]\ttraining-aucpr:0.14033\n",
      "[16:27:42] [173]\ttraining-aucpr:0.14059\n",
      "[16:27:42] [174]\ttraining-aucpr:0.14088\n",
      "[16:27:42] [175]\ttraining-aucpr:0.14101\n",
      "[16:27:42] [176]\ttraining-aucpr:0.14122\n",
      "[16:27:42] [177]\ttraining-aucpr:0.14134\n",
      "[16:27:42] [178]\ttraining-aucpr:0.14156\n",
      "[16:27:42] [179]\ttraining-aucpr:0.14196\n",
      "[16:27:42] [180]\ttraining-aucpr:0.14215\n",
      "[16:27:42] [181]\ttraining-aucpr:0.14246\n",
      "[16:27:42] [182]\ttraining-aucpr:0.14265\n",
      "[16:27:42] [183]\ttraining-aucpr:0.14277\n",
      "[16:27:42] [184]\ttraining-aucpr:0.14309\n",
      "[16:27:42] [185]\ttraining-aucpr:0.14313\n",
      "[16:27:42] [186]\ttraining-aucpr:0.14324\n",
      "[16:27:42] [187]\ttraining-aucpr:0.14357\n",
      "[16:27:42] [188]\ttraining-aucpr:0.14368\n",
      "[16:27:43] [189]\ttraining-aucpr:0.14381\n",
      "[16:27:43] [190]\ttraining-aucpr:0.14383\n",
      "[16:27:43] [191]\ttraining-aucpr:0.14395\n",
      "[16:27:43] [192]\ttraining-aucpr:0.14410\n",
      "[16:27:43] [193]\ttraining-aucpr:0.14421\n",
      "[16:27:43] [194]\ttraining-aucpr:0.14455\n",
      "[16:27:43] [195]\ttraining-aucpr:0.14456\n",
      "[16:27:43] [196]\ttraining-aucpr:0.14476\n",
      "[16:27:43] [197]\ttraining-aucpr:0.14479\n",
      "[16:27:43] [198]\ttraining-aucpr:0.14509\n",
      "[16:27:43] [199]\ttraining-aucpr:0.14536\n",
      "[16:27:43] [200]\ttraining-aucpr:0.14562\n",
      "[16:27:43] [201]\ttraining-aucpr:0.14581\n",
      "[16:27:43] [202]\ttraining-aucpr:0.14583\n",
      "[16:27:43] [203]\ttraining-aucpr:0.14589\n",
      "[16:27:43] [204]\ttraining-aucpr:0.14594\n",
      "[16:27:43] [205]\ttraining-aucpr:0.14642\n",
      "[16:27:43] [206]\ttraining-aucpr:0.14655\n",
      "[16:27:43] [207]\ttraining-aucpr:0.14670\n",
      "[16:27:43] [208]\ttraining-aucpr:0.14675\n",
      "[16:27:43] [209]\ttraining-aucpr:0.14686\n",
      "[16:27:43] [210]\ttraining-aucpr:0.14704\n",
      "[16:27:43] [211]\ttraining-aucpr:0.14719\n",
      "[16:27:43] [212]\ttraining-aucpr:0.14732\n",
      "[16:27:43] [213]\ttraining-aucpr:0.14758\n",
      "[16:27:43] [214]\ttraining-aucpr:0.14777\n",
      "[16:27:43] [215]\ttraining-aucpr:0.14780\n",
      "[16:27:43] [216]\ttraining-aucpr:0.14791\n",
      "[16:27:43] [217]\ttraining-aucpr:0.14802\n",
      "[16:27:43] [218]\ttraining-aucpr:0.14823\n",
      "[16:27:43] [219]\ttraining-aucpr:0.14833\n",
      "[16:27:43] [220]\ttraining-aucpr:0.14847\n",
      "2025-07-28 16:27:44,996 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:27:45,108 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:27:45,890] Trial 4 finished with value: 0.16860693287797848 and parameters: {'max_depth': 5, 'learning_rate': 0.013940346079873234, 'n_estimators': 221, 'min_child_weight': 5, 'gamma': 0.12203823484477883, 'subsample': 0.7475884550556351, 'colsample_bytree': 0.5171942605576092, 'reg_alpha': 0.9093204020787821, 'reg_lambda': 0.2587799816000169}. Best is trial 2 with value: 0.18255500709762018.\n",
      "2025-07-28 16:27:46,007 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.8875664116805573, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.18485445552552704, 'learning_rate': 0.028869220380495747, 'max_depth': 8, 'min_child_weight': 6, 'reg_alpha': 0.9394989415641891, 'reg_lambda': 0.8948273504276488, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9847923138822793, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 180}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:27:47,401 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:27:48] Task 0 got rank 0[16:27:48] Task 1 got rank 1\n",
      "\n",
      "[16:27:48] Task 3 got rank 3\n",
      "[16:27:48] Task 2 got rank 2\n",
      "[16:27:49] [0]\ttraining-aucpr:0.08716\n",
      "[16:27:49] [1]\ttraining-aucpr:0.10743\n",
      "[16:27:49] [2]\ttraining-aucpr:0.11540\n",
      "[16:27:49] [3]\ttraining-aucpr:0.11568\n",
      "[16:27:49] [4]\ttraining-aucpr:0.11718\n",
      "[16:27:49] [5]\ttraining-aucpr:0.11900\n",
      "[16:27:49] [6]\ttraining-aucpr:0.12436\n",
      "[16:27:49] [7]\ttraining-aucpr:0.12370\n",
      "[16:27:49] [8]\ttraining-aucpr:0.12458\n",
      "[16:27:49] [9]\ttraining-aucpr:0.12611\n",
      "[16:27:49] [10]\ttraining-aucpr:0.12663\n",
      "[16:27:49] [11]\ttraining-aucpr:0.12753\n",
      "[16:27:49] [12]\ttraining-aucpr:0.12778\n",
      "[16:27:49] [13]\ttraining-aucpr:0.13173\n",
      "[16:27:49] [14]\ttraining-aucpr:0.13275\n",
      "[16:27:49] [15]\ttraining-aucpr:0.13292\n",
      "[16:27:49] [16]\ttraining-aucpr:0.13420\n",
      "[16:27:49] [17]\ttraining-aucpr:0.13455\n",
      "[16:27:49] [18]\ttraining-aucpr:0.13589\n",
      "[16:27:49] [19]\ttraining-aucpr:0.13815\n",
      "[16:27:49] [20]\ttraining-aucpr:0.13850\n",
      "[16:27:50] [21]\ttraining-aucpr:0.13963\n",
      "[16:27:50] [22]\ttraining-aucpr:0.14011\n",
      "[16:27:50] [23]\ttraining-aucpr:0.14158\n",
      "[16:27:50] [24]\ttraining-aucpr:0.14260\n",
      "[16:27:50] [25]\ttraining-aucpr:0.14333\n",
      "[16:27:50] [26]\ttraining-aucpr:0.14406\n",
      "[16:27:50] [27]\ttraining-aucpr:0.14456\n",
      "[16:27:50] [28]\ttraining-aucpr:0.14557\n",
      "[16:27:50] [29]\ttraining-aucpr:0.14619\n",
      "[16:27:50] [30]\ttraining-aucpr:0.14670\n",
      "[16:27:50] [31]\ttraining-aucpr:0.14718\n",
      "[16:27:50] [32]\ttraining-aucpr:0.14887\n",
      "[16:27:50] [33]\ttraining-aucpr:0.14962\n",
      "[16:27:50] [34]\ttraining-aucpr:0.14988\n",
      "[16:27:50] [35]\ttraining-aucpr:0.15126\n",
      "[16:27:50] [36]\ttraining-aucpr:0.15298\n",
      "[16:27:50] [37]\ttraining-aucpr:0.15424\n",
      "[16:27:50] [38]\ttraining-aucpr:0.15539\n",
      "[16:27:50] [39]\ttraining-aucpr:0.15660\n",
      "[16:27:50] [40]\ttraining-aucpr:0.15793\n",
      "[16:27:50] [41]\ttraining-aucpr:0.15851\n",
      "[16:27:51] [42]\ttraining-aucpr:0.15914\n",
      "[16:27:51] [43]\ttraining-aucpr:0.16021\n",
      "[16:27:51] [44]\ttraining-aucpr:0.16093\n",
      "[16:27:51] [45]\ttraining-aucpr:0.16145\n",
      "[16:27:51] [46]\ttraining-aucpr:0.16339\n",
      "[16:27:51] [47]\ttraining-aucpr:0.16418\n",
      "[16:27:51] [48]\ttraining-aucpr:0.16584\n",
      "[16:27:51] [49]\ttraining-aucpr:0.16619\n",
      "[16:27:51] [50]\ttraining-aucpr:0.16774\n",
      "[16:27:51] [51]\ttraining-aucpr:0.16886\n",
      "[16:27:51] [52]\ttraining-aucpr:0.16967\n",
      "[16:27:51] [53]\ttraining-aucpr:0.17067\n",
      "[16:27:51] [54]\ttraining-aucpr:0.17209\n",
      "[16:27:51] [55]\ttraining-aucpr:0.17297\n",
      "[16:27:51] [56]\ttraining-aucpr:0.17380\n",
      "[16:27:51] [57]\ttraining-aucpr:0.17479\n",
      "[16:27:51] [58]\ttraining-aucpr:0.17640\n",
      "[16:27:51] [59]\ttraining-aucpr:0.17676\n",
      "[16:27:51] [60]\ttraining-aucpr:0.17779\n",
      "[16:27:51] [61]\ttraining-aucpr:0.17864\n",
      "[16:27:51] [62]\ttraining-aucpr:0.17977\n",
      "[16:27:52] [63]\ttraining-aucpr:0.18099\n",
      "[16:27:52] [64]\ttraining-aucpr:0.18189\n",
      "[16:27:52] [65]\ttraining-aucpr:0.18291\n",
      "[16:27:52] [66]\ttraining-aucpr:0.18382\n",
      "[16:27:52] [67]\ttraining-aucpr:0.18474\n",
      "[16:27:52] [68]\ttraining-aucpr:0.18555\n",
      "[16:27:52] [69]\ttraining-aucpr:0.18630\n",
      "[16:27:52] [70]\ttraining-aucpr:0.18694\n",
      "[16:27:52] [71]\ttraining-aucpr:0.18777\n",
      "[16:27:52] [72]\ttraining-aucpr:0.18898\n",
      "[16:27:52] [73]\ttraining-aucpr:0.19004\n",
      "[16:27:52] [74]\ttraining-aucpr:0.19131\n",
      "[16:27:52] [75]\ttraining-aucpr:0.19283\n",
      "[16:27:52] [76]\ttraining-aucpr:0.19414\n",
      "[16:27:52] [77]\ttraining-aucpr:0.19508\n",
      "[16:27:52] [78]\ttraining-aucpr:0.19613\n",
      "[16:27:52] [79]\ttraining-aucpr:0.19707\n",
      "[16:27:52] [80]\ttraining-aucpr:0.19778\n",
      "[16:27:52] [81]\ttraining-aucpr:0.19841\n",
      "[16:27:52] [82]\ttraining-aucpr:0.19955\n",
      "[16:27:52] [83]\ttraining-aucpr:0.20049\n",
      "[16:27:53] [84]\ttraining-aucpr:0.20130\n",
      "[16:27:53] [85]\ttraining-aucpr:0.20204\n",
      "[16:27:53] [86]\ttraining-aucpr:0.20296\n",
      "[16:27:53] [87]\ttraining-aucpr:0.20424\n",
      "[16:27:53] [88]\ttraining-aucpr:0.20495\n",
      "[16:27:53] [89]\ttraining-aucpr:0.20578\n",
      "[16:27:53] [90]\ttraining-aucpr:0.20675\n",
      "[16:27:53] [91]\ttraining-aucpr:0.20770\n",
      "[16:27:53] [92]\ttraining-aucpr:0.20832\n",
      "[16:27:53] [93]\ttraining-aucpr:0.20905\n",
      "[16:27:53] [94]\ttraining-aucpr:0.21005\n",
      "[16:27:53] [95]\ttraining-aucpr:0.21085\n",
      "[16:27:53] [96]\ttraining-aucpr:0.21158\n",
      "[16:27:53] [97]\ttraining-aucpr:0.21247\n",
      "[16:27:53] [98]\ttraining-aucpr:0.21334\n",
      "[16:27:53] [99]\ttraining-aucpr:0.21448\n",
      "[16:27:53] [100]\ttraining-aucpr:0.21554\n",
      "[16:27:53] [101]\ttraining-aucpr:0.21642\n",
      "[16:27:53] [102]\ttraining-aucpr:0.21705\n",
      "[16:27:53] [103]\ttraining-aucpr:0.21739\n",
      "[16:27:53] [104]\ttraining-aucpr:0.21768\n",
      "[16:27:53] [105]\ttraining-aucpr:0.21867\n",
      "[16:27:54] [106]\ttraining-aucpr:0.21937\n",
      "[16:27:54] [107]\ttraining-aucpr:0.22012\n",
      "[16:27:54] [108]\ttraining-aucpr:0.22130\n",
      "[16:27:54] [109]\ttraining-aucpr:0.22196\n",
      "[16:27:54] [110]\ttraining-aucpr:0.22302\n",
      "[16:27:54] [111]\ttraining-aucpr:0.22364\n",
      "[16:27:54] [112]\ttraining-aucpr:0.22455\n",
      "[16:27:54] [113]\ttraining-aucpr:0.22502\n",
      "[16:27:54] [114]\ttraining-aucpr:0.22603\n",
      "[16:27:54] [115]\ttraining-aucpr:0.22677\n",
      "[16:27:54] [116]\ttraining-aucpr:0.22739\n",
      "[16:27:54] [117]\ttraining-aucpr:0.22801\n",
      "[16:27:54] [118]\ttraining-aucpr:0.22902\n",
      "[16:27:54] [119]\ttraining-aucpr:0.22987\n",
      "[16:27:54] [120]\ttraining-aucpr:0.23076\n",
      "[16:27:54] [121]\ttraining-aucpr:0.23137\n",
      "[16:27:54] [122]\ttraining-aucpr:0.23231\n",
      "[16:27:54] [123]\ttraining-aucpr:0.23310\n",
      "[16:27:54] [124]\ttraining-aucpr:0.23384\n",
      "[16:27:54] [125]\ttraining-aucpr:0.23487\n",
      "[16:27:54] [126]\ttraining-aucpr:0.23578\n",
      "[16:27:54] [127]\ttraining-aucpr:0.23623\n",
      "[16:27:54] [128]\ttraining-aucpr:0.23677\n",
      "[16:27:55] [129]\ttraining-aucpr:0.23752\n",
      "[16:27:55] [130]\ttraining-aucpr:0.23828\n",
      "[16:27:55] [131]\ttraining-aucpr:0.23886\n",
      "[16:27:55] [132]\ttraining-aucpr:0.23981\n",
      "[16:27:55] [133]\ttraining-aucpr:0.24075\n",
      "[16:27:55] [134]\ttraining-aucpr:0.24130\n",
      "[16:27:55] [135]\ttraining-aucpr:0.24195\n",
      "[16:27:55] [136]\ttraining-aucpr:0.24288\n",
      "[16:27:55] [137]\ttraining-aucpr:0.24350\n",
      "[16:27:55] [138]\ttraining-aucpr:0.24434\n",
      "[16:27:55] [139]\ttraining-aucpr:0.24509\n",
      "[16:27:55] [140]\ttraining-aucpr:0.24536\n",
      "[16:27:55] [141]\ttraining-aucpr:0.24632\n",
      "[16:27:55] [142]\ttraining-aucpr:0.24717\n",
      "[16:27:55] [143]\ttraining-aucpr:0.24796\n",
      "[16:27:55] [144]\ttraining-aucpr:0.24881\n",
      "[16:27:55] [145]\ttraining-aucpr:0.24915\n",
      "[16:27:55] [146]\ttraining-aucpr:0.24973\n",
      "[16:27:55] [147]\ttraining-aucpr:0.25078\n",
      "[16:27:55] [148]\ttraining-aucpr:0.25122\n",
      "[16:27:55] [149]\ttraining-aucpr:0.25169\n",
      "[16:27:55] [150]\ttraining-aucpr:0.25247\n",
      "[16:27:55] [151]\ttraining-aucpr:0.25312\n",
      "[16:27:56] [152]\ttraining-aucpr:0.25386\n",
      "[16:27:56] [153]\ttraining-aucpr:0.25438\n",
      "[16:27:56] [154]\ttraining-aucpr:0.25517\n",
      "[16:27:56] [155]\ttraining-aucpr:0.25559\n",
      "[16:27:56] [156]\ttraining-aucpr:0.25651\n",
      "[16:27:56] [157]\ttraining-aucpr:0.25684\n",
      "[16:27:56] [158]\ttraining-aucpr:0.25771\n",
      "[16:27:56] [159]\ttraining-aucpr:0.25887\n",
      "[16:27:56] [160]\ttraining-aucpr:0.25958\n",
      "[16:27:56] [161]\ttraining-aucpr:0.25994\n",
      "[16:27:56] [162]\ttraining-aucpr:0.26030\n",
      "[16:27:56] [163]\ttraining-aucpr:0.26064\n",
      "[16:27:56] [164]\ttraining-aucpr:0.26130\n",
      "[16:27:56] [165]\ttraining-aucpr:0.26189\n",
      "[16:27:56] [166]\ttraining-aucpr:0.26219\n",
      "[16:27:56] [167]\ttraining-aucpr:0.26274\n",
      "[16:27:56] [168]\ttraining-aucpr:0.26323\n",
      "[16:27:56] [169]\ttraining-aucpr:0.26387\n",
      "[16:27:56] [170]\ttraining-aucpr:0.26464\n",
      "[16:27:56] [171]\ttraining-aucpr:0.26515\n",
      "[16:27:56] [172]\ttraining-aucpr:0.26545\n",
      "[16:27:56] [173]\ttraining-aucpr:0.26597\n",
      "[16:27:56] [174]\ttraining-aucpr:0.26670\n",
      "[16:27:56] [175]\ttraining-aucpr:0.26727\n",
      "[16:27:57] [176]\ttraining-aucpr:0.26782\n",
      "[16:27:57] [177]\ttraining-aucpr:0.26799\n",
      "[16:27:57] [178]\ttraining-aucpr:0.26812\n",
      "[16:27:57] [179]\ttraining-aucpr:0.26870\n",
      "2025-07-28 16:27:58,188 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:27:58,327 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:27:59,049] Trial 5 finished with value: 0.17038004142650792 and parameters: {'max_depth': 8, 'learning_rate': 0.028869220380495747, 'n_estimators': 180, 'min_child_weight': 6, 'gamma': 0.18485445552552704, 'subsample': 0.9847923138822793, 'colsample_bytree': 0.8875664116805573, 'reg_alpha': 0.9394989415641891, 'reg_lambda': 0.8948273504276488}. Best is trial 2 with value: 0.18255500709762018.\n",
      "2025-07-28 16:27:59,161 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.6943386448447411, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.045227288910538066, 'learning_rate': 0.22999586428143728, 'max_depth': 7, 'min_child_weight': 2, 'reg_alpha': 0.2713490317738959, 'reg_lambda': 0.8287375091519293, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.6626651653816322, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 72}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:28:00,555 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:28:01] Task 1 got rank 1\n",
      "[16:28:01] Task 0 got rank 0\n",
      "[16:28:01] Task 2 got rank 2\n",
      "[16:28:01] Task 3 got rank 3\n",
      "[16:28:02] [0]\ttraining-aucpr:0.07639\n",
      "[16:28:02] [1]\ttraining-aucpr:0.09835\n",
      "[16:28:02] [2]\ttraining-aucpr:0.10715\n",
      "[16:28:02] [3]\ttraining-aucpr:0.11491\n",
      "[16:28:02] [4]\ttraining-aucpr:0.12032\n",
      "[16:28:02] [5]\ttraining-aucpr:0.12722\n",
      "[16:28:02] [6]\ttraining-aucpr:0.13427\n",
      "[16:28:02] [7]\ttraining-aucpr:0.13801\n",
      "[16:28:02] [8]\ttraining-aucpr:0.14395\n",
      "[16:28:02] [9]\ttraining-aucpr:0.15037\n",
      "[16:28:02] [10]\ttraining-aucpr:0.15553\n",
      "[16:28:02] [11]\ttraining-aucpr:0.15912\n",
      "[16:28:02] [12]\ttraining-aucpr:0.16227\n",
      "[16:28:02] [13]\ttraining-aucpr:0.16668\n",
      "[16:28:02] [14]\ttraining-aucpr:0.16934\n",
      "[16:28:02] [15]\ttraining-aucpr:0.17383\n",
      "[16:28:02] [16]\ttraining-aucpr:0.17654\n",
      "[16:28:02] [17]\ttraining-aucpr:0.17872\n",
      "[16:28:02] [18]\ttraining-aucpr:0.18149\n",
      "[16:28:02] [19]\ttraining-aucpr:0.18246\n",
      "[16:28:02] [20]\ttraining-aucpr:0.18416\n",
      "[16:28:02] [21]\ttraining-aucpr:0.18724\n",
      "[16:28:03] [22]\ttraining-aucpr:0.19021\n",
      "[16:28:03] [23]\ttraining-aucpr:0.19196\n",
      "[16:28:03] [24]\ttraining-aucpr:0.19388\n",
      "[16:28:03] [25]\ttraining-aucpr:0.19546\n",
      "[16:28:03] [26]\ttraining-aucpr:0.19775\n",
      "[16:28:03] [27]\ttraining-aucpr:0.19922\n",
      "[16:28:03] [28]\ttraining-aucpr:0.20092\n",
      "[16:28:03] [29]\ttraining-aucpr:0.20237\n",
      "[16:28:03] [30]\ttraining-aucpr:0.20422\n",
      "[16:28:03] [31]\ttraining-aucpr:0.20562\n",
      "[16:28:03] [32]\ttraining-aucpr:0.20679\n",
      "[16:28:03] [33]\ttraining-aucpr:0.20773\n",
      "[16:28:03] [34]\ttraining-aucpr:0.20857\n",
      "[16:28:03] [35]\ttraining-aucpr:0.20975\n",
      "[16:28:03] [36]\ttraining-aucpr:0.21213\n",
      "[16:28:03] [37]\ttraining-aucpr:0.21476\n",
      "[16:28:03] [38]\ttraining-aucpr:0.21752\n",
      "[16:28:03] [39]\ttraining-aucpr:0.21956\n",
      "[16:28:03] [40]\ttraining-aucpr:0.22026\n",
      "[16:28:03] [41]\ttraining-aucpr:0.22119\n",
      "[16:28:03] [42]\ttraining-aucpr:0.22181\n",
      "[16:28:03] [43]\ttraining-aucpr:0.22280\n",
      "[16:28:03] [44]\ttraining-aucpr:0.22415\n",
      "[16:28:03] [45]\ttraining-aucpr:0.22544\n",
      "[16:28:03] [46]\ttraining-aucpr:0.22697\n",
      "[16:28:03] [47]\ttraining-aucpr:0.22777\n",
      "[16:28:03] [48]\ttraining-aucpr:0.22910\n",
      "[16:28:03] [49]\ttraining-aucpr:0.23046\n",
      "[16:28:03] [50]\ttraining-aucpr:0.23263\n",
      "[16:28:04] [51]\ttraining-aucpr:0.23438\n",
      "[16:28:04] [52]\ttraining-aucpr:0.23515\n",
      "[16:28:04] [53]\ttraining-aucpr:0.23631\n",
      "[16:28:04] [54]\ttraining-aucpr:0.23790\n",
      "[16:28:04] [55]\ttraining-aucpr:0.23981\n",
      "[16:28:04] [56]\ttraining-aucpr:0.24139\n",
      "[16:28:04] [57]\ttraining-aucpr:0.24245\n",
      "[16:28:04] [58]\ttraining-aucpr:0.24370\n",
      "[16:28:04] [59]\ttraining-aucpr:0.24545\n",
      "[16:28:04] [60]\ttraining-aucpr:0.24632\n",
      "[16:28:04] [61]\ttraining-aucpr:0.24780\n",
      "[16:28:04] [62]\ttraining-aucpr:0.24869\n",
      "[16:28:04] [63]\ttraining-aucpr:0.24961\n",
      "[16:28:04] [64]\ttraining-aucpr:0.25044\n",
      "[16:28:04] [65]\ttraining-aucpr:0.25166\n",
      "[16:28:04] [66]\ttraining-aucpr:0.25184\n",
      "[16:28:04] [67]\ttraining-aucpr:0.25273\n",
      "[16:28:04] [68]\ttraining-aucpr:0.25411\n",
      "[16:28:04] [69]\ttraining-aucpr:0.25493\n",
      "[16:28:04] [70]\ttraining-aucpr:0.25613\n",
      "[16:28:04] [71]\ttraining-aucpr:0.25747\n",
      "2025-07-28 16:28:05,735 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:28:05,845 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:28:06,667] Trial 6 finished with value: 0.14815983728511323 and parameters: {'max_depth': 7, 'learning_rate': 0.22999586428143728, 'n_estimators': 72, 'min_child_weight': 2, 'gamma': 0.045227288910538066, 'subsample': 0.6626651653816322, 'colsample_bytree': 0.6943386448447411, 'reg_alpha': 0.2713490317738959, 'reg_lambda': 0.8287375091519293}. Best is trial 2 with value: 0.18255500709762018.\n",
      "2025-07-28 16:28:06,796 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.9934434683002586, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.8021969807540397, 'learning_rate': 0.026000059117302653, 'max_depth': 5, 'min_child_weight': 2, 'reg_alpha': 0.7722447692966574, 'reg_lambda': 0.1987156815341724, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.5372753218398854, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 186}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:28:08,173 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:28:09] Task 0 got rank 0\n",
      "[16:28:09] Task 2 got rank 2\n",
      "[16:28:09] Task 1 got rank 1\n",
      "[16:28:09] Task 3 got rank 3\n",
      "[16:28:09] [0]\ttraining-aucpr:0.06041\n",
      "[16:28:09] [1]\ttraining-aucpr:0.07407\n",
      "[16:28:09] [2]\ttraining-aucpr:0.08100\n",
      "[16:28:09] [3]\ttraining-aucpr:0.08124\n",
      "[16:28:09] [4]\ttraining-aucpr:0.08130\n",
      "[16:28:09] [5]\ttraining-aucpr:0.08219\n",
      "[16:28:09] [6]\ttraining-aucpr:0.08640\n",
      "[16:28:09] [7]\ttraining-aucpr:0.08734\n",
      "[16:28:09] [8]\ttraining-aucpr:0.08770\n",
      "[16:28:09] [9]\ttraining-aucpr:0.08779\n",
      "[16:28:10] [10]\ttraining-aucpr:0.08828\n",
      "[16:28:10] [11]\ttraining-aucpr:0.08853\n",
      "[16:28:10] [12]\ttraining-aucpr:0.08877\n",
      "[16:28:10] [13]\ttraining-aucpr:0.09032\n",
      "[16:28:10] [14]\ttraining-aucpr:0.09099\n",
      "[16:28:10] [15]\ttraining-aucpr:0.09097\n",
      "[16:28:10] [16]\ttraining-aucpr:0.09256\n",
      "[16:28:10] [17]\ttraining-aucpr:0.09237\n",
      "[16:28:10] [18]\ttraining-aucpr:0.09482\n",
      "[16:28:10] [19]\ttraining-aucpr:0.09584\n",
      "[16:28:10] [20]\ttraining-aucpr:0.09705\n",
      "[16:28:10] [21]\ttraining-aucpr:0.09802\n",
      "[16:28:10] [22]\ttraining-aucpr:0.09806\n",
      "[16:28:10] [23]\ttraining-aucpr:0.09890\n",
      "[16:28:10] [24]\ttraining-aucpr:0.09909\n",
      "[16:28:10] [25]\ttraining-aucpr:0.09996\n",
      "[16:28:10] [26]\ttraining-aucpr:0.10017\n",
      "[16:28:10] [27]\ttraining-aucpr:0.10080\n",
      "[16:28:10] [28]\ttraining-aucpr:0.10107\n",
      "[16:28:10] [29]\ttraining-aucpr:0.10239\n",
      "[16:28:10] [30]\ttraining-aucpr:0.10281\n",
      "[16:28:10] [31]\ttraining-aucpr:0.10329\n",
      "[16:28:10] [32]\ttraining-aucpr:0.10468\n",
      "[16:28:10] [33]\ttraining-aucpr:0.10541\n",
      "[16:28:10] [34]\ttraining-aucpr:0.10634\n",
      "[16:28:10] [35]\ttraining-aucpr:0.10663\n",
      "[16:28:10] [36]\ttraining-aucpr:0.10739\n",
      "[16:28:10] [37]\ttraining-aucpr:0.10823\n",
      "[16:28:10] [38]\ttraining-aucpr:0.10850\n",
      "[16:28:10] [39]\ttraining-aucpr:0.10888\n",
      "[16:28:10] [40]\ttraining-aucpr:0.10910\n",
      "[16:28:10] [41]\ttraining-aucpr:0.11026\n",
      "[16:28:10] [42]\ttraining-aucpr:0.11084\n",
      "[16:28:11] [43]\ttraining-aucpr:0.11079\n",
      "[16:28:11] [44]\ttraining-aucpr:0.11100\n",
      "[16:28:11] [45]\ttraining-aucpr:0.11165\n",
      "[16:28:11] [46]\ttraining-aucpr:0.11320\n",
      "[16:28:11] [47]\ttraining-aucpr:0.11388\n",
      "[16:28:11] [48]\ttraining-aucpr:0.11415\n",
      "[16:28:11] [49]\ttraining-aucpr:0.11515\n",
      "[16:28:11] [50]\ttraining-aucpr:0.11553\n",
      "[16:28:11] [51]\ttraining-aucpr:0.11646\n",
      "[16:28:11] [52]\ttraining-aucpr:0.11730\n",
      "[16:28:11] [53]\ttraining-aucpr:0.11795\n",
      "[16:28:11] [54]\ttraining-aucpr:0.11866\n",
      "[16:28:11] [55]\ttraining-aucpr:0.11970\n",
      "[16:28:11] [56]\ttraining-aucpr:0.11984\n",
      "[16:28:11] [57]\ttraining-aucpr:0.12111\n",
      "[16:28:11] [58]\ttraining-aucpr:0.12189\n",
      "[16:28:11] [59]\ttraining-aucpr:0.12306\n",
      "[16:28:11] [60]\ttraining-aucpr:0.12352\n",
      "[16:28:11] [61]\ttraining-aucpr:0.12435\n",
      "[16:28:11] [62]\ttraining-aucpr:0.12565\n",
      "[16:28:11] [63]\ttraining-aucpr:0.12597\n",
      "[16:28:11] [64]\ttraining-aucpr:0.12639\n",
      "[16:28:11] [65]\ttraining-aucpr:0.12675\n",
      "[16:28:11] [66]\ttraining-aucpr:0.12729\n",
      "[16:28:11] [67]\ttraining-aucpr:0.12737\n",
      "[16:28:11] [68]\ttraining-aucpr:0.12769\n",
      "[16:28:11] [69]\ttraining-aucpr:0.12793\n",
      "[16:28:11] [70]\ttraining-aucpr:0.12854\n",
      "[16:28:11] [71]\ttraining-aucpr:0.12903\n",
      "[16:28:11] [72]\ttraining-aucpr:0.12945\n",
      "[16:28:11] [73]\ttraining-aucpr:0.12984\n",
      "[16:28:12] [74]\ttraining-aucpr:0.13082\n",
      "[16:28:12] [75]\ttraining-aucpr:0.13128\n",
      "[16:28:12] [76]\ttraining-aucpr:0.13228\n",
      "[16:28:12] [77]\ttraining-aucpr:0.13261\n",
      "[16:28:12] [78]\ttraining-aucpr:0.13329\n",
      "[16:28:12] [79]\ttraining-aucpr:0.13407\n",
      "[16:28:12] [80]\ttraining-aucpr:0.13469\n",
      "[16:28:12] [81]\ttraining-aucpr:0.13502\n",
      "[16:28:12] [82]\ttraining-aucpr:0.13552\n",
      "[16:28:12] [83]\ttraining-aucpr:0.13632\n",
      "[16:28:12] [84]\ttraining-aucpr:0.13640\n",
      "[16:28:12] [85]\ttraining-aucpr:0.13710\n",
      "[16:28:12] [86]\ttraining-aucpr:0.13720\n",
      "[16:28:12] [87]\ttraining-aucpr:0.13720\n",
      "[16:28:12] [88]\ttraining-aucpr:0.13777\n",
      "[16:28:12] [89]\ttraining-aucpr:0.13859\n",
      "[16:28:12] [90]\ttraining-aucpr:0.13866\n",
      "[16:28:12] [91]\ttraining-aucpr:0.13908\n",
      "[16:28:12] [92]\ttraining-aucpr:0.13946\n",
      "[16:28:12] [93]\ttraining-aucpr:0.13975\n",
      "[16:28:12] [94]\ttraining-aucpr:0.14036\n",
      "[16:28:12] [95]\ttraining-aucpr:0.14111\n",
      "[16:28:12] [96]\ttraining-aucpr:0.14183\n",
      "[16:28:12] [97]\ttraining-aucpr:0.14233\n",
      "[16:28:12] [98]\ttraining-aucpr:0.14259\n",
      "[16:28:12] [99]\ttraining-aucpr:0.14283\n",
      "[16:28:12] [100]\ttraining-aucpr:0.14335\n",
      "[16:28:12] [101]\ttraining-aucpr:0.14402\n",
      "[16:28:12] [102]\ttraining-aucpr:0.14426\n",
      "[16:28:12] [103]\ttraining-aucpr:0.14461\n",
      "[16:28:12] [104]\ttraining-aucpr:0.14501\n",
      "[16:28:13] [105]\ttraining-aucpr:0.14548\n",
      "[16:28:13] [106]\ttraining-aucpr:0.14582\n",
      "[16:28:13] [107]\ttraining-aucpr:0.14638\n",
      "[16:28:13] [108]\ttraining-aucpr:0.14674\n",
      "[16:28:13] [109]\ttraining-aucpr:0.14700\n",
      "[16:28:13] [110]\ttraining-aucpr:0.14721\n",
      "[16:28:13] [111]\ttraining-aucpr:0.14763\n",
      "[16:28:13] [112]\ttraining-aucpr:0.14816\n",
      "[16:28:13] [113]\ttraining-aucpr:0.14848\n",
      "[16:28:13] [114]\ttraining-aucpr:0.14855\n",
      "[16:28:13] [115]\ttraining-aucpr:0.14879\n",
      "[16:28:13] [116]\ttraining-aucpr:0.14889\n",
      "[16:28:13] [117]\ttraining-aucpr:0.14921\n",
      "[16:28:13] [118]\ttraining-aucpr:0.14970\n",
      "[16:28:13] [119]\ttraining-aucpr:0.15018\n",
      "[16:28:13] [120]\ttraining-aucpr:0.15038\n",
      "[16:28:13] [121]\ttraining-aucpr:0.15056\n",
      "[16:28:13] [122]\ttraining-aucpr:0.15108\n",
      "[16:28:13] [123]\ttraining-aucpr:0.15122\n",
      "[16:28:13] [124]\ttraining-aucpr:0.15146\n",
      "[16:28:13] [125]\ttraining-aucpr:0.15188\n",
      "[16:28:13] [126]\ttraining-aucpr:0.15215\n",
      "[16:28:13] [127]\ttraining-aucpr:0.15232\n",
      "[16:28:13] [128]\ttraining-aucpr:0.15263\n",
      "[16:28:13] [129]\ttraining-aucpr:0.15293\n",
      "[16:28:13] [130]\ttraining-aucpr:0.15307\n",
      "[16:28:13] [131]\ttraining-aucpr:0.15316\n",
      "[16:28:13] [132]\ttraining-aucpr:0.15350\n",
      "[16:28:13] [133]\ttraining-aucpr:0.15404\n",
      "[16:28:13] [134]\ttraining-aucpr:0.15429\n",
      "[16:28:13] [135]\ttraining-aucpr:0.15458\n",
      "[16:28:13] [136]\ttraining-aucpr:0.15456\n",
      "[16:28:14] [137]\ttraining-aucpr:0.15492\n",
      "[16:28:14] [138]\ttraining-aucpr:0.15523\n",
      "[16:28:14] [139]\ttraining-aucpr:0.15545\n",
      "[16:28:14] [140]\ttraining-aucpr:0.15587\n",
      "[16:28:14] [141]\ttraining-aucpr:0.15596\n",
      "[16:28:14] [142]\ttraining-aucpr:0.15652\n",
      "[16:28:14] [143]\ttraining-aucpr:0.15673\n",
      "[16:28:14] [144]\ttraining-aucpr:0.15713\n",
      "[16:28:14] [145]\ttraining-aucpr:0.15735\n",
      "[16:28:14] [146]\ttraining-aucpr:0.15772\n",
      "[16:28:14] [147]\ttraining-aucpr:0.15782\n",
      "[16:28:14] [148]\ttraining-aucpr:0.15826\n",
      "[16:28:14] [149]\ttraining-aucpr:0.15852\n",
      "[16:28:14] [150]\ttraining-aucpr:0.15875\n",
      "[16:28:14] [151]\ttraining-aucpr:0.15897\n",
      "[16:28:14] [152]\ttraining-aucpr:0.15924\n",
      "[16:28:14] [153]\ttraining-aucpr:0.15947\n",
      "[16:28:14] [154]\ttraining-aucpr:0.15965\n",
      "[16:28:14] [155]\ttraining-aucpr:0.16010\n",
      "[16:28:14] [156]\ttraining-aucpr:0.16045\n",
      "[16:28:14] [157]\ttraining-aucpr:0.16066\n",
      "[16:28:14] [158]\ttraining-aucpr:0.16092\n",
      "[16:28:14] [159]\ttraining-aucpr:0.16114\n",
      "[16:28:14] [160]\ttraining-aucpr:0.16141\n",
      "[16:28:14] [161]\ttraining-aucpr:0.16171\n",
      "[16:28:14] [162]\ttraining-aucpr:0.16183\n",
      "[16:28:14] [163]\ttraining-aucpr:0.16198\n",
      "[16:28:14] [164]\ttraining-aucpr:0.16230\n",
      "[16:28:14] [165]\ttraining-aucpr:0.16257\n",
      "[16:28:14] [166]\ttraining-aucpr:0.16261\n",
      "[16:28:14] [167]\ttraining-aucpr:0.16299\n",
      "[16:28:14] [168]\ttraining-aucpr:0.16336\n",
      "[16:28:15] [169]\ttraining-aucpr:0.16362\n",
      "[16:28:15] [170]\ttraining-aucpr:0.16377\n",
      "[16:28:15] [171]\ttraining-aucpr:0.16404\n",
      "[16:28:15] [172]\ttraining-aucpr:0.16429\n",
      "[16:28:15] [173]\ttraining-aucpr:0.16456\n",
      "[16:28:15] [174]\ttraining-aucpr:0.16478\n",
      "[16:28:15] [175]\ttraining-aucpr:0.16513\n",
      "[16:28:15] [176]\ttraining-aucpr:0.16565\n",
      "[16:28:15] [177]\ttraining-aucpr:0.16572\n",
      "[16:28:15] [178]\ttraining-aucpr:0.16584\n",
      "[16:28:15] [179]\ttraining-aucpr:0.16606\n",
      "[16:28:15] [180]\ttraining-aucpr:0.16633\n",
      "[16:28:15] [181]\ttraining-aucpr:0.16649\n",
      "[16:28:15] [182]\ttraining-aucpr:0.16668\n",
      "[16:28:15] [183]\ttraining-aucpr:0.16721\n",
      "[16:28:15] [184]\ttraining-aucpr:0.16749\n",
      "[16:28:15] [185]\ttraining-aucpr:0.16776\n",
      "2025-07-28 16:28:16,497 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:28:16,600 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:28:17,321] Trial 7 finished with value: 0.17754464817461704 and parameters: {'max_depth': 5, 'learning_rate': 0.026000059117302653, 'n_estimators': 186, 'min_child_weight': 2, 'gamma': 0.8021969807540397, 'subsample': 0.5372753218398854, 'colsample_bytree': 0.9934434683002586, 'reg_alpha': 0.7722447692966574, 'reg_lambda': 0.1987156815341724}. Best is trial 2 with value: 0.18255500709762018.\n",
      "2025-07-28 16:28:17,442 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.6792328642721364, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.7712703466859457, 'learning_rate': 0.1601531217136121, 'max_depth': 3, 'min_child_weight': 8, 'reg_alpha': 0.11586905952512971, 'reg_lambda': 0.8631034258755935, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.5370223258670452, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 227}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:28:18,831 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:28:19] Task 1 got rank 1\n",
      "[16:28:19] Task 0 got rank 0\n",
      "[16:28:19] Task 2 got rank 2\n",
      "[16:28:19] Task 3 got rank 3\n",
      "[16:28:20] [0]\ttraining-aucpr:0.04719\n",
      "[16:28:20] [1]\ttraining-aucpr:0.04830\n",
      "[16:28:20] [2]\ttraining-aucpr:0.05865\n",
      "[16:28:20] [3]\ttraining-aucpr:0.05932\n",
      "[16:28:20] [4]\ttraining-aucpr:0.06545\n",
      "[16:28:20] [5]\ttraining-aucpr:0.07280\n",
      "[16:28:20] [6]\ttraining-aucpr:0.08440\n",
      "[16:28:20] [7]\ttraining-aucpr:0.08856\n",
      "[16:28:20] [8]\ttraining-aucpr:0.09081\n",
      "[16:28:20] [9]\ttraining-aucpr:0.09661\n",
      "[16:28:20] [10]\ttraining-aucpr:0.10304\n",
      "[16:28:20] [11]\ttraining-aucpr:0.10388\n",
      "[16:28:20] [12]\ttraining-aucpr:0.10766\n",
      "[16:28:20] [13]\ttraining-aucpr:0.10898\n",
      "[16:28:20] [14]\ttraining-aucpr:0.11052\n",
      "[16:28:20] [15]\ttraining-aucpr:0.11268\n",
      "[16:28:20] [16]\ttraining-aucpr:0.11397\n",
      "[16:28:20] [17]\ttraining-aucpr:0.11757\n",
      "[16:28:20] [18]\ttraining-aucpr:0.11918\n",
      "[16:28:20] [19]\ttraining-aucpr:0.12181\n",
      "[16:28:20] [20]\ttraining-aucpr:0.12280\n",
      "[16:28:20] [21]\ttraining-aucpr:0.12590\n",
      "[16:28:20] [22]\ttraining-aucpr:0.12652\n",
      "[16:28:20] [23]\ttraining-aucpr:0.12674\n",
      "[16:28:20] [24]\ttraining-aucpr:0.12818\n",
      "[16:28:20] [25]\ttraining-aucpr:0.12853\n",
      "[16:28:21] [26]\ttraining-aucpr:0.12962\n",
      "[16:28:21] [27]\ttraining-aucpr:0.13266\n",
      "[16:28:21] [28]\ttraining-aucpr:0.13544\n",
      "[16:28:21] [29]\ttraining-aucpr:0.13612\n",
      "[16:28:21] [30]\ttraining-aucpr:0.13731\n",
      "[16:28:21] [31]\ttraining-aucpr:0.13765\n",
      "[16:28:21] [32]\ttraining-aucpr:0.13863\n",
      "[16:28:21] [33]\ttraining-aucpr:0.14104\n",
      "[16:28:21] [34]\ttraining-aucpr:0.14136\n",
      "[16:28:21] [35]\ttraining-aucpr:0.14227\n",
      "[16:28:21] [36]\ttraining-aucpr:0.14308\n",
      "[16:28:21] [37]\ttraining-aucpr:0.14459\n",
      "[16:28:21] [38]\ttraining-aucpr:0.14471\n",
      "[16:28:21] [39]\ttraining-aucpr:0.14571\n",
      "[16:28:21] [40]\ttraining-aucpr:0.14619\n",
      "[16:28:21] [41]\ttraining-aucpr:0.14648\n",
      "[16:28:21] [42]\ttraining-aucpr:0.14738\n",
      "[16:28:21] [43]\ttraining-aucpr:0.14901\n",
      "[16:28:21] [44]\ttraining-aucpr:0.15026\n",
      "[16:28:21] [45]\ttraining-aucpr:0.15046\n",
      "[16:28:21] [46]\ttraining-aucpr:0.15067\n",
      "[16:28:21] [47]\ttraining-aucpr:0.15141\n",
      "[16:28:21] [48]\ttraining-aucpr:0.15211\n",
      "[16:28:21] [49]\ttraining-aucpr:0.15259\n",
      "[16:28:21] [50]\ttraining-aucpr:0.15341\n",
      "[16:28:21] [51]\ttraining-aucpr:0.15386\n",
      "[16:28:21] [52]\ttraining-aucpr:0.15400\n",
      "[16:28:21] [53]\ttraining-aucpr:0.15455\n",
      "[16:28:21] [54]\ttraining-aucpr:0.15494\n",
      "[16:28:21] [55]\ttraining-aucpr:0.15520\n",
      "[16:28:21] [56]\ttraining-aucpr:0.15549\n",
      "[16:28:21] [57]\ttraining-aucpr:0.15645\n",
      "[16:28:21] [58]\ttraining-aucpr:0.15713\n",
      "[16:28:21] [59]\ttraining-aucpr:0.15699\n",
      "[16:28:21] [60]\ttraining-aucpr:0.15830\n",
      "[16:28:21] [61]\ttraining-aucpr:0.15845\n",
      "[16:28:21] [62]\ttraining-aucpr:0.15893\n",
      "[16:28:21] [63]\ttraining-aucpr:0.15897\n",
      "[16:28:21] [64]\ttraining-aucpr:0.15947\n",
      "[16:28:21] [65]\ttraining-aucpr:0.15993\n",
      "[16:28:22] [66]\ttraining-aucpr:0.16034\n",
      "[16:28:22] [67]\ttraining-aucpr:0.16058\n",
      "[16:28:22] [68]\ttraining-aucpr:0.16094\n",
      "[16:28:22] [69]\ttraining-aucpr:0.16133\n",
      "[16:28:22] [70]\ttraining-aucpr:0.16160\n",
      "[16:28:22] [71]\ttraining-aucpr:0.16186\n",
      "[16:28:22] [72]\ttraining-aucpr:0.16212\n",
      "[16:28:22] [73]\ttraining-aucpr:0.16223\n",
      "[16:28:22] [74]\ttraining-aucpr:0.16243\n",
      "[16:28:22] [75]\ttraining-aucpr:0.16266\n",
      "[16:28:22] [76]\ttraining-aucpr:0.16319\n",
      "[16:28:22] [77]\ttraining-aucpr:0.16350\n",
      "[16:28:22] [78]\ttraining-aucpr:0.16378\n",
      "[16:28:22] [79]\ttraining-aucpr:0.16452\n",
      "[16:28:22] [80]\ttraining-aucpr:0.16485\n",
      "[16:28:22] [81]\ttraining-aucpr:0.16532\n",
      "[16:28:22] [82]\ttraining-aucpr:0.16550\n",
      "[16:28:22] [83]\ttraining-aucpr:0.16562\n",
      "[16:28:22] [84]\ttraining-aucpr:0.16599\n",
      "[16:28:22] [85]\ttraining-aucpr:0.16631\n",
      "[16:28:22] [86]\ttraining-aucpr:0.16657\n",
      "[16:28:22] [87]\ttraining-aucpr:0.16680\n",
      "[16:28:22] [88]\ttraining-aucpr:0.16702\n",
      "[16:28:22] [89]\ttraining-aucpr:0.16750\n",
      "[16:28:22] [90]\ttraining-aucpr:0.16773\n",
      "[16:28:22] [91]\ttraining-aucpr:0.16785\n",
      "[16:28:22] [92]\ttraining-aucpr:0.16784\n",
      "[16:28:22] [93]\ttraining-aucpr:0.16809\n",
      "[16:28:22] [94]\ttraining-aucpr:0.16793\n",
      "[16:28:22] [95]\ttraining-aucpr:0.16792\n",
      "[16:28:22] [96]\ttraining-aucpr:0.16807\n",
      "[16:28:22] [97]\ttraining-aucpr:0.16837\n",
      "[16:28:22] [98]\ttraining-aucpr:0.16846\n",
      "[16:28:22] [99]\ttraining-aucpr:0.16860\n",
      "[16:28:22] [100]\ttraining-aucpr:0.16862\n",
      "[16:28:22] [101]\ttraining-aucpr:0.16872\n",
      "[16:28:22] [102]\ttraining-aucpr:0.16870\n",
      "[16:28:22] [103]\ttraining-aucpr:0.16876\n",
      "[16:28:22] [104]\ttraining-aucpr:0.16895\n",
      "[16:28:22] [105]\ttraining-aucpr:0.16916\n",
      "[16:28:22] [106]\ttraining-aucpr:0.16926\n",
      "[16:28:22] [107]\ttraining-aucpr:0.16933\n",
      "[16:28:23] [108]\ttraining-aucpr:0.16956\n",
      "[16:28:23] [109]\ttraining-aucpr:0.16967\n",
      "[16:28:23] [110]\ttraining-aucpr:0.16996\n",
      "[16:28:23] [111]\ttraining-aucpr:0.17007\n",
      "[16:28:23] [112]\ttraining-aucpr:0.17011\n",
      "[16:28:23] [113]\ttraining-aucpr:0.17008\n",
      "[16:28:23] [114]\ttraining-aucpr:0.17016\n",
      "[16:28:23] [115]\ttraining-aucpr:0.17030\n",
      "[16:28:23] [116]\ttraining-aucpr:0.17034\n",
      "[16:28:23] [117]\ttraining-aucpr:0.17053\n",
      "[16:28:23] [118]\ttraining-aucpr:0.17079\n",
      "[16:28:23] [119]\ttraining-aucpr:0.17091\n",
      "[16:28:23] [120]\ttraining-aucpr:0.17094\n",
      "[16:28:23] [121]\ttraining-aucpr:0.17104\n",
      "[16:28:23] [122]\ttraining-aucpr:0.17109\n",
      "[16:28:23] [123]\ttraining-aucpr:0.17112\n",
      "[16:28:23] [124]\ttraining-aucpr:0.17124\n",
      "[16:28:23] [125]\ttraining-aucpr:0.17159\n",
      "[16:28:23] [126]\ttraining-aucpr:0.17157\n",
      "[16:28:23] [127]\ttraining-aucpr:0.17164\n",
      "[16:28:23] [128]\ttraining-aucpr:0.17174\n",
      "[16:28:23] [129]\ttraining-aucpr:0.17176\n",
      "[16:28:23] [130]\ttraining-aucpr:0.17195\n",
      "[16:28:23] [131]\ttraining-aucpr:0.17202\n",
      "[16:28:23] [132]\ttraining-aucpr:0.17214\n",
      "[16:28:23] [133]\ttraining-aucpr:0.17236\n",
      "[16:28:23] [134]\ttraining-aucpr:0.17244\n",
      "[16:28:23] [135]\ttraining-aucpr:0.17248\n",
      "[16:28:23] [136]\ttraining-aucpr:0.17253\n",
      "[16:28:23] [137]\ttraining-aucpr:0.17257\n",
      "[16:28:23] [138]\ttraining-aucpr:0.17258\n",
      "[16:28:23] [139]\ttraining-aucpr:0.17255\n",
      "[16:28:23] [140]\ttraining-aucpr:0.17257\n",
      "[16:28:23] [141]\ttraining-aucpr:0.17258\n",
      "[16:28:23] [142]\ttraining-aucpr:0.17266\n",
      "[16:28:23] [143]\ttraining-aucpr:0.17286\n",
      "[16:28:23] [144]\ttraining-aucpr:0.17297\n",
      "[16:28:23] [145]\ttraining-aucpr:0.17305\n",
      "[16:28:23] [146]\ttraining-aucpr:0.17294\n",
      "[16:28:23] [147]\ttraining-aucpr:0.17296\n",
      "[16:28:24] [148]\ttraining-aucpr:0.17306\n",
      "[16:28:24] [149]\ttraining-aucpr:0.17312\n",
      "[16:28:24] [150]\ttraining-aucpr:0.17315\n",
      "[16:28:24] [151]\ttraining-aucpr:0.17320\n",
      "[16:28:24] [152]\ttraining-aucpr:0.17323\n",
      "[16:28:24] [153]\ttraining-aucpr:0.17337\n",
      "[16:28:24] [154]\ttraining-aucpr:0.17338\n",
      "[16:28:24] [155]\ttraining-aucpr:0.17361\n",
      "[16:28:24] [156]\ttraining-aucpr:0.17363\n",
      "[16:28:24] [157]\ttraining-aucpr:0.17380\n",
      "[16:28:24] [158]\ttraining-aucpr:0.17385\n",
      "[16:28:24] [159]\ttraining-aucpr:0.17392\n",
      "[16:28:24] [160]\ttraining-aucpr:0.17402\n",
      "[16:28:24] [161]\ttraining-aucpr:0.17408\n",
      "[16:28:24] [162]\ttraining-aucpr:0.17410\n",
      "[16:28:24] [163]\ttraining-aucpr:0.17408\n",
      "[16:28:24] [164]\ttraining-aucpr:0.17411\n",
      "[16:28:24] [165]\ttraining-aucpr:0.17413\n",
      "[16:28:24] [166]\ttraining-aucpr:0.17408\n",
      "[16:28:24] [167]\ttraining-aucpr:0.17412\n",
      "[16:28:24] [168]\ttraining-aucpr:0.17412\n",
      "[16:28:24] [169]\ttraining-aucpr:0.17406\n",
      "[16:28:24] [170]\ttraining-aucpr:0.17397\n",
      "[16:28:24] [171]\ttraining-aucpr:0.17423\n",
      "[16:28:24] [172]\ttraining-aucpr:0.17421\n",
      "[16:28:24] [173]\ttraining-aucpr:0.17416\n",
      "[16:28:24] [174]\ttraining-aucpr:0.17425\n",
      "[16:28:24] [175]\ttraining-aucpr:0.17420\n",
      "[16:28:24] [176]\ttraining-aucpr:0.17428\n",
      "[16:28:24] [177]\ttraining-aucpr:0.17426\n",
      "[16:28:24] [178]\ttraining-aucpr:0.17420\n",
      "[16:28:24] [179]\ttraining-aucpr:0.17443\n",
      "[16:28:24] [180]\ttraining-aucpr:0.17444\n",
      "[16:28:24] [181]\ttraining-aucpr:0.17462\n",
      "[16:28:24] [182]\ttraining-aucpr:0.17463\n",
      "[16:28:24] [183]\ttraining-aucpr:0.17467\n",
      "[16:28:24] [184]\ttraining-aucpr:0.17465\n",
      "[16:28:24] [185]\ttraining-aucpr:0.17473\n",
      "[16:28:24] [186]\ttraining-aucpr:0.17477\n",
      "[16:28:24] [187]\ttraining-aucpr:0.17475\n",
      "[16:28:24] [188]\ttraining-aucpr:0.17479\n",
      "[16:28:24] [189]\ttraining-aucpr:0.17482\n",
      "[16:28:25] [190]\ttraining-aucpr:0.17507\n",
      "[16:28:25] [191]\ttraining-aucpr:0.17510\n",
      "[16:28:25] [192]\ttraining-aucpr:0.17508\n",
      "[16:28:25] [193]\ttraining-aucpr:0.17523\n",
      "[16:28:25] [194]\ttraining-aucpr:0.17530\n",
      "[16:28:25] [195]\ttraining-aucpr:0.17526\n",
      "[16:28:25] [196]\ttraining-aucpr:0.17528\n",
      "[16:28:25] [197]\ttraining-aucpr:0.17529\n",
      "[16:28:25] [198]\ttraining-aucpr:0.17538\n",
      "[16:28:25] [199]\ttraining-aucpr:0.17557\n",
      "[16:28:25] [200]\ttraining-aucpr:0.17553\n",
      "[16:28:25] [201]\ttraining-aucpr:0.17556\n",
      "[16:28:25] [202]\ttraining-aucpr:0.17564\n",
      "[16:28:25] [203]\ttraining-aucpr:0.17571\n",
      "[16:28:25] [204]\ttraining-aucpr:0.17576\n",
      "[16:28:25] [205]\ttraining-aucpr:0.17570\n",
      "[16:28:25] [206]\ttraining-aucpr:0.17564\n",
      "[16:28:25] [207]\ttraining-aucpr:0.17572\n",
      "[16:28:25] [208]\ttraining-aucpr:0.17577\n",
      "[16:28:25] [209]\ttraining-aucpr:0.17567\n",
      "[16:28:25] [210]\ttraining-aucpr:0.17575\n",
      "[16:28:25] [211]\ttraining-aucpr:0.17568\n",
      "[16:28:25] [212]\ttraining-aucpr:0.17573\n",
      "[16:28:25] [213]\ttraining-aucpr:0.17566\n",
      "[16:28:25] [214]\ttraining-aucpr:0.17564\n",
      "[16:28:25] [215]\ttraining-aucpr:0.17563\n",
      "[16:28:25] [216]\ttraining-aucpr:0.17563\n",
      "[16:28:25] [217]\ttraining-aucpr:0.17552\n",
      "[16:28:25] [218]\ttraining-aucpr:0.17562\n",
      "[16:28:25] [219]\ttraining-aucpr:0.17566\n",
      "[16:28:25] [220]\ttraining-aucpr:0.17580\n",
      "[16:28:25] [221]\ttraining-aucpr:0.17588\n",
      "[16:28:25] [222]\ttraining-aucpr:0.17584\n",
      "[16:28:25] [223]\ttraining-aucpr:0.17590\n",
      "[16:28:25] [224]\ttraining-aucpr:0.17594\n",
      "[16:28:25] [225]\ttraining-aucpr:0.17595\n",
      "[16:28:25] [226]\ttraining-aucpr:0.17610\n",
      "2025-07-28 16:28:26,931 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:28:27,022 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:28:27,762] Trial 8 finished with value: 0.1882981114544946 and parameters: {'max_depth': 3, 'learning_rate': 0.1601531217136121, 'n_estimators': 227, 'min_child_weight': 8, 'gamma': 0.7712703466859457, 'subsample': 0.5370223258670452, 'colsample_bytree': 0.6792328642721364, 'reg_alpha': 0.11586905952512971, 'reg_lambda': 0.8631034258755935}. Best is trial 8 with value: 0.1882981114544946.\n",
      "2025-07-28 16:28:27,873 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.8187787356776066, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.32518332202674705, 'learning_rate': 0.030816017044468066, 'max_depth': 7, 'min_child_weight': 4, 'reg_alpha': 0.8872127425763265, 'reg_lambda': 0.4722149251619493, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.864803089169032, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 65}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:28:29,256 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:28:30] Task 1 got rank 1\n",
      "[16:28:30] Task 0 got rank 0[16:28:30] Task 2 got rank 2\n",
      "\n",
      "[16:28:30] Task 3 got rank 3\n",
      "[16:28:30] [0]\ttraining-aucpr:0.08140\n",
      "[16:28:30] [1]\ttraining-aucpr:0.09747\n",
      "[16:28:30] [2]\ttraining-aucpr:0.10192\n",
      "[16:28:30] [3]\ttraining-aucpr:0.10514\n",
      "[16:28:30] [4]\ttraining-aucpr:0.10587\n",
      "[16:28:31] [5]\ttraining-aucpr:0.10905\n",
      "[16:28:31] [6]\ttraining-aucpr:0.11218\n",
      "[16:28:31] [7]\ttraining-aucpr:0.11162\n",
      "[16:28:31] [8]\ttraining-aucpr:0.11171\n",
      "[16:28:31] [9]\ttraining-aucpr:0.11211\n",
      "[16:28:31] [10]\ttraining-aucpr:0.11321\n",
      "[16:28:31] [11]\ttraining-aucpr:0.11289\n",
      "[16:28:31] [12]\ttraining-aucpr:0.11320\n",
      "[16:28:31] [13]\ttraining-aucpr:0.11756\n",
      "[16:28:31] [14]\ttraining-aucpr:0.11930\n",
      "[16:28:31] [15]\ttraining-aucpr:0.12050\n",
      "[16:28:31] [16]\ttraining-aucpr:0.12160\n",
      "[16:28:31] [17]\ttraining-aucpr:0.12293\n",
      "[16:28:31] [18]\ttraining-aucpr:0.12518\n",
      "[16:28:31] [19]\ttraining-aucpr:0.12651\n",
      "[16:28:31] [20]\ttraining-aucpr:0.12706\n",
      "[16:28:31] [21]\ttraining-aucpr:0.12838\n",
      "[16:28:31] [22]\ttraining-aucpr:0.12892\n",
      "[16:28:31] [23]\ttraining-aucpr:0.12878\n",
      "[16:28:31] [24]\ttraining-aucpr:0.12936\n",
      "[16:28:31] [25]\ttraining-aucpr:0.12985\n",
      "[16:28:31] [26]\ttraining-aucpr:0.13017\n",
      "[16:28:31] [27]\ttraining-aucpr:0.13125\n",
      "[16:28:31] [28]\ttraining-aucpr:0.13183\n",
      "[16:28:32] [29]\ttraining-aucpr:0.13233\n",
      "[16:28:32] [30]\ttraining-aucpr:0.13250\n",
      "[16:28:32] [31]\ttraining-aucpr:0.13350\n",
      "[16:28:32] [32]\ttraining-aucpr:0.13479\n",
      "[16:28:32] [33]\ttraining-aucpr:0.13501\n",
      "[16:28:32] [34]\ttraining-aucpr:0.13535\n",
      "[16:28:32] [35]\ttraining-aucpr:0.13639\n",
      "[16:28:32] [36]\ttraining-aucpr:0.13770\n",
      "[16:28:32] [37]\ttraining-aucpr:0.13855\n",
      "[16:28:32] [38]\ttraining-aucpr:0.13987\n",
      "[16:28:32] [39]\ttraining-aucpr:0.14082\n",
      "[16:28:32] [40]\ttraining-aucpr:0.14231\n",
      "[16:28:32] [41]\ttraining-aucpr:0.14265\n",
      "[16:28:32] [42]\ttraining-aucpr:0.14331\n",
      "[16:28:32] [43]\ttraining-aucpr:0.14348\n",
      "[16:28:32] [44]\ttraining-aucpr:0.14440\n",
      "[16:28:32] [45]\ttraining-aucpr:0.14523\n",
      "[16:28:32] [46]\ttraining-aucpr:0.14667\n",
      "[16:28:32] [47]\ttraining-aucpr:0.14715\n",
      "[16:28:32] [48]\ttraining-aucpr:0.14741\n",
      "[16:28:32] [49]\ttraining-aucpr:0.14843\n",
      "[16:28:32] [50]\ttraining-aucpr:0.14898\n",
      "[16:28:32] [51]\ttraining-aucpr:0.14986\n",
      "[16:28:32] [52]\ttraining-aucpr:0.15111\n",
      "[16:28:32] [53]\ttraining-aucpr:0.15182\n",
      "[16:28:33] [54]\ttraining-aucpr:0.15284\n",
      "[16:28:33] [55]\ttraining-aucpr:0.15382\n",
      "[16:28:33] [56]\ttraining-aucpr:0.15443\n",
      "[16:28:33] [57]\ttraining-aucpr:0.15498\n",
      "[16:28:33] [58]\ttraining-aucpr:0.15602\n",
      "[16:28:33] [59]\ttraining-aucpr:0.15710\n",
      "[16:28:33] [60]\ttraining-aucpr:0.15839\n",
      "[16:28:33] [61]\ttraining-aucpr:0.15924\n",
      "[16:28:33] [62]\ttraining-aucpr:0.16002\n",
      "[16:28:33] [63]\ttraining-aucpr:0.16065\n",
      "[16:28:33] [64]\ttraining-aucpr:0.16157\n",
      "2025-07-28 16:28:34,428 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:28:34,519 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:28:35,224] Trial 9 finished with value: 0.15355238782615996 and parameters: {'max_depth': 7, 'learning_rate': 0.030816017044468066, 'n_estimators': 65, 'min_child_weight': 4, 'gamma': 0.32518332202674705, 'subsample': 0.864803089169032, 'colsample_bytree': 0.8187787356776066, 'reg_alpha': 0.8872127425763265, 'reg_lambda': 0.4722149251619493}. Best is trial 8 with value: 0.1882981114544946.\n",
      "2025-07-28 16:28:35,329 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.6289634763007459, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.9792234821987593, 'learning_rate': 0.11041930924484158, 'max_depth': 3, 'min_child_weight': 10, 'reg_alpha': 0.011114384822711307, 'reg_lambda': 0.9693130727540947, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.8451235367845725, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 127}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:28:36,725 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:28:37] Task 0 got rank 0\n",
      "[16:28:37] Task 1 got rank 1\n",
      "[16:28:37] Task 3 got rank 3\n",
      "[16:28:37] Task 2 got rank 2\n",
      "[16:28:38] [0]\ttraining-aucpr:0.04728\n",
      "[16:28:38] [1]\ttraining-aucpr:0.04822\n",
      "[16:28:38] [2]\ttraining-aucpr:0.05855\n",
      "[16:28:38] [3]\ttraining-aucpr:0.05871\n",
      "[16:28:38] [4]\ttraining-aucpr:0.05921\n",
      "[16:28:38] [5]\ttraining-aucpr:0.06549\n",
      "[16:28:38] [6]\ttraining-aucpr:0.07239\n",
      "[16:28:38] [7]\ttraining-aucpr:0.07314\n",
      "[16:28:38] [8]\ttraining-aucpr:0.07817\n",
      "[16:28:38] [9]\ttraining-aucpr:0.08005\n",
      "[16:28:38] [10]\ttraining-aucpr:0.08042\n",
      "[16:28:38] [11]\ttraining-aucpr:0.08150\n",
      "[16:28:38] [12]\ttraining-aucpr:0.09115\n",
      "[16:28:38] [13]\ttraining-aucpr:0.10044\n",
      "[16:28:38] [14]\ttraining-aucpr:0.10122\n",
      "[16:28:38] [15]\ttraining-aucpr:0.10196\n",
      "[16:28:38] [16]\ttraining-aucpr:0.10529\n",
      "[16:28:38] [17]\ttraining-aucpr:0.10842\n",
      "[16:28:38] [18]\ttraining-aucpr:0.11036\n",
      "[16:28:38] [19]\ttraining-aucpr:0.10995\n",
      "[16:28:38] [20]\ttraining-aucpr:0.11169\n",
      "[16:28:38] [21]\ttraining-aucpr:0.11155\n",
      "[16:28:38] [22]\ttraining-aucpr:0.11473\n",
      "[16:28:38] [23]\ttraining-aucpr:0.11583\n",
      "[16:28:38] [24]\ttraining-aucpr:0.11810\n",
      "[16:28:38] [25]\ttraining-aucpr:0.12044\n",
      "[16:28:38] [26]\ttraining-aucpr:0.12143\n",
      "[16:28:38] [27]\ttraining-aucpr:0.12378\n",
      "[16:28:38] [28]\ttraining-aucpr:0.12672\n",
      "[16:28:38] [29]\ttraining-aucpr:0.12694\n",
      "[16:28:38] [30]\ttraining-aucpr:0.12759\n",
      "[16:28:39] [31]\ttraining-aucpr:0.12769\n",
      "[16:28:39] [32]\ttraining-aucpr:0.12931\n",
      "[16:28:39] [33]\ttraining-aucpr:0.12925\n",
      "[16:28:39] [34]\ttraining-aucpr:0.12964\n",
      "[16:28:39] [35]\ttraining-aucpr:0.13032\n",
      "[16:28:39] [36]\ttraining-aucpr:0.13120\n",
      "[16:28:39] [37]\ttraining-aucpr:0.13275\n",
      "[16:28:39] [38]\ttraining-aucpr:0.13391\n",
      "[16:28:39] [39]\ttraining-aucpr:0.13389\n",
      "[16:28:39] [40]\ttraining-aucpr:0.13496\n",
      "[16:28:39] [41]\ttraining-aucpr:0.13550\n",
      "[16:28:39] [42]\ttraining-aucpr:0.13581\n",
      "[16:28:39] [43]\ttraining-aucpr:0.13686\n",
      "[16:28:39] [44]\ttraining-aucpr:0.13686\n",
      "[16:28:39] [45]\ttraining-aucpr:0.13865\n",
      "[16:28:39] [46]\ttraining-aucpr:0.13921\n",
      "[16:28:39] [47]\ttraining-aucpr:0.13993\n",
      "[16:28:39] [48]\ttraining-aucpr:0.14069\n",
      "[16:28:39] [49]\ttraining-aucpr:0.14111\n",
      "[16:28:39] [50]\ttraining-aucpr:0.14202\n",
      "[16:28:39] [51]\ttraining-aucpr:0.14261\n",
      "[16:28:39] [52]\ttraining-aucpr:0.14304\n",
      "[16:28:39] [53]\ttraining-aucpr:0.14385\n",
      "[16:28:39] [54]\ttraining-aucpr:0.14438\n",
      "[16:28:39] [55]\ttraining-aucpr:0.14535\n",
      "[16:28:39] [56]\ttraining-aucpr:0.14511\n",
      "[16:28:39] [57]\ttraining-aucpr:0.14529\n",
      "[16:28:39] [58]\ttraining-aucpr:0.14627\n",
      "[16:28:39] [59]\ttraining-aucpr:0.14683\n",
      "[16:28:39] [60]\ttraining-aucpr:0.14681\n",
      "[16:28:39] [61]\ttraining-aucpr:0.14715\n",
      "[16:28:39] [62]\ttraining-aucpr:0.14782\n",
      "[16:28:39] [63]\ttraining-aucpr:0.14798\n",
      "[16:28:39] [64]\ttraining-aucpr:0.14838\n",
      "[16:28:39] [65]\ttraining-aucpr:0.14930\n",
      "[16:28:39] [66]\ttraining-aucpr:0.14968\n",
      "[16:28:39] [67]\ttraining-aucpr:0.15004\n",
      "[16:28:39] [68]\ttraining-aucpr:0.15075\n",
      "[16:28:39] [69]\ttraining-aucpr:0.15141\n",
      "[16:28:39] [70]\ttraining-aucpr:0.15182\n",
      "[16:28:40] [71]\ttraining-aucpr:0.15207\n",
      "[16:28:40] [72]\ttraining-aucpr:0.15284\n",
      "[16:28:40] [73]\ttraining-aucpr:0.15327\n",
      "[16:28:40] [74]\ttraining-aucpr:0.15412\n",
      "[16:28:40] [75]\ttraining-aucpr:0.15457\n",
      "[16:28:40] [76]\ttraining-aucpr:0.15511\n",
      "[16:28:40] [77]\ttraining-aucpr:0.15532\n",
      "[16:28:40] [78]\ttraining-aucpr:0.15570\n",
      "[16:28:40] [79]\ttraining-aucpr:0.15625\n",
      "[16:28:40] [80]\ttraining-aucpr:0.15667\n",
      "[16:28:40] [81]\ttraining-aucpr:0.15699\n",
      "[16:28:40] [82]\ttraining-aucpr:0.15743\n",
      "[16:28:40] [83]\ttraining-aucpr:0.15787\n",
      "[16:28:40] [84]\ttraining-aucpr:0.15808\n",
      "[16:28:40] [85]\ttraining-aucpr:0.15812\n",
      "[16:28:40] [86]\ttraining-aucpr:0.15818\n",
      "[16:28:40] [87]\ttraining-aucpr:0.15844\n",
      "[16:28:40] [88]\ttraining-aucpr:0.15846\n",
      "[16:28:40] [89]\ttraining-aucpr:0.15887\n",
      "[16:28:40] [90]\ttraining-aucpr:0.15907\n",
      "[16:28:40] [91]\ttraining-aucpr:0.15940\n",
      "[16:28:40] [92]\ttraining-aucpr:0.15966\n",
      "[16:28:40] [93]\ttraining-aucpr:0.15996\n",
      "[16:28:40] [94]\ttraining-aucpr:0.16026\n",
      "[16:28:40] [95]\ttraining-aucpr:0.16048\n",
      "[16:28:40] [96]\ttraining-aucpr:0.16081\n",
      "[16:28:40] [97]\ttraining-aucpr:0.16103\n",
      "[16:28:40] [98]\ttraining-aucpr:0.16099\n",
      "[16:28:40] [99]\ttraining-aucpr:0.16128\n",
      "[16:28:40] [100]\ttraining-aucpr:0.16147\n",
      "[16:28:40] [101]\ttraining-aucpr:0.16161\n",
      "[16:28:40] [102]\ttraining-aucpr:0.16176\n",
      "[16:28:40] [103]\ttraining-aucpr:0.16192\n",
      "[16:28:40] [104]\ttraining-aucpr:0.16226\n",
      "[16:28:40] [105]\ttraining-aucpr:0.16266\n",
      "[16:28:40] [106]\ttraining-aucpr:0.16273\n",
      "[16:28:40] [107]\ttraining-aucpr:0.16271\n",
      "[16:28:40] [108]\ttraining-aucpr:0.16292\n",
      "[16:28:40] [109]\ttraining-aucpr:0.16319\n",
      "[16:28:40] [110]\ttraining-aucpr:0.16331\n",
      "[16:28:40] [111]\ttraining-aucpr:0.16334\n",
      "[16:28:40] [112]\ttraining-aucpr:0.16396\n",
      "[16:28:41] [113]\ttraining-aucpr:0.16410\n",
      "[16:28:41] [114]\ttraining-aucpr:0.16418\n",
      "[16:28:41] [115]\ttraining-aucpr:0.16471\n",
      "[16:28:41] [116]\ttraining-aucpr:0.16490\n",
      "[16:28:41] [117]\ttraining-aucpr:0.16514\n",
      "[16:28:41] [118]\ttraining-aucpr:0.16520\n",
      "[16:28:41] [119]\ttraining-aucpr:0.16543\n",
      "[16:28:41] [120]\ttraining-aucpr:0.16570\n",
      "[16:28:41] [121]\ttraining-aucpr:0.16590\n",
      "[16:28:41] [122]\ttraining-aucpr:0.16596\n",
      "[16:28:41] [123]\ttraining-aucpr:0.16613\n",
      "[16:28:41] [124]\ttraining-aucpr:0.16621\n",
      "[16:28:41] [125]\ttraining-aucpr:0.16652\n",
      "[16:28:41] [126]\ttraining-aucpr:0.16661\n",
      "2025-07-28 16:28:42,337 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:28:42,414 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:28:43,151] Trial 10 finished with value: 0.19120947778198935 and parameters: {'max_depth': 3, 'learning_rate': 0.11041930924484158, 'n_estimators': 127, 'min_child_weight': 10, 'gamma': 0.9792234821987593, 'subsample': 0.8451235367845725, 'colsample_bytree': 0.6289634763007459, 'reg_alpha': 0.011114384822711307, 'reg_lambda': 0.9693130727540947}. Best is trial 10 with value: 0.19120947778198935.\n",
      "2025-07-28 16:28:43,265 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.6422284320908205, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.9897203644673119, 'learning_rate': 0.11511579879447047, 'max_depth': 3, 'min_child_weight': 10, 'reg_alpha': 0.011046564754964586, 'reg_lambda': 0.9989171937915848, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.8513496163789992, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 121}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:28:44,652 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:28:45] Task 1 got rank 1\n",
      "[16:28:45] Task 0 got rank 0\n",
      "[16:28:45] Task 3 got rank 3\n",
      "[16:28:45] Task 2 got rank 2\n",
      "[16:28:46] [0]\ttraining-aucpr:0.04724\n",
      "[16:28:46] [1]\ttraining-aucpr:0.04822\n",
      "[16:28:46] [2]\ttraining-aucpr:0.05770\n",
      "[16:28:46] [3]\ttraining-aucpr:0.06320\n",
      "[16:28:46] [4]\ttraining-aucpr:0.06342\n",
      "[16:28:46] [5]\ttraining-aucpr:0.06666\n",
      "[16:28:46] [6]\ttraining-aucpr:0.07338\n",
      "[16:28:46] [7]\ttraining-aucpr:0.07448\n",
      "[16:28:46] [8]\ttraining-aucpr:0.07872\n",
      "[16:28:46] [9]\ttraining-aucpr:0.07995\n",
      "[16:28:46] [10]\ttraining-aucpr:0.08664\n",
      "[16:28:46] [11]\ttraining-aucpr:0.08786\n",
      "[16:28:46] [12]\ttraining-aucpr:0.08732\n",
      "[16:28:46] [13]\ttraining-aucpr:0.09757\n",
      "[16:28:46] [14]\ttraining-aucpr:0.10042\n",
      "[16:28:46] [15]\ttraining-aucpr:0.10007\n",
      "[16:28:46] [16]\ttraining-aucpr:0.10133\n",
      "[16:28:46] [17]\ttraining-aucpr:0.10484\n",
      "[16:28:46] [18]\ttraining-aucpr:0.10558\n",
      "[16:28:46] [19]\ttraining-aucpr:0.10583\n",
      "[16:28:46] [20]\ttraining-aucpr:0.10834\n",
      "[16:28:46] [21]\ttraining-aucpr:0.10911\n",
      "[16:28:46] [22]\ttraining-aucpr:0.11257\n",
      "[16:28:46] [23]\ttraining-aucpr:0.11255\n",
      "[16:28:46] [24]\ttraining-aucpr:0.11627\n",
      "[16:28:46] [25]\ttraining-aucpr:0.11866\n",
      "[16:28:46] [26]\ttraining-aucpr:0.12040\n",
      "[16:28:46] [27]\ttraining-aucpr:0.12274\n",
      "[16:28:46] [28]\ttraining-aucpr:0.12539\n",
      "[16:28:46] [29]\ttraining-aucpr:0.12601\n",
      "[16:28:46] [30]\ttraining-aucpr:0.12698\n",
      "[16:28:46] [31]\ttraining-aucpr:0.12708\n",
      "[16:28:46] [32]\ttraining-aucpr:0.12811\n",
      "[16:28:46] [33]\ttraining-aucpr:0.12799\n",
      "[16:28:46] [34]\ttraining-aucpr:0.12897\n",
      "[16:28:46] [35]\ttraining-aucpr:0.12939\n",
      "[16:28:46] [36]\ttraining-aucpr:0.12989\n",
      "[16:28:47] [37]\ttraining-aucpr:0.13215\n",
      "[16:28:47] [38]\ttraining-aucpr:0.13403\n",
      "[16:28:47] [39]\ttraining-aucpr:0.13426\n",
      "[16:28:47] [40]\ttraining-aucpr:0.13581\n",
      "[16:28:47] [41]\ttraining-aucpr:0.13580\n",
      "[16:28:47] [42]\ttraining-aucpr:0.13642\n",
      "[16:28:47] [43]\ttraining-aucpr:0.13721\n",
      "[16:28:47] [44]\ttraining-aucpr:0.13768\n",
      "[16:28:47] [45]\ttraining-aucpr:0.13931\n",
      "[16:28:47] [46]\ttraining-aucpr:0.14036\n",
      "[16:28:47] [47]\ttraining-aucpr:0.14065\n",
      "[16:28:47] [48]\ttraining-aucpr:0.14171\n",
      "[16:28:47] [49]\ttraining-aucpr:0.14256\n",
      "[16:28:47] [50]\ttraining-aucpr:0.14302\n",
      "[16:28:47] [51]\ttraining-aucpr:0.14352\n",
      "[16:28:47] [52]\ttraining-aucpr:0.14393\n",
      "[16:28:47] [53]\ttraining-aucpr:0.14415\n",
      "[16:28:47] [54]\ttraining-aucpr:0.14453\n",
      "[16:28:47] [55]\ttraining-aucpr:0.14542\n",
      "[16:28:47] [56]\ttraining-aucpr:0.14584\n",
      "[16:28:47] [57]\ttraining-aucpr:0.14672\n",
      "[16:28:47] [58]\ttraining-aucpr:0.14693\n",
      "[16:28:47] [59]\ttraining-aucpr:0.14758\n",
      "[16:28:47] [60]\ttraining-aucpr:0.14790\n",
      "[16:28:47] [61]\ttraining-aucpr:0.14812\n",
      "[16:28:47] [62]\ttraining-aucpr:0.14834\n",
      "[16:28:47] [63]\ttraining-aucpr:0.14882\n",
      "[16:28:47] [64]\ttraining-aucpr:0.14931\n",
      "[16:28:47] [65]\ttraining-aucpr:0.14967\n",
      "[16:28:47] [66]\ttraining-aucpr:0.15039\n",
      "[16:28:47] [67]\ttraining-aucpr:0.15068\n",
      "[16:28:47] [68]\ttraining-aucpr:0.15151\n",
      "[16:28:47] [69]\ttraining-aucpr:0.15187\n",
      "[16:28:47] [70]\ttraining-aucpr:0.15265\n",
      "[16:28:47] [71]\ttraining-aucpr:0.15314\n",
      "[16:28:47] [72]\ttraining-aucpr:0.15368\n",
      "[16:28:47] [73]\ttraining-aucpr:0.15389\n",
      "[16:28:47] [74]\ttraining-aucpr:0.15470\n",
      "[16:28:47] [75]\ttraining-aucpr:0.15490\n",
      "[16:28:47] [76]\ttraining-aucpr:0.15529\n",
      "[16:28:47] [77]\ttraining-aucpr:0.15551\n",
      "[16:28:47] [78]\ttraining-aucpr:0.15589\n",
      "[16:28:47] [79]\ttraining-aucpr:0.15623\n",
      "[16:28:48] [80]\ttraining-aucpr:0.15661\n",
      "[16:28:48] [81]\ttraining-aucpr:0.15707\n",
      "[16:28:48] [82]\ttraining-aucpr:0.15751\n",
      "[16:28:48] [83]\ttraining-aucpr:0.15800\n",
      "[16:28:48] [84]\ttraining-aucpr:0.15842\n",
      "[16:28:48] [85]\ttraining-aucpr:0.15860\n",
      "[16:28:48] [86]\ttraining-aucpr:0.15878\n",
      "[16:28:48] [87]\ttraining-aucpr:0.15920\n",
      "[16:28:48] [88]\ttraining-aucpr:0.15943\n",
      "[16:28:48] [89]\ttraining-aucpr:0.15966\n",
      "[16:28:48] [90]\ttraining-aucpr:0.15967\n",
      "[16:28:48] [91]\ttraining-aucpr:0.16016\n",
      "[16:28:48] [92]\ttraining-aucpr:0.16021\n",
      "[16:28:48] [93]\ttraining-aucpr:0.16063\n",
      "[16:28:48] [94]\ttraining-aucpr:0.16099\n",
      "[16:28:48] [95]\ttraining-aucpr:0.16111\n",
      "[16:28:48] [96]\ttraining-aucpr:0.16147\n",
      "[16:28:48] [97]\ttraining-aucpr:0.16178\n",
      "[16:28:48] [98]\ttraining-aucpr:0.16189\n",
      "[16:28:48] [99]\ttraining-aucpr:0.16222\n",
      "[16:28:48] [100]\ttraining-aucpr:0.16254\n",
      "[16:28:48] [101]\ttraining-aucpr:0.16255\n",
      "[16:28:48] [102]\ttraining-aucpr:0.16280\n",
      "[16:28:48] [103]\ttraining-aucpr:0.16293\n",
      "[16:28:48] [104]\ttraining-aucpr:0.16296\n",
      "[16:28:48] [105]\ttraining-aucpr:0.16285\n",
      "[16:28:48] [106]\ttraining-aucpr:0.16309\n",
      "[16:28:48] [107]\ttraining-aucpr:0.16322\n",
      "[16:28:48] [108]\ttraining-aucpr:0.16344\n",
      "[16:28:48] [109]\ttraining-aucpr:0.16373\n",
      "[16:28:48] [110]\ttraining-aucpr:0.16390\n",
      "[16:28:48] [111]\ttraining-aucpr:0.16405\n",
      "[16:28:48] [112]\ttraining-aucpr:0.16427\n",
      "[16:28:48] [113]\ttraining-aucpr:0.16437\n",
      "[16:28:48] [114]\ttraining-aucpr:0.16454\n",
      "[16:28:48] [115]\ttraining-aucpr:0.16469\n",
      "[16:28:48] [116]\ttraining-aucpr:0.16490\n",
      "[16:28:48] [117]\ttraining-aucpr:0.16516\n",
      "[16:28:48] [118]\ttraining-aucpr:0.16521\n",
      "[16:28:48] [119]\ttraining-aucpr:0.16537\n",
      "[16:28:48] [120]\ttraining-aucpr:0.16561\n",
      "2025-07-28 16:28:50,005 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:28:50,096 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:28:50,768] Trial 11 finished with value: 0.19092385714953872 and parameters: {'max_depth': 3, 'learning_rate': 0.11511579879447047, 'n_estimators': 121, 'min_child_weight': 10, 'gamma': 0.9897203644673119, 'subsample': 0.8513496163789992, 'colsample_bytree': 0.6422284320908205, 'reg_alpha': 0.011046564754964586, 'reg_lambda': 0.9989171937915848}. Best is trial 10 with value: 0.19120947778198935.\n",
      "2025-07-28 16:28:50,879 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.611423984149992, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.9971405225644667, 'learning_rate': 0.08995147228189711, 'max_depth': 3, 'min_child_weight': 10, 'reg_alpha': 0.0335858849344719, 'reg_lambda': 0.9920380779070681, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.8588729908849948, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 122}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:28:52,264 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:28:53] Task 0 got rank 0\n",
      "[16:28:53] Task 2 got rank 2\n",
      "[16:28:53] Task 1 got rank 1\n",
      "[16:28:53] Task 3 got rank 3\n",
      "[16:28:53] [0]\ttraining-aucpr:0.04724\n",
      "[16:28:53] [1]\ttraining-aucpr:0.04822\n",
      "[16:28:53] [2]\ttraining-aucpr:0.05768\n",
      "[16:28:53] [3]\ttraining-aucpr:0.06268\n",
      "[16:28:53] [4]\ttraining-aucpr:0.06308\n",
      "[16:28:53] [5]\ttraining-aucpr:0.06621\n",
      "[16:28:53] [6]\ttraining-aucpr:0.07287\n",
      "[16:28:53] [7]\ttraining-aucpr:0.07367\n",
      "[16:28:53] [8]\ttraining-aucpr:0.07822\n",
      "[16:28:53] [9]\ttraining-aucpr:0.08230\n",
      "[16:28:54] [10]\ttraining-aucpr:0.08533\n",
      "[16:28:54] [11]\ttraining-aucpr:0.08556\n",
      "[16:28:54] [12]\ttraining-aucpr:0.08565\n",
      "[16:28:54] [13]\ttraining-aucpr:0.09570\n",
      "[16:28:54] [14]\ttraining-aucpr:0.09632\n",
      "[16:28:54] [15]\ttraining-aucpr:0.09749\n",
      "[16:28:54] [16]\ttraining-aucpr:0.10092\n",
      "[16:28:54] [17]\ttraining-aucpr:0.10388\n",
      "[16:28:54] [18]\ttraining-aucpr:0.10435\n",
      "[16:28:54] [19]\ttraining-aucpr:0.10631\n",
      "[16:28:54] [20]\ttraining-aucpr:0.10688\n",
      "[16:28:54] [21]\ttraining-aucpr:0.10771\n",
      "[16:28:54] [22]\ttraining-aucpr:0.10912\n",
      "[16:28:54] [23]\ttraining-aucpr:0.10936\n",
      "[16:28:54] [24]\ttraining-aucpr:0.11140\n",
      "[16:28:54] [25]\ttraining-aucpr:0.11371\n",
      "[16:28:54] [26]\ttraining-aucpr:0.11466\n",
      "[16:28:54] [27]\ttraining-aucpr:0.11596\n",
      "[16:28:54] [28]\ttraining-aucpr:0.11990\n",
      "[16:28:54] [29]\ttraining-aucpr:0.12077\n",
      "[16:28:54] [30]\ttraining-aucpr:0.12153\n",
      "[16:28:54] [31]\ttraining-aucpr:0.12296\n",
      "[16:28:54] [32]\ttraining-aucpr:0.12473\n",
      "[16:28:54] [33]\ttraining-aucpr:0.12498\n",
      "[16:28:54] [34]\ttraining-aucpr:0.12575\n",
      "[16:28:54] [35]\ttraining-aucpr:0.12629\n",
      "[16:28:54] [36]\ttraining-aucpr:0.12672\n",
      "[16:28:54] [37]\ttraining-aucpr:0.12817\n",
      "[16:28:54] [38]\ttraining-aucpr:0.12905\n",
      "[16:28:54] [39]\ttraining-aucpr:0.12980\n",
      "[16:28:54] [40]\ttraining-aucpr:0.13125\n",
      "[16:28:54] [41]\ttraining-aucpr:0.13143\n",
      "[16:28:54] [42]\ttraining-aucpr:0.13199\n",
      "[16:28:54] [43]\ttraining-aucpr:0.13201\n",
      "[16:28:54] [44]\ttraining-aucpr:0.13211\n",
      "[16:28:54] [45]\ttraining-aucpr:0.13324\n",
      "[16:28:54] [46]\ttraining-aucpr:0.13445\n",
      "[16:28:54] [47]\ttraining-aucpr:0.13487\n",
      "[16:28:54] [48]\ttraining-aucpr:0.13549\n",
      "[16:28:54] [49]\ttraining-aucpr:0.13574\n",
      "[16:28:54] [50]\ttraining-aucpr:0.13743\n",
      "[16:28:54] [51]\ttraining-aucpr:0.13767\n",
      "[16:28:54] [52]\ttraining-aucpr:0.13820\n",
      "[16:28:54] [53]\ttraining-aucpr:0.13844\n",
      "[16:28:55] [54]\ttraining-aucpr:0.14010\n",
      "[16:28:55] [55]\ttraining-aucpr:0.14008\n",
      "[16:28:55] [56]\ttraining-aucpr:0.14059\n",
      "[16:28:55] [57]\ttraining-aucpr:0.14108\n",
      "[16:28:55] [58]\ttraining-aucpr:0.14189\n",
      "[16:28:55] [59]\ttraining-aucpr:0.14275\n",
      "[16:28:55] [60]\ttraining-aucpr:0.14321\n",
      "[16:28:55] [61]\ttraining-aucpr:0.14336\n",
      "[16:28:55] [62]\ttraining-aucpr:0.14344\n",
      "[16:28:55] [63]\ttraining-aucpr:0.14363\n",
      "[16:28:55] [64]\ttraining-aucpr:0.14390\n",
      "[16:28:55] [65]\ttraining-aucpr:0.14466\n",
      "[16:28:55] [66]\ttraining-aucpr:0.14485\n",
      "[16:28:55] [67]\ttraining-aucpr:0.14569\n",
      "[16:28:55] [68]\ttraining-aucpr:0.14611\n",
      "[16:28:55] [69]\ttraining-aucpr:0.14686\n",
      "[16:28:55] [70]\ttraining-aucpr:0.14708\n",
      "[16:28:55] [71]\ttraining-aucpr:0.14763\n",
      "[16:28:55] [72]\ttraining-aucpr:0.14840\n",
      "[16:28:55] [73]\ttraining-aucpr:0.14842\n",
      "[16:28:55] [74]\ttraining-aucpr:0.14878\n",
      "[16:28:55] [75]\ttraining-aucpr:0.14939\n",
      "[16:28:55] [76]\ttraining-aucpr:0.14995\n",
      "[16:28:55] [77]\ttraining-aucpr:0.15015\n",
      "[16:28:55] [78]\ttraining-aucpr:0.15059\n",
      "[16:28:55] [79]\ttraining-aucpr:0.15111\n",
      "[16:28:55] [80]\ttraining-aucpr:0.15122\n",
      "[16:28:55] [81]\ttraining-aucpr:0.15158\n",
      "[16:28:55] [82]\ttraining-aucpr:0.15208\n",
      "[16:28:55] [83]\ttraining-aucpr:0.15267\n",
      "[16:28:55] [84]\ttraining-aucpr:0.15322\n",
      "[16:28:55] [85]\ttraining-aucpr:0.15350\n",
      "[16:28:55] [86]\ttraining-aucpr:0.15373\n",
      "[16:28:55] [87]\ttraining-aucpr:0.15406\n",
      "[16:28:55] [88]\ttraining-aucpr:0.15402\n",
      "[16:28:55] [89]\ttraining-aucpr:0.15414\n",
      "[16:28:55] [90]\ttraining-aucpr:0.15445\n",
      "[16:28:55] [91]\ttraining-aucpr:0.15453\n",
      "[16:28:55] [92]\ttraining-aucpr:0.15496\n",
      "[16:28:55] [93]\ttraining-aucpr:0.15539\n",
      "[16:28:55] [94]\ttraining-aucpr:0.15566\n",
      "[16:28:56] [95]\ttraining-aucpr:0.15565\n",
      "[16:28:56] [96]\ttraining-aucpr:0.15589\n",
      "[16:28:56] [97]\ttraining-aucpr:0.15604\n",
      "[16:28:56] [98]\ttraining-aucpr:0.15620\n",
      "[16:28:56] [99]\ttraining-aucpr:0.15658\n",
      "[16:28:56] [100]\ttraining-aucpr:0.15676\n",
      "[16:28:56] [101]\ttraining-aucpr:0.15709\n",
      "[16:28:56] [102]\ttraining-aucpr:0.15696\n",
      "[16:28:56] [103]\ttraining-aucpr:0.15748\n",
      "[16:28:56] [104]\ttraining-aucpr:0.15776\n",
      "[16:28:56] [105]\ttraining-aucpr:0.15808\n",
      "[16:28:56] [106]\ttraining-aucpr:0.15837\n",
      "[16:28:56] [107]\ttraining-aucpr:0.15859\n",
      "[16:28:56] [108]\ttraining-aucpr:0.15883\n",
      "[16:28:56] [109]\ttraining-aucpr:0.15923\n",
      "[16:28:56] [110]\ttraining-aucpr:0.15926\n",
      "[16:28:56] [111]\ttraining-aucpr:0.15938\n",
      "[16:28:56] [112]\ttraining-aucpr:0.15993\n",
      "[16:28:56] [113]\ttraining-aucpr:0.16000\n",
      "[16:28:56] [114]\ttraining-aucpr:0.16008\n",
      "[16:28:56] [115]\ttraining-aucpr:0.16044\n",
      "[16:28:56] [116]\ttraining-aucpr:0.16082\n",
      "[16:28:56] [117]\ttraining-aucpr:0.16104\n",
      "[16:28:56] [118]\ttraining-aucpr:0.16111\n",
      "[16:28:56] [119]\ttraining-aucpr:0.16124\n",
      "[16:28:56] [120]\ttraining-aucpr:0.16148\n",
      "[16:28:56] [121]\ttraining-aucpr:0.16172\n",
      "2025-07-28 16:28:57,646 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:28:57,742 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:28:58,456] Trial 12 finished with value: 0.1854952323792174 and parameters: {'max_depth': 3, 'learning_rate': 0.08995147228189711, 'n_estimators': 122, 'min_child_weight': 10, 'gamma': 0.9971405225644667, 'subsample': 0.8588729908849948, 'colsample_bytree': 0.611423984149992, 'reg_alpha': 0.0335858849344719, 'reg_lambda': 0.9920380779070681}. Best is trial 10 with value: 0.19120947778198935.\n",
      "2025-07-28 16:28:58,566 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.6207774664901697, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.985605201044127, 'learning_rate': 0.09596139670893554, 'max_depth': 10, 'min_child_weight': 10, 'reg_alpha': 0.49708539361085236, 'reg_lambda': 0.6855046081289342, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.8585787245031457, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 129}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:28:59,955 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:29:00] Task 2 got rank 2[16:29:00] Task 1 got rank 1\n",
      "\n",
      "[16:29:00] Task 0 got rank 0\n",
      "[16:29:00] Task 3 got rank 3\n",
      "[16:29:01] [0]\ttraining-aucpr:0.10920\n",
      "[16:29:01] [1]\ttraining-aucpr:0.14093\n",
      "[16:29:01] [2]\ttraining-aucpr:0.15692\n",
      "[16:29:01] [3]\ttraining-aucpr:0.16212\n",
      "[16:29:01] [4]\ttraining-aucpr:0.16598\n",
      "[16:29:01] [5]\ttraining-aucpr:0.17247\n",
      "[16:29:02] [6]\ttraining-aucpr:0.18219\n",
      "[16:29:02] [7]\ttraining-aucpr:0.18626\n",
      "[16:29:02] [8]\ttraining-aucpr:0.19516\n",
      "[16:29:02] [9]\ttraining-aucpr:0.20221\n",
      "[16:29:02] [10]\ttraining-aucpr:0.20802\n",
      "[16:29:02] [11]\ttraining-aucpr:0.21161\n",
      "[16:29:02] [12]\ttraining-aucpr:0.21512\n",
      "[16:29:02] [13]\ttraining-aucpr:0.22164\n",
      "[16:29:02] [14]\ttraining-aucpr:0.22610\n",
      "[16:29:02] [15]\ttraining-aucpr:0.22917\n",
      "[16:29:02] [16]\ttraining-aucpr:0.23248\n",
      "[16:29:02] [17]\ttraining-aucpr:0.23850\n",
      "[16:29:02] [18]\ttraining-aucpr:0.24305\n",
      "[16:29:02] [19]\ttraining-aucpr:0.24631\n",
      "[16:29:03] [20]\ttraining-aucpr:0.25284\n",
      "[16:29:03] [21]\ttraining-aucpr:0.26028\n",
      "[16:29:03] [22]\ttraining-aucpr:0.26781\n",
      "[16:29:03] [23]\ttraining-aucpr:0.26945\n",
      "[16:29:03] [24]\ttraining-aucpr:0.27622\n",
      "[16:29:03] [25]\ttraining-aucpr:0.28234\n",
      "[16:29:03] [26]\ttraining-aucpr:0.28788\n",
      "[16:29:03] [27]\ttraining-aucpr:0.29382\n",
      "[16:29:03] [28]\ttraining-aucpr:0.30119\n",
      "[16:29:03] [29]\ttraining-aucpr:0.30624\n",
      "[16:29:03] [30]\ttraining-aucpr:0.31063\n",
      "[16:29:03] [31]\ttraining-aucpr:0.31320\n",
      "[16:29:03] [32]\ttraining-aucpr:0.31887\n",
      "[16:29:03] [33]\ttraining-aucpr:0.32318\n",
      "[16:29:04] [34]\ttraining-aucpr:0.32769\n",
      "[16:29:04] [35]\ttraining-aucpr:0.33105\n",
      "[16:29:04] [36]\ttraining-aucpr:0.33412\n",
      "[16:29:04] [37]\ttraining-aucpr:0.34045\n",
      "[16:29:04] [38]\ttraining-aucpr:0.34578\n",
      "[16:29:04] [39]\ttraining-aucpr:0.34896\n",
      "[16:29:04] [40]\ttraining-aucpr:0.35407\n",
      "[16:29:04] [41]\ttraining-aucpr:0.35617\n",
      "[16:29:04] [42]\ttraining-aucpr:0.36078\n",
      "[16:29:04] [43]\ttraining-aucpr:0.36338\n",
      "[16:29:04] [44]\ttraining-aucpr:0.36723\n",
      "[16:29:04] [45]\ttraining-aucpr:0.37098\n",
      "[16:29:04] [46]\ttraining-aucpr:0.37528\n",
      "[16:29:04] [47]\ttraining-aucpr:0.38074\n",
      "[16:29:04] [48]\ttraining-aucpr:0.38489\n",
      "[16:29:04] [49]\ttraining-aucpr:0.38974\n",
      "[16:29:05] [50]\ttraining-aucpr:0.39292\n",
      "[16:29:05] [51]\ttraining-aucpr:0.39821\n",
      "[16:29:05] [52]\ttraining-aucpr:0.40150\n",
      "[16:29:05] [53]\ttraining-aucpr:0.40631\n",
      "[16:29:05] [54]\ttraining-aucpr:0.41057\n",
      "[16:29:05] [55]\ttraining-aucpr:0.41396\n",
      "[16:29:05] [56]\ttraining-aucpr:0.41749\n",
      "[16:29:05] [57]\ttraining-aucpr:0.42128\n",
      "[16:29:05] [58]\ttraining-aucpr:0.42477\n",
      "[16:29:05] [59]\ttraining-aucpr:0.42839\n",
      "[16:29:05] [60]\ttraining-aucpr:0.43143\n",
      "[16:29:05] [61]\ttraining-aucpr:0.43642\n",
      "[16:29:05] [62]\ttraining-aucpr:0.44091\n",
      "[16:29:05] [63]\ttraining-aucpr:0.44374\n",
      "[16:29:05] [64]\ttraining-aucpr:0.44698\n",
      "[16:29:05] [65]\ttraining-aucpr:0.45019\n",
      "[16:29:05] [66]\ttraining-aucpr:0.45443\n",
      "[16:29:05] [67]\ttraining-aucpr:0.45739\n",
      "[16:29:06] [68]\ttraining-aucpr:0.46119\n",
      "[16:29:06] [69]\ttraining-aucpr:0.46535\n",
      "[16:29:06] [70]\ttraining-aucpr:0.46839\n",
      "[16:29:06] [71]\ttraining-aucpr:0.47295\n",
      "[16:29:06] [72]\ttraining-aucpr:0.47700\n",
      "[16:29:06] [73]\ttraining-aucpr:0.48020\n",
      "[16:29:06] [74]\ttraining-aucpr:0.48386\n",
      "[16:29:06] [75]\ttraining-aucpr:0.48727\n",
      "[16:29:06] [76]\ttraining-aucpr:0.49057\n",
      "[16:29:06] [77]\ttraining-aucpr:0.49328\n",
      "[16:29:06] [78]\ttraining-aucpr:0.49777\n",
      "[16:29:06] [79]\ttraining-aucpr:0.50147\n",
      "[16:29:06] [80]\ttraining-aucpr:0.50447\n",
      "[16:29:06] [81]\ttraining-aucpr:0.50757\n",
      "[16:29:06] [82]\ttraining-aucpr:0.51188\n",
      "[16:29:06] [83]\ttraining-aucpr:0.51382\n",
      "[16:29:06] [84]\ttraining-aucpr:0.51757\n",
      "[16:29:07] [85]\ttraining-aucpr:0.52079\n",
      "[16:29:07] [86]\ttraining-aucpr:0.52475\n",
      "[16:29:07] [87]\ttraining-aucpr:0.52697\n",
      "[16:29:07] [88]\ttraining-aucpr:0.53059\n",
      "[16:29:07] [89]\ttraining-aucpr:0.53400\n",
      "[16:29:07] [90]\ttraining-aucpr:0.53885\n",
      "[16:29:07] [91]\ttraining-aucpr:0.54344\n",
      "[16:29:07] [92]\ttraining-aucpr:0.54584\n",
      "[16:29:07] [93]\ttraining-aucpr:0.54769\n",
      "[16:29:07] [94]\ttraining-aucpr:0.55109\n",
      "[16:29:07] [95]\ttraining-aucpr:0.55287\n",
      "[16:29:07] [96]\ttraining-aucpr:0.55699\n",
      "[16:29:07] [97]\ttraining-aucpr:0.55980\n",
      "[16:29:07] [98]\ttraining-aucpr:0.56212\n",
      "[16:29:07] [99]\ttraining-aucpr:0.56511\n",
      "[16:29:07] [100]\ttraining-aucpr:0.56885\n",
      "[16:29:07] [101]\ttraining-aucpr:0.57322\n",
      "[16:29:07] [102]\ttraining-aucpr:0.57561\n",
      "[16:29:07] [103]\ttraining-aucpr:0.57872\n",
      "[16:29:08] [104]\ttraining-aucpr:0.58038\n",
      "[16:29:08] [105]\ttraining-aucpr:0.58309\n",
      "[16:29:08] [106]\ttraining-aucpr:0.58482\n",
      "[16:29:08] [107]\ttraining-aucpr:0.58712\n",
      "[16:29:08] [108]\ttraining-aucpr:0.59076\n",
      "[16:29:08] [109]\ttraining-aucpr:0.59354\n",
      "[16:29:08] [110]\ttraining-aucpr:0.59838\n",
      "[16:29:08] [111]\ttraining-aucpr:0.60200\n",
      "[16:29:08] [112]\ttraining-aucpr:0.60662\n",
      "[16:29:08] [113]\ttraining-aucpr:0.60950\n",
      "[16:29:08] [114]\ttraining-aucpr:0.61354\n",
      "[16:29:08] [115]\ttraining-aucpr:0.61607\n",
      "[16:29:08] [116]\ttraining-aucpr:0.61897\n",
      "[16:29:08] [117]\ttraining-aucpr:0.62061\n",
      "[16:29:08] [118]\ttraining-aucpr:0.62388\n",
      "[16:29:08] [119]\ttraining-aucpr:0.62671\n",
      "[16:29:08] [120]\ttraining-aucpr:0.63019\n",
      "[16:29:08] [121]\ttraining-aucpr:0.63363\n",
      "[16:29:08] [122]\ttraining-aucpr:0.63952\n",
      "[16:29:08] [123]\ttraining-aucpr:0.64137\n",
      "[16:29:09] [124]\ttraining-aucpr:0.64447\n",
      "[16:29:09] [125]\ttraining-aucpr:0.64841\n",
      "[16:29:09] [126]\ttraining-aucpr:0.65203\n",
      "[16:29:09] [127]\ttraining-aucpr:0.65420\n",
      "[16:29:09] [128]\ttraining-aucpr:0.65752\n",
      "2025-07-28 16:29:10,370 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:29:10,550 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:29:11,238] Trial 13 finished with value: 0.14183096022490488 and parameters: {'max_depth': 10, 'learning_rate': 0.09596139670893554, 'n_estimators': 129, 'min_child_weight': 10, 'gamma': 0.985605201044127, 'subsample': 0.8585787245031457, 'colsample_bytree': 0.6207774664901697, 'reg_alpha': 0.49708539361085236, 'reg_lambda': 0.6855046081289342}. Best is trial 10 with value: 0.19120947778198935.\n",
      "2025-07-28 16:29:11,361 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.7817859664759539, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.6347413894605385, 'learning_rate': 0.13570390610987368, 'max_depth': 3, 'min_child_weight': 8, 'reg_alpha': 0.19338016393942253, 'reg_lambda': 0.38733255250772025, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9501945220447685, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 142}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:29:12,745 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:29:13] Task 0 got rank 0\n",
      "[16:29:13] Task 1 got rank 1\n",
      "[16:29:13] Task 3 got rank 3\n",
      "[16:29:13] Task 2 got rank 2\n",
      "[16:29:14] [0]\ttraining-aucpr:0.04710\n",
      "[16:29:14] [1]\ttraining-aucpr:0.04814\n",
      "[16:29:14] [2]\ttraining-aucpr:0.05790\n",
      "[16:29:14] [3]\ttraining-aucpr:0.06350\n",
      "[16:29:14] [4]\ttraining-aucpr:0.06375\n",
      "[16:29:14] [5]\ttraining-aucpr:0.06904\n",
      "[16:29:14] [6]\ttraining-aucpr:0.07203\n",
      "[16:29:14] [7]\ttraining-aucpr:0.07670\n",
      "[16:29:14] [8]\ttraining-aucpr:0.08514\n",
      "[16:29:14] [9]\ttraining-aucpr:0.09368\n",
      "[16:29:14] [10]\ttraining-aucpr:0.09438\n",
      "[16:29:14] [11]\ttraining-aucpr:0.09478\n",
      "[16:29:14] [12]\ttraining-aucpr:0.10248\n",
      "[16:29:14] [13]\ttraining-aucpr:0.10531\n",
      "[16:29:14] [14]\ttraining-aucpr:0.10853\n",
      "[16:29:14] [15]\ttraining-aucpr:0.10869\n",
      "[16:29:14] [16]\ttraining-aucpr:0.10975\n",
      "[16:29:14] [17]\ttraining-aucpr:0.11310\n",
      "[16:29:14] [18]\ttraining-aucpr:0.11397\n",
      "[16:29:14] [19]\ttraining-aucpr:0.11539\n",
      "[16:29:14] [20]\ttraining-aucpr:0.11826\n",
      "[16:29:14] [21]\ttraining-aucpr:0.11913\n",
      "[16:29:14] [22]\ttraining-aucpr:0.12135\n",
      "[16:29:14] [23]\ttraining-aucpr:0.12169\n",
      "[16:29:14] [24]\ttraining-aucpr:0.12453\n",
      "[16:29:14] [25]\ttraining-aucpr:0.12458\n",
      "[16:29:14] [26]\ttraining-aucpr:0.12624\n",
      "[16:29:14] [27]\ttraining-aucpr:0.12884\n",
      "[16:29:14] [28]\ttraining-aucpr:0.12940\n",
      "[16:29:14] [29]\ttraining-aucpr:0.13036\n",
      "[16:29:14] [30]\ttraining-aucpr:0.13174\n",
      "[16:29:14] [31]\ttraining-aucpr:0.13362\n",
      "[16:29:14] [32]\ttraining-aucpr:0.13571\n",
      "[16:29:14] [33]\ttraining-aucpr:0.13624\n",
      "[16:29:15] [34]\ttraining-aucpr:0.13631\n",
      "[16:29:15] [35]\ttraining-aucpr:0.13767\n",
      "[16:29:15] [36]\ttraining-aucpr:0.13919\n",
      "[16:29:15] [37]\ttraining-aucpr:0.13877\n",
      "[16:29:15] [38]\ttraining-aucpr:0.13923\n",
      "[16:29:15] [39]\ttraining-aucpr:0.14019\n",
      "[16:29:15] [40]\ttraining-aucpr:0.14134\n",
      "[16:29:15] [41]\ttraining-aucpr:0.14194\n",
      "[16:29:15] [42]\ttraining-aucpr:0.14353\n",
      "[16:29:15] [43]\ttraining-aucpr:0.14350\n",
      "[16:29:15] [44]\ttraining-aucpr:0.14408\n",
      "[16:29:15] [45]\ttraining-aucpr:0.14492\n",
      "[16:29:15] [46]\ttraining-aucpr:0.14631\n",
      "[16:29:15] [47]\ttraining-aucpr:0.14666\n",
      "[16:29:15] [48]\ttraining-aucpr:0.14733\n",
      "[16:29:15] [49]\ttraining-aucpr:0.14824\n",
      "[16:29:15] [50]\ttraining-aucpr:0.14895\n",
      "[16:29:15] [51]\ttraining-aucpr:0.15007\n",
      "[16:29:15] [52]\ttraining-aucpr:0.15085\n",
      "[16:29:15] [53]\ttraining-aucpr:0.15069\n",
      "[16:29:15] [54]\ttraining-aucpr:0.15140\n",
      "[16:29:15] [55]\ttraining-aucpr:0.15163\n",
      "[16:29:15] [56]\ttraining-aucpr:0.15206\n",
      "[16:29:15] [57]\ttraining-aucpr:0.15273\n",
      "[16:29:15] [58]\ttraining-aucpr:0.15331\n",
      "[16:29:15] [59]\ttraining-aucpr:0.15369\n",
      "[16:29:15] [60]\ttraining-aucpr:0.15420\n",
      "[16:29:15] [61]\ttraining-aucpr:0.15427\n",
      "[16:29:15] [62]\ttraining-aucpr:0.15518\n",
      "[16:29:15] [63]\ttraining-aucpr:0.15552\n",
      "[16:29:15] [64]\ttraining-aucpr:0.15628\n",
      "[16:29:15] [65]\ttraining-aucpr:0.15638\n",
      "[16:29:15] [66]\ttraining-aucpr:0.15673\n",
      "[16:29:15] [67]\ttraining-aucpr:0.15704\n",
      "[16:29:15] [68]\ttraining-aucpr:0.15748\n",
      "[16:29:15] [69]\ttraining-aucpr:0.15772\n",
      "[16:29:15] [70]\ttraining-aucpr:0.15814\n",
      "[16:29:15] [71]\ttraining-aucpr:0.15882\n",
      "[16:29:15] [72]\ttraining-aucpr:0.15923\n",
      "[16:29:15] [73]\ttraining-aucpr:0.15976\n",
      "[16:29:15] [74]\ttraining-aucpr:0.16042\n",
      "[16:29:15] [75]\ttraining-aucpr:0.16057\n",
      "[16:29:15] [76]\ttraining-aucpr:0.16068\n",
      "[16:29:15] [77]\ttraining-aucpr:0.16105\n",
      "[16:29:15] [78]\ttraining-aucpr:0.16150\n",
      "[16:29:16] [79]\ttraining-aucpr:0.16156\n",
      "[16:29:16] [80]\ttraining-aucpr:0.16192\n",
      "[16:29:16] [81]\ttraining-aucpr:0.16221\n",
      "[16:29:16] [82]\ttraining-aucpr:0.16231\n",
      "[16:29:16] [83]\ttraining-aucpr:0.16247\n",
      "[16:29:16] [84]\ttraining-aucpr:0.16273\n",
      "[16:29:16] [85]\ttraining-aucpr:0.16290\n",
      "[16:29:16] [86]\ttraining-aucpr:0.16309\n",
      "[16:29:16] [87]\ttraining-aucpr:0.16358\n",
      "[16:29:16] [88]\ttraining-aucpr:0.16371\n",
      "[16:29:16] [89]\ttraining-aucpr:0.16415\n",
      "[16:29:16] [90]\ttraining-aucpr:0.16449\n",
      "[16:29:16] [91]\ttraining-aucpr:0.16464\n",
      "[16:29:16] [92]\ttraining-aucpr:0.16477\n",
      "[16:29:16] [93]\ttraining-aucpr:0.16535\n",
      "[16:29:16] [94]\ttraining-aucpr:0.16557\n",
      "[16:29:16] [95]\ttraining-aucpr:0.16583\n",
      "[16:29:16] [96]\ttraining-aucpr:0.16586\n",
      "[16:29:16] [97]\ttraining-aucpr:0.16595\n",
      "[16:29:16] [98]\ttraining-aucpr:0.16600\n",
      "[16:29:16] [99]\ttraining-aucpr:0.16616\n",
      "[16:29:16] [100]\ttraining-aucpr:0.16636\n",
      "[16:29:16] [101]\ttraining-aucpr:0.16667\n",
      "[16:29:16] [102]\ttraining-aucpr:0.16697\n",
      "[16:29:16] [103]\ttraining-aucpr:0.16716\n",
      "[16:29:16] [104]\ttraining-aucpr:0.16747\n",
      "[16:29:16] [105]\ttraining-aucpr:0.16770\n",
      "[16:29:16] [106]\ttraining-aucpr:0.16789\n",
      "[16:29:16] [107]\ttraining-aucpr:0.16800\n",
      "[16:29:16] [108]\ttraining-aucpr:0.16808\n",
      "[16:29:16] [109]\ttraining-aucpr:0.16829\n",
      "[16:29:16] [110]\ttraining-aucpr:0.16846\n",
      "[16:29:16] [111]\ttraining-aucpr:0.16867\n",
      "[16:29:16] [112]\ttraining-aucpr:0.16885\n",
      "[16:29:16] [113]\ttraining-aucpr:0.16895\n",
      "[16:29:16] [114]\ttraining-aucpr:0.16905\n",
      "[16:29:16] [115]\ttraining-aucpr:0.16924\n",
      "[16:29:16] [116]\ttraining-aucpr:0.16933\n",
      "[16:29:16] [117]\ttraining-aucpr:0.16936\n",
      "[16:29:16] [118]\ttraining-aucpr:0.16961\n",
      "[16:29:16] [119]\ttraining-aucpr:0.16970\n",
      "[16:29:16] [120]\ttraining-aucpr:0.16990\n",
      "[16:29:16] [121]\ttraining-aucpr:0.17010\n",
      "[16:29:17] [122]\ttraining-aucpr:0.17025\n",
      "[16:29:17] [123]\ttraining-aucpr:0.17045\n",
      "[16:29:17] [124]\ttraining-aucpr:0.17071\n",
      "[16:29:17] [125]\ttraining-aucpr:0.17087\n",
      "[16:29:17] [126]\ttraining-aucpr:0.17103\n",
      "[16:29:17] [127]\ttraining-aucpr:0.17116\n",
      "[16:29:17] [128]\ttraining-aucpr:0.17121\n",
      "[16:29:17] [129]\ttraining-aucpr:0.17154\n",
      "[16:29:17] [130]\ttraining-aucpr:0.17163\n",
      "[16:29:17] [131]\ttraining-aucpr:0.17172\n",
      "[16:29:17] [132]\ttraining-aucpr:0.17184\n",
      "[16:29:17] [133]\ttraining-aucpr:0.17187\n",
      "[16:29:17] [134]\ttraining-aucpr:0.17189\n",
      "[16:29:17] [135]\ttraining-aucpr:0.17198\n",
      "[16:29:17] [136]\ttraining-aucpr:0.17198\n",
      "[16:29:17] [137]\ttraining-aucpr:0.17196\n",
      "[16:29:17] [138]\ttraining-aucpr:0.17205\n",
      "[16:29:17] [139]\ttraining-aucpr:0.17208\n",
      "[16:29:17] [140]\ttraining-aucpr:0.17217\n",
      "[16:29:17] [141]\ttraining-aucpr:0.17225\n",
      "2025-07-28 16:29:18,473 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:29:18,560 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:29:19,264] Trial 14 finished with value: 0.19427317857446047 and parameters: {'max_depth': 3, 'learning_rate': 0.13570390610987368, 'n_estimators': 142, 'min_child_weight': 8, 'gamma': 0.6347413894605385, 'subsample': 0.9501945220447685, 'colsample_bytree': 0.7817859664759539, 'reg_alpha': 0.19338016393942253, 'reg_lambda': 0.38733255250772025}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:29:19,386 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.803801124762104, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.5357665227123615, 'learning_rate': 0.05604787954969532, 'max_depth': 4, 'min_child_weight': 8, 'reg_alpha': 0.21381308130746754, 'reg_lambda': 0.3745078296021424, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9913430783889023, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 149}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:29:20,777 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:29:21] Task 1 got rank 1\n",
      "[16:29:21] Task 3 got rank 3[16:29:21] Task 0 got rank 0\n",
      "\n",
      "[16:29:21] Task 2 got rank 2\n",
      "[16:29:22] [0]\ttraining-aucpr:0.05563\n",
      "[16:29:22] [1]\ttraining-aucpr:0.06009\n",
      "[16:29:22] [2]\ttraining-aucpr:0.06649\n",
      "[16:29:22] [3]\ttraining-aucpr:0.06803\n",
      "[16:29:22] [4]\ttraining-aucpr:0.06813\n",
      "[16:29:22] [5]\ttraining-aucpr:0.07413\n",
      "[16:29:22] [6]\ttraining-aucpr:0.07481\n",
      "[16:29:22] [7]\ttraining-aucpr:0.07567\n",
      "[16:29:22] [8]\ttraining-aucpr:0.07933\n",
      "[16:29:22] [9]\ttraining-aucpr:0.08012\n",
      "[16:29:22] [10]\ttraining-aucpr:0.08054\n",
      "[16:29:22] [11]\ttraining-aucpr:0.08163\n",
      "[16:29:22] [12]\ttraining-aucpr:0.08229\n",
      "[16:29:22] [13]\ttraining-aucpr:0.08632\n",
      "[16:29:22] [14]\ttraining-aucpr:0.08961\n",
      "[16:29:22] [15]\ttraining-aucpr:0.08963\n",
      "[16:29:22] [16]\ttraining-aucpr:0.08995\n",
      "[16:29:22] [17]\ttraining-aucpr:0.09060\n",
      "[16:29:22] [18]\ttraining-aucpr:0.09147\n",
      "[16:29:22] [19]\ttraining-aucpr:0.09523\n",
      "[16:29:22] [20]\ttraining-aucpr:0.09516\n",
      "[16:29:22] [21]\ttraining-aucpr:0.09751\n",
      "[16:29:22] [22]\ttraining-aucpr:0.10140\n",
      "[16:29:22] [23]\ttraining-aucpr:0.10406\n",
      "[16:29:22] [24]\ttraining-aucpr:0.10603\n",
      "[16:29:22] [25]\ttraining-aucpr:0.10735\n",
      "[16:29:22] [26]\ttraining-aucpr:0.10985\n",
      "[16:29:22] [27]\ttraining-aucpr:0.11167\n",
      "[16:29:23] [28]\ttraining-aucpr:0.11201\n",
      "[16:29:23] [29]\ttraining-aucpr:0.11286\n",
      "[16:29:23] [30]\ttraining-aucpr:0.11459\n",
      "[16:29:23] [31]\ttraining-aucpr:0.11584\n",
      "[16:29:23] [32]\ttraining-aucpr:0.11625\n",
      "[16:29:23] [33]\ttraining-aucpr:0.11843\n",
      "[16:29:23] [34]\ttraining-aucpr:0.11911\n",
      "[16:29:23] [35]\ttraining-aucpr:0.12018\n",
      "[16:29:23] [36]\ttraining-aucpr:0.12034\n",
      "[16:29:23] [37]\ttraining-aucpr:0.12146\n",
      "[16:29:23] [38]\ttraining-aucpr:0.12176\n",
      "[16:29:23] [39]\ttraining-aucpr:0.12298\n",
      "[16:29:23] [40]\ttraining-aucpr:0.12379\n",
      "[16:29:23] [41]\ttraining-aucpr:0.12460\n",
      "[16:29:23] [42]\ttraining-aucpr:0.12680\n",
      "[16:29:23] [43]\ttraining-aucpr:0.12724\n",
      "[16:29:23] [44]\ttraining-aucpr:0.12737\n",
      "[16:29:23] [45]\ttraining-aucpr:0.12823\n",
      "[16:29:23] [46]\ttraining-aucpr:0.12906\n",
      "[16:29:23] [47]\ttraining-aucpr:0.12927\n",
      "[16:29:23] [48]\ttraining-aucpr:0.13039\n",
      "[16:29:23] [49]\ttraining-aucpr:0.13094\n",
      "[16:29:23] [50]\ttraining-aucpr:0.13126\n",
      "[16:29:23] [51]\ttraining-aucpr:0.13169\n",
      "[16:29:23] [52]\ttraining-aucpr:0.13175\n",
      "[16:29:23] [53]\ttraining-aucpr:0.13261\n",
      "[16:29:23] [54]\ttraining-aucpr:0.13419\n",
      "[16:29:23] [55]\ttraining-aucpr:0.13469\n",
      "[16:29:23] [56]\ttraining-aucpr:0.13579\n",
      "[16:29:23] [57]\ttraining-aucpr:0.13611\n",
      "[16:29:23] [58]\ttraining-aucpr:0.13659\n",
      "[16:29:23] [59]\ttraining-aucpr:0.13672\n",
      "[16:29:23] [60]\ttraining-aucpr:0.13751\n",
      "[16:29:23] [61]\ttraining-aucpr:0.13800\n",
      "[16:29:23] [62]\ttraining-aucpr:0.13877\n",
      "[16:29:23] [63]\ttraining-aucpr:0.13977\n",
      "[16:29:23] [64]\ttraining-aucpr:0.13986\n",
      "[16:29:23] [65]\ttraining-aucpr:0.14036\n",
      "[16:29:24] [66]\ttraining-aucpr:0.14124\n",
      "[16:29:24] [67]\ttraining-aucpr:0.14145\n",
      "[16:29:24] [68]\ttraining-aucpr:0.14221\n",
      "[16:29:24] [69]\ttraining-aucpr:0.14290\n",
      "[16:29:24] [70]\ttraining-aucpr:0.14357\n",
      "[16:29:24] [71]\ttraining-aucpr:0.14385\n",
      "[16:29:24] [72]\ttraining-aucpr:0.14446\n",
      "[16:29:24] [73]\ttraining-aucpr:0.14482\n",
      "[16:29:24] [74]\ttraining-aucpr:0.14497\n",
      "[16:29:24] [75]\ttraining-aucpr:0.14540\n",
      "[16:29:24] [76]\ttraining-aucpr:0.14603\n",
      "[16:29:24] [77]\ttraining-aucpr:0.14653\n",
      "[16:29:24] [78]\ttraining-aucpr:0.14685\n",
      "[16:29:24] [79]\ttraining-aucpr:0.14712\n",
      "[16:29:24] [80]\ttraining-aucpr:0.14770\n",
      "[16:29:24] [81]\ttraining-aucpr:0.14801\n",
      "[16:29:24] [82]\ttraining-aucpr:0.14882\n",
      "[16:29:24] [83]\ttraining-aucpr:0.14947\n",
      "[16:29:24] [84]\ttraining-aucpr:0.14989\n",
      "[16:29:24] [85]\ttraining-aucpr:0.15027\n",
      "[16:29:24] [86]\ttraining-aucpr:0.15059\n",
      "[16:29:24] [87]\ttraining-aucpr:0.15105\n",
      "[16:29:24] [88]\ttraining-aucpr:0.15138\n",
      "[16:29:24] [89]\ttraining-aucpr:0.15199\n",
      "[16:29:24] [90]\ttraining-aucpr:0.15211\n",
      "[16:29:24] [91]\ttraining-aucpr:0.15272\n",
      "[16:29:24] [92]\ttraining-aucpr:0.15289\n",
      "[16:29:24] [93]\ttraining-aucpr:0.15301\n",
      "[16:29:24] [94]\ttraining-aucpr:0.15351\n",
      "[16:29:24] [95]\ttraining-aucpr:0.15376\n",
      "[16:29:24] [96]\ttraining-aucpr:0.15415\n",
      "[16:29:24] [97]\ttraining-aucpr:0.15454\n",
      "[16:29:24] [98]\ttraining-aucpr:0.15496\n",
      "[16:29:24] [99]\ttraining-aucpr:0.15554\n",
      "[16:29:24] [100]\ttraining-aucpr:0.15598\n",
      "[16:29:24] [101]\ttraining-aucpr:0.15635\n",
      "[16:29:24] [102]\ttraining-aucpr:0.15637\n",
      "[16:29:25] [103]\ttraining-aucpr:0.15674\n",
      "[16:29:25] [104]\ttraining-aucpr:0.15719\n",
      "[16:29:25] [105]\ttraining-aucpr:0.15756\n",
      "[16:29:25] [106]\ttraining-aucpr:0.15808\n",
      "[16:29:25] [107]\ttraining-aucpr:0.15825\n",
      "[16:29:25] [108]\ttraining-aucpr:0.15851\n",
      "[16:29:25] [109]\ttraining-aucpr:0.15899\n",
      "[16:29:25] [110]\ttraining-aucpr:0.15910\n",
      "[16:29:25] [111]\ttraining-aucpr:0.15951\n",
      "[16:29:25] [112]\ttraining-aucpr:0.15972\n",
      "[16:29:25] [113]\ttraining-aucpr:0.16000\n",
      "[16:29:25] [114]\ttraining-aucpr:0.16024\n",
      "[16:29:25] [115]\ttraining-aucpr:0.16063\n",
      "[16:29:25] [116]\ttraining-aucpr:0.16103\n",
      "[16:29:25] [117]\ttraining-aucpr:0.16123\n",
      "[16:29:25] [118]\ttraining-aucpr:0.16148\n",
      "[16:29:25] [119]\ttraining-aucpr:0.16156\n",
      "[16:29:25] [120]\ttraining-aucpr:0.16188\n",
      "[16:29:25] [121]\ttraining-aucpr:0.16223\n",
      "[16:29:25] [122]\ttraining-aucpr:0.16238\n",
      "[16:29:25] [123]\ttraining-aucpr:0.16258\n",
      "[16:29:25] [124]\ttraining-aucpr:0.16291\n",
      "[16:29:25] [125]\ttraining-aucpr:0.16337\n",
      "[16:29:25] [126]\ttraining-aucpr:0.16369\n",
      "[16:29:25] [127]\ttraining-aucpr:0.16382\n",
      "[16:29:25] [128]\ttraining-aucpr:0.16400\n",
      "[16:29:25] [129]\ttraining-aucpr:0.16429\n",
      "[16:29:25] [130]\ttraining-aucpr:0.16467\n",
      "[16:29:25] [131]\ttraining-aucpr:0.16481\n",
      "[16:29:25] [132]\ttraining-aucpr:0.16500\n",
      "[16:29:25] [133]\ttraining-aucpr:0.16527\n",
      "[16:29:25] [134]\ttraining-aucpr:0.16553\n",
      "[16:29:25] [135]\ttraining-aucpr:0.16570\n",
      "[16:29:25] [136]\ttraining-aucpr:0.16590\n",
      "[16:29:25] [137]\ttraining-aucpr:0.16612\n",
      "[16:29:25] [138]\ttraining-aucpr:0.16638\n",
      "[16:29:25] [139]\ttraining-aucpr:0.16656\n",
      "[16:29:25] [140]\ttraining-aucpr:0.16674\n",
      "[16:29:25] [141]\ttraining-aucpr:0.16710\n",
      "[16:29:25] [142]\ttraining-aucpr:0.16738\n",
      "[16:29:25] [143]\ttraining-aucpr:0.16763\n",
      "[16:29:26] [144]\ttraining-aucpr:0.16773\n",
      "[16:29:26] [145]\ttraining-aucpr:0.16790\n",
      "[16:29:26] [146]\ttraining-aucpr:0.16813\n",
      "[16:29:26] [147]\ttraining-aucpr:0.16840\n",
      "[16:29:26] [148]\ttraining-aucpr:0.16862\n",
      "2025-07-28 16:29:27,137 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:29:27,232 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:29:27,929] Trial 15 finished with value: 0.18908339851355827 and parameters: {'max_depth': 4, 'learning_rate': 0.05604787954969532, 'n_estimators': 149, 'min_child_weight': 8, 'gamma': 0.5357665227123615, 'subsample': 0.9913430783889023, 'colsample_bytree': 0.803801124762104, 'reg_alpha': 0.21381308130746754, 'reg_lambda': 0.3745078296021424}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:29:28,068 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.8285726521637806, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.5712036246104564, 'learning_rate': 0.15345811193574893, 'max_depth': 4, 'min_child_weight': 8, 'reg_alpha': 0.42716888742799164, 'reg_lambda': 0.013179214380923399, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9298249375977756, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 96}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:29:29,456 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:29:30] Task 1 got rank 1\n",
      "[16:29:30] Task 2 got rank 2\n",
      "[16:29:30] Task 0 got rank 0\n",
      "[16:29:30] Task 3 got rank 3\n",
      "[16:29:31] [0]\ttraining-aucpr:0.05563\n",
      "[16:29:31] [1]\ttraining-aucpr:0.06027\n",
      "[16:29:31] [2]\ttraining-aucpr:0.06748\n",
      "[16:29:31] [3]\ttraining-aucpr:0.07510\n",
      "[16:29:31] [4]\ttraining-aucpr:0.08058\n",
      "[16:29:31] [5]\ttraining-aucpr:0.08166\n",
      "[16:29:31] [6]\ttraining-aucpr:0.09087\n",
      "[16:29:31] [7]\ttraining-aucpr:0.09346\n",
      "[16:29:31] [8]\ttraining-aucpr:0.10345\n",
      "[16:29:31] [9]\ttraining-aucpr:0.10870\n",
      "[16:29:31] [10]\ttraining-aucpr:0.11068\n",
      "[16:29:31] [11]\ttraining-aucpr:0.11307\n",
      "[16:29:31] [12]\ttraining-aucpr:0.11546\n",
      "[16:29:31] [13]\ttraining-aucpr:0.11892\n",
      "[16:29:31] [14]\ttraining-aucpr:0.12003\n",
      "[16:29:31] [15]\ttraining-aucpr:0.12029\n",
      "[16:29:31] [16]\ttraining-aucpr:0.12240\n",
      "[16:29:31] [17]\ttraining-aucpr:0.12563\n",
      "[16:29:31] [18]\ttraining-aucpr:0.12940\n",
      "[16:29:31] [19]\ttraining-aucpr:0.13128\n",
      "[16:29:31] [20]\ttraining-aucpr:0.13396\n",
      "[16:29:31] [21]\ttraining-aucpr:0.13426\n",
      "[16:29:31] [22]\ttraining-aucpr:0.13588\n",
      "[16:29:31] [23]\ttraining-aucpr:0.13698\n",
      "[16:29:31] [24]\ttraining-aucpr:0.13872\n",
      "[16:29:31] [25]\ttraining-aucpr:0.14075\n",
      "[16:29:31] [26]\ttraining-aucpr:0.14335\n",
      "[16:29:31] [27]\ttraining-aucpr:0.14452\n",
      "[16:29:31] [28]\ttraining-aucpr:0.14553\n",
      "[16:29:31] [29]\ttraining-aucpr:0.14684\n",
      "[16:29:31] [30]\ttraining-aucpr:0.14808\n",
      "[16:29:31] [31]\ttraining-aucpr:0.14900\n",
      "[16:29:31] [32]\ttraining-aucpr:0.15051\n",
      "[16:29:31] [33]\ttraining-aucpr:0.15168\n",
      "[16:29:31] [34]\ttraining-aucpr:0.15249\n",
      "[16:29:31] [35]\ttraining-aucpr:0.15305\n",
      "[16:29:31] [36]\ttraining-aucpr:0.15441\n",
      "[16:29:31] [37]\ttraining-aucpr:0.15565\n",
      "[16:29:31] [38]\ttraining-aucpr:0.15629\n",
      "[16:29:32] [39]\ttraining-aucpr:0.15697\n",
      "[16:29:32] [40]\ttraining-aucpr:0.15809\n",
      "[16:29:32] [41]\ttraining-aucpr:0.15880\n",
      "[16:29:32] [42]\ttraining-aucpr:0.15958\n",
      "[16:29:32] [43]\ttraining-aucpr:0.16011\n",
      "[16:29:32] [44]\ttraining-aucpr:0.16057\n",
      "[16:29:32] [45]\ttraining-aucpr:0.16107\n",
      "[16:29:32] [46]\ttraining-aucpr:0.16144\n",
      "[16:29:32] [47]\ttraining-aucpr:0.16223\n",
      "[16:29:32] [48]\ttraining-aucpr:0.16291\n",
      "[16:29:32] [49]\ttraining-aucpr:0.16324\n",
      "[16:29:32] [50]\ttraining-aucpr:0.16414\n",
      "[16:29:32] [51]\ttraining-aucpr:0.16499\n",
      "[16:29:32] [52]\ttraining-aucpr:0.16568\n",
      "[16:29:32] [53]\ttraining-aucpr:0.16592\n",
      "[16:29:32] [54]\ttraining-aucpr:0.16659\n",
      "[16:29:32] [55]\ttraining-aucpr:0.16728\n",
      "[16:29:32] [56]\ttraining-aucpr:0.16770\n",
      "[16:29:32] [57]\ttraining-aucpr:0.16807\n",
      "[16:29:32] [58]\ttraining-aucpr:0.16845\n",
      "[16:29:32] [59]\ttraining-aucpr:0.16883\n",
      "[16:29:32] [60]\ttraining-aucpr:0.16909\n",
      "[16:29:32] [61]\ttraining-aucpr:0.16935\n",
      "[16:29:32] [62]\ttraining-aucpr:0.16971\n",
      "[16:29:32] [63]\ttraining-aucpr:0.17008\n",
      "[16:29:32] [64]\ttraining-aucpr:0.17058\n",
      "[16:29:32] [65]\ttraining-aucpr:0.17087\n",
      "[16:29:32] [66]\ttraining-aucpr:0.17108\n",
      "[16:29:32] [67]\ttraining-aucpr:0.17132\n",
      "[16:29:32] [68]\ttraining-aucpr:0.17152\n",
      "[16:29:32] [69]\ttraining-aucpr:0.17191\n",
      "[16:29:32] [70]\ttraining-aucpr:0.17214\n",
      "[16:29:32] [71]\ttraining-aucpr:0.17231\n",
      "[16:29:32] [72]\ttraining-aucpr:0.17276\n",
      "[16:29:32] [73]\ttraining-aucpr:0.17289\n",
      "[16:29:32] [74]\ttraining-aucpr:0.17320\n",
      "[16:29:32] [75]\ttraining-aucpr:0.17405\n",
      "[16:29:32] [76]\ttraining-aucpr:0.17415\n",
      "[16:29:32] [77]\ttraining-aucpr:0.17430\n",
      "[16:29:33] [78]\ttraining-aucpr:0.17438\n",
      "[16:29:33] [79]\ttraining-aucpr:0.17449\n",
      "[16:29:33] [80]\ttraining-aucpr:0.17499\n",
      "[16:29:33] [81]\ttraining-aucpr:0.17522\n",
      "[16:29:33] [82]\ttraining-aucpr:0.17546\n",
      "[16:29:33] [83]\ttraining-aucpr:0.17554\n",
      "[16:29:33] [84]\ttraining-aucpr:0.17580\n",
      "[16:29:33] [85]\ttraining-aucpr:0.17587\n",
      "[16:29:33] [86]\ttraining-aucpr:0.17618\n",
      "[16:29:33] [87]\ttraining-aucpr:0.17625\n",
      "[16:29:33] [88]\ttraining-aucpr:0.17648\n",
      "[16:29:33] [89]\ttraining-aucpr:0.17688\n",
      "[16:29:33] [90]\ttraining-aucpr:0.17728\n",
      "[16:29:33] [91]\ttraining-aucpr:0.17749\n",
      "[16:29:33] [92]\ttraining-aucpr:0.17778\n",
      "[16:29:33] [93]\ttraining-aucpr:0.17801\n",
      "[16:29:33] [94]\ttraining-aucpr:0.17823\n",
      "[16:29:33] [95]\ttraining-aucpr:0.17831\n",
      "2025-07-28 16:29:34,421 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:29:34,507 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:29:35,198] Trial 16 finished with value: 0.19399482619055328 and parameters: {'max_depth': 4, 'learning_rate': 0.15345811193574893, 'n_estimators': 96, 'min_child_weight': 8, 'gamma': 0.5712036246104564, 'subsample': 0.9298249375977756, 'colsample_bytree': 0.8285726521637806, 'reg_alpha': 0.42716888742799164, 'reg_lambda': 0.013179214380923399}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:29:35,318 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.8008146866662705, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.5392727789146319, 'learning_rate': 0.16521895128705721, 'max_depth': 4, 'min_child_weight': 8, 'reg_alpha': 0.4671264349468485, 'reg_lambda': 0.025515512489219596, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9178393905836026, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 90}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:29:36,698 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:29:37] Task 1 got rank 1\n",
      "[16:29:37] Task 2 got rank 2\n",
      "[16:29:37] Task 0 got rank 0\n",
      "[16:29:37] Task 3 got rank 3\n",
      "[16:29:38] [0]\ttraining-aucpr:0.05562\n",
      "[16:29:38] [1]\ttraining-aucpr:0.06028\n",
      "[16:29:38] [2]\ttraining-aucpr:0.06760\n",
      "[16:29:38] [3]\ttraining-aucpr:0.07025\n",
      "[16:29:38] [4]\ttraining-aucpr:0.07666\n",
      "[16:29:38] [5]\ttraining-aucpr:0.08412\n",
      "[16:29:38] [6]\ttraining-aucpr:0.08963\n",
      "[16:29:38] [7]\ttraining-aucpr:0.09200\n",
      "[16:29:38] [8]\ttraining-aucpr:0.09658\n",
      "[16:29:38] [9]\ttraining-aucpr:0.10492\n",
      "[16:29:38] [10]\ttraining-aucpr:0.10635\n",
      "[16:29:38] [11]\ttraining-aucpr:0.11316\n",
      "[16:29:38] [12]\ttraining-aucpr:0.11760\n",
      "[16:29:38] [13]\ttraining-aucpr:0.12259\n",
      "[16:29:38] [14]\ttraining-aucpr:0.12543\n",
      "[16:29:38] [15]\ttraining-aucpr:0.12502\n",
      "[16:29:38] [16]\ttraining-aucpr:0.12719\n",
      "[16:29:38] [17]\ttraining-aucpr:0.13066\n",
      "[16:29:38] [18]\ttraining-aucpr:0.13186\n",
      "[16:29:38] [19]\ttraining-aucpr:0.13302\n",
      "[16:29:38] [20]\ttraining-aucpr:0.13463\n",
      "[16:29:38] [21]\ttraining-aucpr:0.13766\n",
      "[16:29:38] [22]\ttraining-aucpr:0.13926\n",
      "[16:29:38] [23]\ttraining-aucpr:0.14051\n",
      "[16:29:38] [24]\ttraining-aucpr:0.14243\n",
      "[16:29:38] [25]\ttraining-aucpr:0.14342\n",
      "[16:29:38] [26]\ttraining-aucpr:0.14425\n",
      "[16:29:38] [27]\ttraining-aucpr:0.14678\n",
      "[16:29:38] [28]\ttraining-aucpr:0.14734\n",
      "[16:29:39] [29]\ttraining-aucpr:0.14873\n",
      "[16:29:39] [30]\ttraining-aucpr:0.14997\n",
      "[16:29:39] [31]\ttraining-aucpr:0.15182\n",
      "[16:29:39] [32]\ttraining-aucpr:0.15331\n",
      "[16:29:39] [33]\ttraining-aucpr:0.15437\n",
      "[16:29:39] [34]\ttraining-aucpr:0.15478\n",
      "[16:29:39] [35]\ttraining-aucpr:0.15497\n",
      "[16:29:39] [36]\ttraining-aucpr:0.15637\n",
      "[16:29:39] [37]\ttraining-aucpr:0.15649\n",
      "[16:29:39] [38]\ttraining-aucpr:0.15799\n",
      "[16:29:39] [39]\ttraining-aucpr:0.15915\n",
      "[16:29:39] [40]\ttraining-aucpr:0.16005\n",
      "[16:29:39] [41]\ttraining-aucpr:0.16067\n",
      "[16:29:39] [42]\ttraining-aucpr:0.16199\n",
      "[16:29:39] [43]\ttraining-aucpr:0.16284\n",
      "[16:29:39] [44]\ttraining-aucpr:0.16312\n",
      "[16:29:39] [45]\ttraining-aucpr:0.16353\n",
      "[16:29:39] [46]\ttraining-aucpr:0.16402\n",
      "[16:29:39] [47]\ttraining-aucpr:0.16464\n",
      "[16:29:39] [48]\ttraining-aucpr:0.16570\n",
      "[16:29:39] [49]\ttraining-aucpr:0.16614\n",
      "[16:29:39] [50]\ttraining-aucpr:0.16652\n",
      "[16:29:39] [51]\ttraining-aucpr:0.16705\n",
      "[16:29:39] [52]\ttraining-aucpr:0.16713\n",
      "[16:29:39] [53]\ttraining-aucpr:0.16761\n",
      "[16:29:39] [54]\ttraining-aucpr:0.16824\n",
      "[16:29:39] [55]\ttraining-aucpr:0.16886\n",
      "[16:29:39] [56]\ttraining-aucpr:0.16953\n",
      "[16:29:39] [57]\ttraining-aucpr:0.17009\n",
      "[16:29:39] [58]\ttraining-aucpr:0.17024\n",
      "[16:29:39] [59]\ttraining-aucpr:0.17051\n",
      "[16:29:39] [60]\ttraining-aucpr:0.17083\n",
      "[16:29:39] [61]\ttraining-aucpr:0.17125\n",
      "[16:29:39] [62]\ttraining-aucpr:0.17166\n",
      "[16:29:39] [63]\ttraining-aucpr:0.17209\n",
      "[16:29:39] [64]\ttraining-aucpr:0.17228\n",
      "[16:29:39] [65]\ttraining-aucpr:0.17278\n",
      "[16:29:39] [66]\ttraining-aucpr:0.17316\n",
      "[16:29:39] [67]\ttraining-aucpr:0.17367\n",
      "[16:29:40] [68]\ttraining-aucpr:0.17397\n",
      "[16:29:40] [69]\ttraining-aucpr:0.17421\n",
      "[16:29:40] [70]\ttraining-aucpr:0.17434\n",
      "[16:29:40] [71]\ttraining-aucpr:0.17485\n",
      "[16:29:40] [72]\ttraining-aucpr:0.17519\n",
      "[16:29:40] [73]\ttraining-aucpr:0.17529\n",
      "[16:29:40] [74]\ttraining-aucpr:0.17565\n",
      "[16:29:40] [75]\ttraining-aucpr:0.17611\n",
      "[16:29:40] [76]\ttraining-aucpr:0.17632\n",
      "[16:29:40] [77]\ttraining-aucpr:0.17647\n",
      "[16:29:40] [78]\ttraining-aucpr:0.17677\n",
      "[16:29:40] [79]\ttraining-aucpr:0.17708\n",
      "[16:29:40] [80]\ttraining-aucpr:0.17742\n",
      "[16:29:40] [81]\ttraining-aucpr:0.17783\n",
      "[16:29:40] [82]\ttraining-aucpr:0.17782\n",
      "[16:29:40] [83]\ttraining-aucpr:0.17786\n",
      "[16:29:40] [84]\ttraining-aucpr:0.17802\n",
      "[16:29:40] [85]\ttraining-aucpr:0.17860\n",
      "[16:29:40] [86]\ttraining-aucpr:0.17875\n",
      "[16:29:40] [87]\ttraining-aucpr:0.17893\n",
      "[16:29:40] [88]\ttraining-aucpr:0.17899\n",
      "[16:29:40] [89]\ttraining-aucpr:0.17947\n",
      "2025-07-28 16:29:41,540 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:29:41,634 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:29:42,338] Trial 17 finished with value: 0.1930331726644431 and parameters: {'max_depth': 4, 'learning_rate': 0.16521895128705721, 'n_estimators': 90, 'min_child_weight': 8, 'gamma': 0.5392727789146319, 'subsample': 0.9178393905836026, 'colsample_bytree': 0.8008146866662705, 'reg_alpha': 0.4671264349468485, 'reg_lambda': 0.025515512489219596}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:29:42,443 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.8825680334098686, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.66872322009866, 'learning_rate': 0.14982807062764944, 'max_depth': 4, 'min_child_weight': 4, 'reg_alpha': 0.6447321685572042, 'reg_lambda': 0.020046606182420563, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9277987793749347, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 96}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:29:43,829 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:29:44] Task 0 got rank 0\n",
      "[16:29:44] Task 2 got rank 2\n",
      "[16:29:44] Task 1 got rank 1\n",
      "[16:29:44] Task 3 got rank 3\n",
      "[16:29:45] [0]\ttraining-aucpr:0.05563\n",
      "[16:29:45] [1]\ttraining-aucpr:0.06027\n",
      "[16:29:45] [2]\ttraining-aucpr:0.06937\n",
      "[16:29:45] [3]\ttraining-aucpr:0.07598\n",
      "[16:29:45] [4]\ttraining-aucpr:0.08184\n",
      "[16:29:45] [5]\ttraining-aucpr:0.08245\n",
      "[16:29:45] [6]\ttraining-aucpr:0.08363\n",
      "[16:29:45] [7]\ttraining-aucpr:0.09079\n",
      "[16:29:45] [8]\ttraining-aucpr:0.10166\n",
      "[16:29:45] [9]\ttraining-aucpr:0.10765\n",
      "[16:29:45] [10]\ttraining-aucpr:0.11011\n",
      "[16:29:45] [11]\ttraining-aucpr:0.11044\n",
      "[16:29:45] [12]\ttraining-aucpr:0.11369\n",
      "[16:29:45] [13]\ttraining-aucpr:0.11668\n",
      "[16:29:45] [14]\ttraining-aucpr:0.12079\n",
      "[16:29:45] [15]\ttraining-aucpr:0.12282\n",
      "[16:29:45] [16]\ttraining-aucpr:0.12565\n",
      "[16:29:45] [17]\ttraining-aucpr:0.12786\n",
      "[16:29:45] [18]\ttraining-aucpr:0.13070\n",
      "[16:29:45] [19]\ttraining-aucpr:0.13097\n",
      "[16:29:45] [20]\ttraining-aucpr:0.13227\n",
      "[16:29:45] [21]\ttraining-aucpr:0.13430\n",
      "[16:29:45] [22]\ttraining-aucpr:0.13443\n",
      "[16:29:45] [23]\ttraining-aucpr:0.13633\n",
      "[16:29:45] [24]\ttraining-aucpr:0.13720\n",
      "[16:29:46] [25]\ttraining-aucpr:0.13840\n",
      "[16:29:46] [26]\ttraining-aucpr:0.14024\n",
      "[16:29:46] [27]\ttraining-aucpr:0.14256\n",
      "[16:29:46] [28]\ttraining-aucpr:0.14262\n",
      "[16:29:46] [29]\ttraining-aucpr:0.14442\n",
      "[16:29:46] [30]\ttraining-aucpr:0.14589\n",
      "[16:29:46] [31]\ttraining-aucpr:0.14750\n",
      "[16:29:46] [32]\ttraining-aucpr:0.14914\n",
      "[16:29:46] [33]\ttraining-aucpr:0.14960\n",
      "[16:29:46] [34]\ttraining-aucpr:0.15121\n",
      "[16:29:46] [35]\ttraining-aucpr:0.15161\n",
      "[16:29:46] [36]\ttraining-aucpr:0.15317\n",
      "[16:29:46] [37]\ttraining-aucpr:0.15410\n",
      "[16:29:46] [38]\ttraining-aucpr:0.15488\n",
      "[16:29:46] [39]\ttraining-aucpr:0.15564\n",
      "[16:29:46] [40]\ttraining-aucpr:0.15626\n",
      "[16:29:46] [41]\ttraining-aucpr:0.15680\n",
      "[16:29:46] [42]\ttraining-aucpr:0.15748\n",
      "[16:29:46] [43]\ttraining-aucpr:0.15819\n",
      "[16:29:46] [44]\ttraining-aucpr:0.15895\n",
      "[16:29:46] [45]\ttraining-aucpr:0.15934\n",
      "[16:29:46] [46]\ttraining-aucpr:0.16010\n",
      "[16:29:46] [47]\ttraining-aucpr:0.16103\n",
      "[16:29:46] [48]\ttraining-aucpr:0.16198\n",
      "[16:29:46] [49]\ttraining-aucpr:0.16247\n",
      "[16:29:46] [50]\ttraining-aucpr:0.16301\n",
      "[16:29:46] [51]\ttraining-aucpr:0.16374\n",
      "[16:29:46] [52]\ttraining-aucpr:0.16449\n",
      "[16:29:46] [53]\ttraining-aucpr:0.16521\n",
      "[16:29:46] [54]\ttraining-aucpr:0.16571\n",
      "[16:29:46] [55]\ttraining-aucpr:0.16635\n",
      "[16:29:46] [56]\ttraining-aucpr:0.16717\n",
      "[16:29:46] [57]\ttraining-aucpr:0.16762\n",
      "[16:29:46] [58]\ttraining-aucpr:0.16809\n",
      "[16:29:46] [59]\ttraining-aucpr:0.16833\n",
      "[16:29:46] [60]\ttraining-aucpr:0.16848\n",
      "[16:29:46] [61]\ttraining-aucpr:0.16920\n",
      "[16:29:46] [62]\ttraining-aucpr:0.16950\n",
      "[16:29:47] [63]\ttraining-aucpr:0.16962\n",
      "[16:29:47] [64]\ttraining-aucpr:0.16995\n",
      "[16:29:47] [65]\ttraining-aucpr:0.17010\n",
      "[16:29:47] [66]\ttraining-aucpr:0.17018\n",
      "[16:29:47] [67]\ttraining-aucpr:0.17027\n",
      "[16:29:47] [68]\ttraining-aucpr:0.17055\n",
      "[16:29:47] [69]\ttraining-aucpr:0.17080\n",
      "[16:29:47] [70]\ttraining-aucpr:0.17110\n",
      "[16:29:47] [71]\ttraining-aucpr:0.17172\n",
      "[16:29:47] [72]\ttraining-aucpr:0.17233\n",
      "[16:29:47] [73]\ttraining-aucpr:0.17238\n",
      "[16:29:47] [74]\ttraining-aucpr:0.17278\n",
      "[16:29:47] [75]\ttraining-aucpr:0.17316\n",
      "[16:29:47] [76]\ttraining-aucpr:0.17336\n",
      "[16:29:47] [77]\ttraining-aucpr:0.17360\n",
      "[16:29:47] [78]\ttraining-aucpr:0.17403\n",
      "[16:29:47] [79]\ttraining-aucpr:0.17431\n",
      "[16:29:47] [80]\ttraining-aucpr:0.17489\n",
      "[16:29:47] [81]\ttraining-aucpr:0.17512\n",
      "[16:29:47] [82]\ttraining-aucpr:0.17539\n",
      "[16:29:47] [83]\ttraining-aucpr:0.17558\n",
      "[16:29:47] [84]\ttraining-aucpr:0.17586\n",
      "[16:29:47] [85]\ttraining-aucpr:0.17593\n",
      "[16:29:47] [86]\ttraining-aucpr:0.17600\n",
      "[16:29:47] [87]\ttraining-aucpr:0.17645\n",
      "[16:29:47] [88]\ttraining-aucpr:0.17659\n",
      "[16:29:47] [89]\ttraining-aucpr:0.17657\n",
      "[16:29:47] [90]\ttraining-aucpr:0.17672\n",
      "[16:29:47] [91]\ttraining-aucpr:0.17705\n",
      "[16:29:47] [92]\ttraining-aucpr:0.17704\n",
      "[16:29:47] [93]\ttraining-aucpr:0.17727\n",
      "[16:29:47] [94]\ttraining-aucpr:0.17753\n",
      "[16:29:47] [95]\ttraining-aucpr:0.17754\n",
      "2025-07-28 16:29:48,842 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:29:48,924 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:29:49,606] Trial 18 finished with value: 0.19367633541279064 and parameters: {'max_depth': 4, 'learning_rate': 0.14982807062764944, 'n_estimators': 96, 'min_child_weight': 4, 'gamma': 0.66872322009866, 'subsample': 0.9277987793749347, 'colsample_bytree': 0.8825680334098686, 'reg_alpha': 0.6447321685572042, 'reg_lambda': 0.020046606182420563}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:29:49,710 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.8707060038766514, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.4102204225893098, 'learning_rate': 0.059768170013458806, 'max_depth': 10, 'min_child_weight': 7, 'reg_alpha': 0.4030663466130188, 'reg_lambda': 0.3144784105504465, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.765298451176346, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 150}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:29:51,089 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:29:52] Task 1 got rank 1\n",
      "[16:29:52] Task 2 got rank 2\n",
      "[16:29:52] Task 3 got rank 3\n",
      "[16:29:52] Task 0 got rank 0\n",
      "[16:29:52] [0]\ttraining-aucpr:0.10713\n",
      "[16:29:52] [1]\ttraining-aucpr:0.13689\n",
      "[16:29:52] [2]\ttraining-aucpr:0.15059\n",
      "[16:29:52] [3]\ttraining-aucpr:0.15705\n",
      "[16:29:53] [4]\ttraining-aucpr:0.16227\n",
      "[16:29:53] [5]\ttraining-aucpr:0.16522\n",
      "[16:29:53] [6]\ttraining-aucpr:0.17033\n",
      "[16:29:53] [7]\ttraining-aucpr:0.17553\n",
      "[16:29:53] [8]\ttraining-aucpr:0.17613\n",
      "[16:29:53] [9]\ttraining-aucpr:0.17901\n",
      "[16:29:53] [10]\ttraining-aucpr:0.18264\n",
      "[16:29:53] [11]\ttraining-aucpr:0.18691\n",
      "[16:29:53] [12]\ttraining-aucpr:0.18806\n",
      "[16:29:53] [13]\ttraining-aucpr:0.19527\n",
      "[16:29:53] [14]\ttraining-aucpr:0.19811\n",
      "[16:29:53] [15]\ttraining-aucpr:0.20136\n",
      "[16:29:53] [16]\ttraining-aucpr:0.20435\n",
      "[16:29:54] [17]\ttraining-aucpr:0.20752\n",
      "[16:29:54] [18]\ttraining-aucpr:0.21252\n",
      "[16:29:54] [19]\ttraining-aucpr:0.21935\n",
      "[16:29:54] [20]\ttraining-aucpr:0.22256\n",
      "[16:29:54] [21]\ttraining-aucpr:0.22697\n",
      "[16:29:54] [22]\ttraining-aucpr:0.23088\n",
      "[16:29:54] [23]\ttraining-aucpr:0.23278\n",
      "[16:29:54] [24]\ttraining-aucpr:0.23671\n",
      "[16:29:54] [25]\ttraining-aucpr:0.24033\n",
      "[16:29:54] [26]\ttraining-aucpr:0.24317\n",
      "[16:29:54] [27]\ttraining-aucpr:0.24758\n",
      "[16:29:54] [28]\ttraining-aucpr:0.24876\n",
      "[16:29:54] [29]\ttraining-aucpr:0.25321\n",
      "[16:29:54] [30]\ttraining-aucpr:0.25531\n",
      "[16:29:55] [31]\ttraining-aucpr:0.25928\n",
      "[16:29:55] [32]\ttraining-aucpr:0.26187\n",
      "[16:29:55] [33]\ttraining-aucpr:0.26572\n",
      "[16:29:55] [34]\ttraining-aucpr:0.26820\n",
      "[16:29:55] [35]\ttraining-aucpr:0.27139\n",
      "[16:29:55] [36]\ttraining-aucpr:0.27520\n",
      "[16:29:55] [37]\ttraining-aucpr:0.27847\n",
      "[16:29:55] [38]\ttraining-aucpr:0.28176\n",
      "[16:29:55] [39]\ttraining-aucpr:0.28505\n",
      "[16:29:55] [40]\ttraining-aucpr:0.29039\n",
      "[16:29:55] [41]\ttraining-aucpr:0.29342\n",
      "[16:29:55] [42]\ttraining-aucpr:0.29768\n",
      "[16:29:55] [43]\ttraining-aucpr:0.30148\n",
      "[16:29:56] [44]\ttraining-aucpr:0.30462\n",
      "[16:29:56] [45]\ttraining-aucpr:0.30851\n",
      "[16:29:56] [46]\ttraining-aucpr:0.31017\n",
      "[16:29:56] [47]\ttraining-aucpr:0.31304\n",
      "[16:29:56] [48]\ttraining-aucpr:0.31635\n",
      "[16:29:56] [49]\ttraining-aucpr:0.31809\n",
      "[16:29:56] [50]\ttraining-aucpr:0.32158\n",
      "[16:29:56] [51]\ttraining-aucpr:0.32457\n",
      "[16:29:56] [52]\ttraining-aucpr:0.32782\n",
      "[16:29:56] [53]\ttraining-aucpr:0.32979\n",
      "[16:29:56] [54]\ttraining-aucpr:0.33404\n",
      "[16:29:56] [55]\ttraining-aucpr:0.33719\n",
      "[16:29:56] [56]\ttraining-aucpr:0.34068\n",
      "[16:29:56] [57]\ttraining-aucpr:0.34388\n",
      "[16:29:56] [58]\ttraining-aucpr:0.34618\n",
      "[16:29:57] [59]\ttraining-aucpr:0.34897\n",
      "[16:29:57] [60]\ttraining-aucpr:0.35233\n",
      "[16:29:57] [61]\ttraining-aucpr:0.35461\n",
      "[16:29:57] [62]\ttraining-aucpr:0.35798\n",
      "[16:29:57] [63]\ttraining-aucpr:0.36006\n",
      "[16:29:57] [64]\ttraining-aucpr:0.36333\n",
      "[16:29:57] [65]\ttraining-aucpr:0.36644\n",
      "[16:29:57] [66]\ttraining-aucpr:0.36860\n",
      "[16:29:57] [67]\ttraining-aucpr:0.37167\n",
      "[16:29:57] [68]\ttraining-aucpr:0.37461\n",
      "[16:29:57] [69]\ttraining-aucpr:0.37713\n",
      "[16:29:57] [70]\ttraining-aucpr:0.38023\n",
      "[16:29:57] [71]\ttraining-aucpr:0.38313\n",
      "[16:29:57] [72]\ttraining-aucpr:0.38579\n",
      "[16:29:57] [73]\ttraining-aucpr:0.38788\n",
      "[16:29:57] [74]\ttraining-aucpr:0.39011\n",
      "[16:29:58] [75]\ttraining-aucpr:0.39293\n",
      "[16:29:58] [76]\ttraining-aucpr:0.39544\n",
      "[16:29:58] [77]\ttraining-aucpr:0.39807\n",
      "[16:29:58] [78]\ttraining-aucpr:0.39997\n",
      "[16:29:58] [79]\ttraining-aucpr:0.40370\n",
      "[16:29:58] [80]\ttraining-aucpr:0.40550\n",
      "[16:29:58] [81]\ttraining-aucpr:0.40777\n",
      "[16:29:58] [82]\ttraining-aucpr:0.41045\n",
      "[16:29:58] [83]\ttraining-aucpr:0.41350\n",
      "[16:29:58] [84]\ttraining-aucpr:0.41574\n",
      "[16:29:58] [85]\ttraining-aucpr:0.41865\n",
      "[16:29:58] [86]\ttraining-aucpr:0.42159\n",
      "[16:29:58] [87]\ttraining-aucpr:0.42435\n",
      "[16:29:58] [88]\ttraining-aucpr:0.42678\n",
      "[16:29:58] [89]\ttraining-aucpr:0.42984\n",
      "[16:29:58] [90]\ttraining-aucpr:0.43140\n",
      "[16:29:59] [91]\ttraining-aucpr:0.43307\n",
      "[16:29:59] [92]\ttraining-aucpr:0.43643\n",
      "[16:29:59] [93]\ttraining-aucpr:0.43813\n",
      "[16:29:59] [94]\ttraining-aucpr:0.44058\n",
      "[16:29:59] [95]\ttraining-aucpr:0.44224\n",
      "[16:29:59] [96]\ttraining-aucpr:0.44458\n",
      "[16:29:59] [97]\ttraining-aucpr:0.44665\n",
      "[16:29:59] [98]\ttraining-aucpr:0.44953\n",
      "[16:29:59] [99]\ttraining-aucpr:0.45211\n",
      "[16:29:59] [100]\ttraining-aucpr:0.45434\n",
      "[16:29:59] [101]\ttraining-aucpr:0.45705\n",
      "[16:29:59] [102]\ttraining-aucpr:0.45861\n",
      "[16:29:59] [103]\ttraining-aucpr:0.46055\n",
      "[16:29:59] [104]\ttraining-aucpr:0.46288\n",
      "[16:29:59] [105]\ttraining-aucpr:0.46516\n",
      "[16:29:59] [106]\ttraining-aucpr:0.46707\n",
      "[16:29:59] [107]\ttraining-aucpr:0.46886\n",
      "[16:29:59] [108]\ttraining-aucpr:0.47244\n",
      "[16:30:00] [109]\ttraining-aucpr:0.47482\n",
      "[16:30:00] [110]\ttraining-aucpr:0.47733\n",
      "[16:30:00] [111]\ttraining-aucpr:0.47998\n",
      "[16:30:00] [112]\ttraining-aucpr:0.48176\n",
      "[16:30:00] [113]\ttraining-aucpr:0.48519\n",
      "[16:30:00] [114]\ttraining-aucpr:0.48771\n",
      "[16:30:00] [115]\ttraining-aucpr:0.48944\n",
      "[16:30:00] [116]\ttraining-aucpr:0.49298\n",
      "[16:30:00] [117]\ttraining-aucpr:0.49511\n",
      "[16:30:00] [118]\ttraining-aucpr:0.49691\n",
      "[16:30:00] [119]\ttraining-aucpr:0.49871\n",
      "[16:30:00] [120]\ttraining-aucpr:0.50118\n",
      "[16:30:00] [121]\ttraining-aucpr:0.50333\n",
      "[16:30:00] [122]\ttraining-aucpr:0.50549\n",
      "[16:30:00] [123]\ttraining-aucpr:0.50824\n",
      "[16:30:00] [124]\ttraining-aucpr:0.50997\n",
      "[16:30:00] [125]\ttraining-aucpr:0.51215\n",
      "[16:30:01] [126]\ttraining-aucpr:0.51340\n",
      "[16:30:01] [127]\ttraining-aucpr:0.51598\n",
      "[16:30:01] [128]\ttraining-aucpr:0.51881\n",
      "[16:30:01] [129]\ttraining-aucpr:0.52067\n",
      "[16:30:01] [130]\ttraining-aucpr:0.52267\n",
      "[16:30:01] [131]\ttraining-aucpr:0.52453\n",
      "[16:30:01] [132]\ttraining-aucpr:0.52626\n",
      "[16:30:01] [133]\ttraining-aucpr:0.52883\n",
      "[16:30:01] [134]\ttraining-aucpr:0.53133\n",
      "[16:30:01] [135]\ttraining-aucpr:0.53306\n",
      "[16:30:01] [136]\ttraining-aucpr:0.53492\n",
      "[16:30:01] [137]\ttraining-aucpr:0.53630\n",
      "[16:30:01] [138]\ttraining-aucpr:0.53822\n",
      "[16:30:01] [139]\ttraining-aucpr:0.53988\n",
      "[16:30:01] [140]\ttraining-aucpr:0.54226\n",
      "[16:30:01] [141]\ttraining-aucpr:0.54455\n",
      "[16:30:01] [142]\ttraining-aucpr:0.54636\n",
      "[16:30:01] [143]\ttraining-aucpr:0.54929\n",
      "[16:30:02] [144]\ttraining-aucpr:0.55073\n",
      "[16:30:02] [145]\ttraining-aucpr:0.55262\n",
      "[16:30:02] [146]\ttraining-aucpr:0.55425\n",
      "[16:30:02] [147]\ttraining-aucpr:0.55789\n",
      "[16:30:02] [148]\ttraining-aucpr:0.55962\n",
      "[16:30:02] [149]\ttraining-aucpr:0.56153\n",
      "2025-07-28 16:30:03,378 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:30:03,567 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:30:04,326] Trial 19 finished with value: 0.1386678341835943 and parameters: {'max_depth': 10, 'learning_rate': 0.059768170013458806, 'n_estimators': 150, 'min_child_weight': 7, 'gamma': 0.4102204225893098, 'subsample': 0.765298451176346, 'colsample_bytree': 0.8707060038766514, 'reg_alpha': 0.4030663466130188, 'reg_lambda': 0.3144784105504465}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:30:04,438 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.7643526548893316, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.6510402168377807, 'learning_rate': 0.29175835174865433, 'max_depth': 4, 'min_child_weight': 7, 'reg_alpha': 0.6151146042671063, 'reg_lambda': 0.117082353175192, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9312978390099437, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 265}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:30:05,830 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:30:06] Task 0 got rank 0\n",
      "[16:30:06] Task 2 got rank 2\n",
      "[16:30:06] Task 1 got rank 1\n",
      "[16:30:06] Task 3 got rank 3\n",
      "[16:30:07] [0]\ttraining-aucpr:0.05560\n",
      "[16:30:07] [1]\ttraining-aucpr:0.06047\n",
      "[16:30:07] [2]\ttraining-aucpr:0.07729\n",
      "[16:30:07] [3]\ttraining-aucpr:0.08445\n",
      "[16:30:07] [4]\ttraining-aucpr:0.09407\n",
      "[16:30:07] [5]\ttraining-aucpr:0.10830\n",
      "[16:30:07] [6]\ttraining-aucpr:0.11787\n",
      "[16:30:07] [7]\ttraining-aucpr:0.12104\n",
      "[16:30:07] [8]\ttraining-aucpr:0.12193\n",
      "[16:30:07] [9]\ttraining-aucpr:0.12733\n",
      "[16:30:07] [10]\ttraining-aucpr:0.13015\n",
      "[16:30:07] [11]\ttraining-aucpr:0.12978\n",
      "[16:30:07] [12]\ttraining-aucpr:0.13408\n",
      "[16:30:07] [13]\ttraining-aucpr:0.13862\n",
      "[16:30:07] [14]\ttraining-aucpr:0.14349\n",
      "[16:30:07] [15]\ttraining-aucpr:0.14420\n",
      "[16:30:07] [16]\ttraining-aucpr:0.14540\n",
      "[16:30:07] [17]\ttraining-aucpr:0.14691\n",
      "[16:30:07] [18]\ttraining-aucpr:0.14826\n",
      "[16:30:07] [19]\ttraining-aucpr:0.15111\n",
      "[16:30:07] [20]\ttraining-aucpr:0.15307\n",
      "[16:30:07] [21]\ttraining-aucpr:0.15471\n",
      "[16:30:07] [22]\ttraining-aucpr:0.15634\n",
      "[16:30:07] [23]\ttraining-aucpr:0.15721\n",
      "[16:30:08] [24]\ttraining-aucpr:0.15811\n",
      "[16:30:08] [25]\ttraining-aucpr:0.15881\n",
      "[16:30:08] [26]\ttraining-aucpr:0.15972\n",
      "[16:30:08] [27]\ttraining-aucpr:0.16114\n",
      "[16:30:08] [28]\ttraining-aucpr:0.16180\n",
      "[16:30:08] [29]\ttraining-aucpr:0.16197\n",
      "[16:30:08] [30]\ttraining-aucpr:0.16341\n",
      "[16:30:08] [31]\ttraining-aucpr:0.16451\n",
      "[16:30:08] [32]\ttraining-aucpr:0.16524\n",
      "[16:30:08] [33]\ttraining-aucpr:0.16599\n",
      "[16:30:08] [34]\ttraining-aucpr:0.16648\n",
      "[16:30:08] [35]\ttraining-aucpr:0.16734\n",
      "[16:30:08] [36]\ttraining-aucpr:0.16787\n",
      "[16:30:08] [37]\ttraining-aucpr:0.16816\n",
      "[16:30:08] [38]\ttraining-aucpr:0.16866\n",
      "[16:30:08] [39]\ttraining-aucpr:0.16899\n",
      "[16:30:08] [40]\ttraining-aucpr:0.16993\n",
      "[16:30:08] [41]\ttraining-aucpr:0.17025\n",
      "[16:30:08] [42]\ttraining-aucpr:0.17052\n",
      "[16:30:08] [43]\ttraining-aucpr:0.17061\n",
      "[16:30:08] [44]\ttraining-aucpr:0.17109\n",
      "[16:30:08] [45]\ttraining-aucpr:0.17182\n",
      "[16:30:08] [46]\ttraining-aucpr:0.17251\n",
      "[16:30:08] [47]\ttraining-aucpr:0.17266\n",
      "[16:30:08] [48]\ttraining-aucpr:0.17271\n",
      "[16:30:08] [49]\ttraining-aucpr:0.17331\n",
      "[16:30:08] [50]\ttraining-aucpr:0.17372\n",
      "[16:30:08] [51]\ttraining-aucpr:0.17378\n",
      "[16:30:08] [52]\ttraining-aucpr:0.17409\n",
      "[16:30:08] [53]\ttraining-aucpr:0.17423\n",
      "[16:30:08] [54]\ttraining-aucpr:0.17480\n",
      "[16:30:08] [55]\ttraining-aucpr:0.17512\n",
      "[16:30:08] [56]\ttraining-aucpr:0.17537\n",
      "[16:30:08] [57]\ttraining-aucpr:0.17559\n",
      "[16:30:08] [58]\ttraining-aucpr:0.17586\n",
      "[16:30:08] [59]\ttraining-aucpr:0.17589\n",
      "[16:30:08] [60]\ttraining-aucpr:0.17588\n",
      "[16:30:08] [61]\ttraining-aucpr:0.17622\n",
      "[16:30:08] [62]\ttraining-aucpr:0.17651\n",
      "[16:30:09] [63]\ttraining-aucpr:0.17712\n",
      "[16:30:09] [64]\ttraining-aucpr:0.17772\n",
      "[16:30:09] [65]\ttraining-aucpr:0.17812\n",
      "[16:30:09] [66]\ttraining-aucpr:0.17836\n",
      "[16:30:09] [67]\ttraining-aucpr:0.17863\n",
      "[16:30:09] [68]\ttraining-aucpr:0.17859\n",
      "[16:30:09] [69]\ttraining-aucpr:0.17852\n",
      "[16:30:09] [70]\ttraining-aucpr:0.17879\n",
      "[16:30:09] [71]\ttraining-aucpr:0.17890\n",
      "[16:30:09] [72]\ttraining-aucpr:0.17899\n",
      "[16:30:09] [73]\ttraining-aucpr:0.17923\n",
      "[16:30:09] [74]\ttraining-aucpr:0.17934\n",
      "[16:30:09] [75]\ttraining-aucpr:0.17956\n",
      "[16:30:09] [76]\ttraining-aucpr:0.17999\n",
      "[16:30:09] [77]\ttraining-aucpr:0.18008\n",
      "[16:30:09] [78]\ttraining-aucpr:0.18004\n",
      "[16:30:09] [79]\ttraining-aucpr:0.17995\n",
      "[16:30:09] [80]\ttraining-aucpr:0.18030\n",
      "[16:30:09] [81]\ttraining-aucpr:0.18037\n",
      "[16:30:09] [82]\ttraining-aucpr:0.18052\n",
      "[16:30:09] [83]\ttraining-aucpr:0.18089\n",
      "[16:30:09] [84]\ttraining-aucpr:0.18123\n",
      "[16:30:09] [85]\ttraining-aucpr:0.18110\n",
      "[16:30:09] [86]\ttraining-aucpr:0.18146\n",
      "[16:30:09] [87]\ttraining-aucpr:0.18161\n",
      "[16:30:09] [88]\ttraining-aucpr:0.18170\n",
      "[16:30:09] [89]\ttraining-aucpr:0.18214\n",
      "[16:30:09] [90]\ttraining-aucpr:0.18218\n",
      "[16:30:09] [91]\ttraining-aucpr:0.18211\n",
      "[16:30:09] [92]\ttraining-aucpr:0.18191\n",
      "[16:30:09] [93]\ttraining-aucpr:0.18211\n",
      "[16:30:09] [94]\ttraining-aucpr:0.18229\n",
      "[16:30:09] [95]\ttraining-aucpr:0.18244\n",
      "[16:30:09] [96]\ttraining-aucpr:0.18250\n",
      "[16:30:09] [97]\ttraining-aucpr:0.18252\n",
      "[16:30:09] [98]\ttraining-aucpr:0.18257\n",
      "[16:30:09] [99]\ttraining-aucpr:0.18278\n",
      "[16:30:09] [100]\ttraining-aucpr:0.18311\n",
      "[16:30:09] [101]\ttraining-aucpr:0.18326\n",
      "[16:30:09] [102]\ttraining-aucpr:0.18323\n",
      "[16:30:09] [103]\ttraining-aucpr:0.18323\n",
      "[16:30:10] [104]\ttraining-aucpr:0.18353\n",
      "[16:30:10] [105]\ttraining-aucpr:0.18369\n",
      "[16:30:10] [106]\ttraining-aucpr:0.18405\n",
      "[16:30:10] [107]\ttraining-aucpr:0.18401\n",
      "[16:30:10] [108]\ttraining-aucpr:0.18403\n",
      "[16:30:10] [109]\ttraining-aucpr:0.18424\n",
      "[16:30:10] [110]\ttraining-aucpr:0.18445\n",
      "[16:30:10] [111]\ttraining-aucpr:0.18463\n",
      "[16:30:10] [112]\ttraining-aucpr:0.18481\n",
      "[16:30:10] [113]\ttraining-aucpr:0.18527\n",
      "[16:30:10] [114]\ttraining-aucpr:0.18525\n",
      "[16:30:10] [115]\ttraining-aucpr:0.18532\n",
      "[16:30:10] [116]\ttraining-aucpr:0.18525\n",
      "[16:30:10] [117]\ttraining-aucpr:0.18538\n",
      "[16:30:10] [118]\ttraining-aucpr:0.18563\n",
      "[16:30:10] [119]\ttraining-aucpr:0.18585\n",
      "[16:30:10] [120]\ttraining-aucpr:0.18604\n",
      "[16:30:10] [121]\ttraining-aucpr:0.18603\n",
      "[16:30:10] [122]\ttraining-aucpr:0.18632\n",
      "[16:30:10] [123]\ttraining-aucpr:0.18633\n",
      "[16:30:10] [124]\ttraining-aucpr:0.18671\n",
      "[16:30:10] [125]\ttraining-aucpr:0.18683\n",
      "[16:30:10] [126]\ttraining-aucpr:0.18697\n",
      "[16:30:10] [127]\ttraining-aucpr:0.18720\n",
      "[16:30:10] [128]\ttraining-aucpr:0.18718\n",
      "[16:30:10] [129]\ttraining-aucpr:0.18760\n",
      "[16:30:10] [130]\ttraining-aucpr:0.18763\n",
      "[16:30:10] [131]\ttraining-aucpr:0.18771\n",
      "[16:30:10] [132]\ttraining-aucpr:0.18751\n",
      "[16:30:10] [133]\ttraining-aucpr:0.18767\n",
      "[16:30:10] [134]\ttraining-aucpr:0.18776\n",
      "[16:30:10] [135]\ttraining-aucpr:0.18794\n",
      "[16:30:10] [136]\ttraining-aucpr:0.18811\n",
      "[16:30:10] [137]\ttraining-aucpr:0.18845\n",
      "[16:30:10] [138]\ttraining-aucpr:0.18850\n",
      "[16:30:10] [139]\ttraining-aucpr:0.18877\n",
      "[16:30:10] [140]\ttraining-aucpr:0.18900\n",
      "[16:30:10] [141]\ttraining-aucpr:0.18900\n",
      "[16:30:10] [142]\ttraining-aucpr:0.18919\n",
      "[16:30:10] [143]\ttraining-aucpr:0.18938\n",
      "[16:30:10] [144]\ttraining-aucpr:0.18950\n",
      "[16:30:11] [145]\ttraining-aucpr:0.18965\n",
      "[16:30:11] [146]\ttraining-aucpr:0.18975\n",
      "[16:30:11] [147]\ttraining-aucpr:0.18982\n",
      "[16:30:11] [148]\ttraining-aucpr:0.18982\n",
      "[16:30:11] [149]\ttraining-aucpr:0.18983\n",
      "[16:30:11] [150]\ttraining-aucpr:0.18989\n",
      "[16:30:11] [151]\ttraining-aucpr:0.19032\n",
      "[16:30:11] [152]\ttraining-aucpr:0.19025\n",
      "[16:30:11] [153]\ttraining-aucpr:0.19042\n",
      "[16:30:11] [154]\ttraining-aucpr:0.19053\n",
      "[16:30:11] [155]\ttraining-aucpr:0.19060\n",
      "[16:30:11] [156]\ttraining-aucpr:0.19085\n",
      "[16:30:11] [157]\ttraining-aucpr:0.19106\n",
      "[16:30:11] [158]\ttraining-aucpr:0.19133\n",
      "[16:30:11] [159]\ttraining-aucpr:0.19163\n",
      "[16:30:11] [160]\ttraining-aucpr:0.19172\n",
      "[16:30:11] [161]\ttraining-aucpr:0.19198\n",
      "[16:30:11] [162]\ttraining-aucpr:0.19210\n",
      "[16:30:11] [163]\ttraining-aucpr:0.19236\n",
      "[16:30:11] [164]\ttraining-aucpr:0.19249\n",
      "[16:30:11] [165]\ttraining-aucpr:0.19251\n",
      "[16:30:11] [166]\ttraining-aucpr:0.19265\n",
      "[16:30:11] [167]\ttraining-aucpr:0.19284\n",
      "[16:30:11] [168]\ttraining-aucpr:0.19289\n",
      "[16:30:11] [169]\ttraining-aucpr:0.19318\n",
      "[16:30:11] [170]\ttraining-aucpr:0.19324\n",
      "[16:30:11] [171]\ttraining-aucpr:0.19334\n",
      "[16:30:11] [172]\ttraining-aucpr:0.19342\n",
      "[16:30:11] [173]\ttraining-aucpr:0.19349\n",
      "[16:30:11] [174]\ttraining-aucpr:0.19378\n",
      "[16:30:11] [175]\ttraining-aucpr:0.19382\n",
      "[16:30:11] [176]\ttraining-aucpr:0.19370\n",
      "[16:30:11] [177]\ttraining-aucpr:0.19387\n",
      "[16:30:11] [178]\ttraining-aucpr:0.19415\n",
      "[16:30:11] [179]\ttraining-aucpr:0.19456\n",
      "[16:30:11] [180]\ttraining-aucpr:0.19445\n",
      "[16:30:11] [181]\ttraining-aucpr:0.19457\n",
      "[16:30:11] [182]\ttraining-aucpr:0.19469\n",
      "[16:30:11] [183]\ttraining-aucpr:0.19476\n",
      "[16:30:11] [184]\ttraining-aucpr:0.19465\n",
      "[16:30:11] [185]\ttraining-aucpr:0.19461\n",
      "[16:30:12] [186]\ttraining-aucpr:0.19447\n",
      "[16:30:12] [187]\ttraining-aucpr:0.19463\n",
      "[16:30:12] [188]\ttraining-aucpr:0.19483\n",
      "[16:30:12] [189]\ttraining-aucpr:0.19510\n",
      "[16:30:12] [190]\ttraining-aucpr:0.19518\n",
      "[16:30:12] [191]\ttraining-aucpr:0.19537\n",
      "[16:30:12] [192]\ttraining-aucpr:0.19542\n",
      "[16:30:12] [193]\ttraining-aucpr:0.19548\n",
      "[16:30:12] [194]\ttraining-aucpr:0.19559\n",
      "[16:30:12] [195]\ttraining-aucpr:0.19593\n",
      "[16:30:12] [196]\ttraining-aucpr:0.19593\n",
      "[16:30:12] [197]\ttraining-aucpr:0.19599\n",
      "[16:30:12] [198]\ttraining-aucpr:0.19619\n",
      "[16:30:12] [199]\ttraining-aucpr:0.19633\n",
      "[16:30:12] [200]\ttraining-aucpr:0.19666\n",
      "[16:30:12] [201]\ttraining-aucpr:0.19684\n",
      "[16:30:12] [202]\ttraining-aucpr:0.19688\n",
      "[16:30:12] [203]\ttraining-aucpr:0.19698\n",
      "[16:30:12] [204]\ttraining-aucpr:0.19717\n",
      "[16:30:12] [205]\ttraining-aucpr:0.19733\n",
      "[16:30:12] [206]\ttraining-aucpr:0.19737\n",
      "[16:30:12] [207]\ttraining-aucpr:0.19756\n",
      "[16:30:12] [208]\ttraining-aucpr:0.19775\n",
      "[16:30:12] [209]\ttraining-aucpr:0.19787\n",
      "[16:30:12] [210]\ttraining-aucpr:0.19798\n",
      "[16:30:12] [211]\ttraining-aucpr:0.19809\n",
      "[16:30:12] [212]\ttraining-aucpr:0.19818\n",
      "[16:30:12] [213]\ttraining-aucpr:0.19828\n",
      "[16:30:12] [214]\ttraining-aucpr:0.19866\n",
      "[16:30:12] [215]\ttraining-aucpr:0.19870\n",
      "[16:30:12] [216]\ttraining-aucpr:0.19873\n",
      "[16:30:12] [217]\ttraining-aucpr:0.19912\n",
      "[16:30:12] [218]\ttraining-aucpr:0.19934\n",
      "[16:30:12] [219]\ttraining-aucpr:0.19934\n",
      "[16:30:12] [220]\ttraining-aucpr:0.19938\n",
      "[16:30:12] [221]\ttraining-aucpr:0.19958\n",
      "[16:30:12] [222]\ttraining-aucpr:0.19968\n",
      "[16:30:12] [223]\ttraining-aucpr:0.19977\n",
      "[16:30:12] [224]\ttraining-aucpr:0.19986\n",
      "[16:30:12] [225]\ttraining-aucpr:0.20005\n",
      "[16:30:13] [226]\ttraining-aucpr:0.20019\n",
      "[16:30:13] [227]\ttraining-aucpr:0.20031\n",
      "[16:30:13] [228]\ttraining-aucpr:0.20055\n",
      "[16:30:13] [229]\ttraining-aucpr:0.20056\n",
      "[16:30:13] [230]\ttraining-aucpr:0.20062\n",
      "[16:30:13] [231]\ttraining-aucpr:0.20075\n",
      "[16:30:13] [232]\ttraining-aucpr:0.20116\n",
      "[16:30:13] [233]\ttraining-aucpr:0.20142\n",
      "[16:30:13] [234]\ttraining-aucpr:0.20152\n",
      "[16:30:13] [235]\ttraining-aucpr:0.20167\n",
      "[16:30:13] [236]\ttraining-aucpr:0.20177\n",
      "[16:30:13] [237]\ttraining-aucpr:0.20184\n",
      "[16:30:13] [238]\ttraining-aucpr:0.20199\n",
      "[16:30:13] [239]\ttraining-aucpr:0.20217\n",
      "[16:30:13] [240]\ttraining-aucpr:0.20221\n",
      "[16:30:13] [241]\ttraining-aucpr:0.20242\n",
      "[16:30:13] [242]\ttraining-aucpr:0.20245\n",
      "[16:30:13] [243]\ttraining-aucpr:0.20253\n",
      "[16:30:13] [244]\ttraining-aucpr:0.20252\n",
      "[16:30:13] [245]\ttraining-aucpr:0.20276\n",
      "[16:30:13] [246]\ttraining-aucpr:0.20283\n",
      "[16:30:13] [247]\ttraining-aucpr:0.20291\n",
      "[16:30:13] [248]\ttraining-aucpr:0.20324\n",
      "[16:30:13] [249]\ttraining-aucpr:0.20329\n",
      "[16:30:13] [250]\ttraining-aucpr:0.20345\n",
      "[16:30:13] [251]\ttraining-aucpr:0.20361\n",
      "[16:30:13] [252]\ttraining-aucpr:0.20378\n",
      "[16:30:13] [253]\ttraining-aucpr:0.20383\n",
      "[16:30:13] [254]\ttraining-aucpr:0.20406\n",
      "[16:30:13] [255]\ttraining-aucpr:0.20436\n",
      "[16:30:13] [256]\ttraining-aucpr:0.20443\n",
      "[16:30:13] [257]\ttraining-aucpr:0.20467\n",
      "[16:30:13] [258]\ttraining-aucpr:0.20480\n",
      "[16:30:13] [259]\ttraining-aucpr:0.20494\n",
      "[16:30:13] [260]\ttraining-aucpr:0.20506\n",
      "[16:30:13] [261]\ttraining-aucpr:0.20517\n",
      "[16:30:13] [262]\ttraining-aucpr:0.20524\n",
      "[16:30:13] [263]\ttraining-aucpr:0.20527\n",
      "[16:30:14] [264]\ttraining-aucpr:0.20538\n",
      "2025-07-28 16:30:15,026 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:30:15,158 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:30:15,969] Trial 20 finished with value: 0.16505988671561042 and parameters: {'max_depth': 4, 'learning_rate': 0.29175835174865433, 'n_estimators': 265, 'min_child_weight': 7, 'gamma': 0.6510402168377807, 'subsample': 0.9312978390099437, 'colsample_bytree': 0.7643526548893316, 'reg_alpha': 0.6151146042671063, 'reg_lambda': 0.117082353175192}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:30:16,099 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.9215599879015353, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.6446628396784585, 'learning_rate': 0.14298926156497807, 'max_depth': 4, 'min_child_weight': 4, 'reg_alpha': 0.6577853627066188, 'reg_lambda': 0.010026919125089191, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.925308566320651, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 95}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:30:17,487 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:30:18] Task 2 got rank 2\n",
      "[16:30:18] Task 1 got rank 1\n",
      "[16:30:18] Task 0 got rank 0\n",
      "[16:30:18] Task 3 got rank 3\n",
      "[16:30:19] [0]\ttraining-aucpr:0.05563\n",
      "[16:30:19] [1]\ttraining-aucpr:0.06121\n",
      "[16:30:19] [2]\ttraining-aucpr:0.06964\n",
      "[16:30:19] [3]\ttraining-aucpr:0.07585\n",
      "[16:30:19] [4]\ttraining-aucpr:0.08175\n",
      "[16:30:19] [5]\ttraining-aucpr:0.08230\n",
      "[16:30:19] [6]\ttraining-aucpr:0.08345\n",
      "[16:30:19] [7]\ttraining-aucpr:0.08852\n",
      "[16:30:19] [8]\ttraining-aucpr:0.10199\n",
      "[16:30:19] [9]\ttraining-aucpr:0.10281\n",
      "[16:30:19] [10]\ttraining-aucpr:0.10730\n",
      "[16:30:19] [11]\ttraining-aucpr:0.11484\n",
      "[16:30:19] [12]\ttraining-aucpr:0.11583\n",
      "[16:30:19] [13]\ttraining-aucpr:0.11860\n",
      "[16:30:19] [14]\ttraining-aucpr:0.11876\n",
      "[16:30:19] [15]\ttraining-aucpr:0.12212\n",
      "[16:30:19] [16]\ttraining-aucpr:0.12388\n",
      "[16:30:19] [17]\ttraining-aucpr:0.12584\n",
      "[16:30:19] [18]\ttraining-aucpr:0.12763\n",
      "[16:30:19] [19]\ttraining-aucpr:0.12923\n",
      "[16:30:19] [20]\ttraining-aucpr:0.13337\n",
      "[16:30:19] [21]\ttraining-aucpr:0.13596\n",
      "[16:30:19] [22]\ttraining-aucpr:0.13646\n",
      "[16:30:19] [23]\ttraining-aucpr:0.13756\n",
      "[16:30:19] [24]\ttraining-aucpr:0.14022\n",
      "[16:30:19] [25]\ttraining-aucpr:0.14168\n",
      "[16:30:19] [26]\ttraining-aucpr:0.14204\n",
      "[16:30:19] [27]\ttraining-aucpr:0.14241\n",
      "[16:30:19] [28]\ttraining-aucpr:0.14293\n",
      "[16:30:19] [29]\ttraining-aucpr:0.14489\n",
      "[16:30:19] [30]\ttraining-aucpr:0.14644\n",
      "[16:30:19] [31]\ttraining-aucpr:0.14785\n",
      "[16:30:19] [32]\ttraining-aucpr:0.14913\n",
      "[16:30:19] [33]\ttraining-aucpr:0.15038\n",
      "[16:30:19] [34]\ttraining-aucpr:0.15172\n",
      "[16:30:19] [35]\ttraining-aucpr:0.15269\n",
      "[16:30:19] [36]\ttraining-aucpr:0.15399\n",
      "[16:30:20] [37]\ttraining-aucpr:0.15414\n",
      "[16:30:20] [38]\ttraining-aucpr:0.15511\n",
      "[16:30:20] [39]\ttraining-aucpr:0.15585\n",
      "[16:30:20] [40]\ttraining-aucpr:0.15678\n",
      "[16:30:20] [41]\ttraining-aucpr:0.15811\n",
      "[16:30:20] [42]\ttraining-aucpr:0.15890\n",
      "[16:30:20] [43]\ttraining-aucpr:0.15918\n",
      "[16:30:20] [44]\ttraining-aucpr:0.16009\n",
      "[16:30:20] [45]\ttraining-aucpr:0.16049\n",
      "[16:30:20] [46]\ttraining-aucpr:0.16079\n",
      "[16:30:20] [47]\ttraining-aucpr:0.16147\n",
      "[16:30:20] [48]\ttraining-aucpr:0.16194\n",
      "[16:30:20] [49]\ttraining-aucpr:0.16249\n",
      "[16:30:20] [50]\ttraining-aucpr:0.16361\n",
      "[16:30:20] [51]\ttraining-aucpr:0.16421\n",
      "[16:30:20] [52]\ttraining-aucpr:0.16451\n",
      "[16:30:20] [53]\ttraining-aucpr:0.16500\n",
      "[16:30:20] [54]\ttraining-aucpr:0.16569\n",
      "[16:30:20] [55]\ttraining-aucpr:0.16624\n",
      "[16:30:20] [56]\ttraining-aucpr:0.16644\n",
      "[16:30:20] [57]\ttraining-aucpr:0.16716\n",
      "[16:30:20] [58]\ttraining-aucpr:0.16743\n",
      "[16:30:20] [59]\ttraining-aucpr:0.16798\n",
      "[16:30:20] [60]\ttraining-aucpr:0.16820\n",
      "[16:30:20] [61]\ttraining-aucpr:0.16851\n",
      "[16:30:20] [62]\ttraining-aucpr:0.16874\n",
      "[16:30:20] [63]\ttraining-aucpr:0.16893\n",
      "[16:30:20] [64]\ttraining-aucpr:0.16928\n",
      "[16:30:20] [65]\ttraining-aucpr:0.16997\n",
      "[16:30:20] [66]\ttraining-aucpr:0.17013\n",
      "[16:30:20] [67]\ttraining-aucpr:0.17092\n",
      "[16:30:20] [68]\ttraining-aucpr:0.17097\n",
      "[16:30:20] [69]\ttraining-aucpr:0.17149\n",
      "[16:30:20] [70]\ttraining-aucpr:0.17197\n",
      "[16:30:20] [71]\ttraining-aucpr:0.17236\n",
      "[16:30:20] [72]\ttraining-aucpr:0.17284\n",
      "[16:30:20] [73]\ttraining-aucpr:0.17296\n",
      "[16:30:20] [74]\ttraining-aucpr:0.17336\n",
      "[16:30:21] [75]\ttraining-aucpr:0.17374\n",
      "[16:30:21] [76]\ttraining-aucpr:0.17402\n",
      "[16:30:21] [77]\ttraining-aucpr:0.17426\n",
      "[16:30:21] [78]\ttraining-aucpr:0.17495\n",
      "[16:30:21] [79]\ttraining-aucpr:0.17500\n",
      "[16:30:21] [80]\ttraining-aucpr:0.17532\n",
      "[16:30:21] [81]\ttraining-aucpr:0.17547\n",
      "[16:30:21] [82]\ttraining-aucpr:0.17566\n",
      "[16:30:21] [83]\ttraining-aucpr:0.17601\n",
      "[16:30:21] [84]\ttraining-aucpr:0.17640\n",
      "[16:30:21] [85]\ttraining-aucpr:0.17645\n",
      "[16:30:21] [86]\ttraining-aucpr:0.17647\n",
      "[16:30:21] [87]\ttraining-aucpr:0.17681\n",
      "[16:30:21] [88]\ttraining-aucpr:0.17707\n",
      "[16:30:21] [89]\ttraining-aucpr:0.17723\n",
      "[16:30:21] [90]\ttraining-aucpr:0.17738\n",
      "[16:30:21] [91]\ttraining-aucpr:0.17760\n",
      "[16:30:21] [92]\ttraining-aucpr:0.17767\n",
      "[16:30:21] [93]\ttraining-aucpr:0.17779\n",
      "[16:30:21] [94]\ttraining-aucpr:0.17803\n",
      "2025-07-28 16:30:22,490 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:30:22,582 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:30:23,260] Trial 21 finished with value: 0.1891568282020209 and parameters: {'max_depth': 4, 'learning_rate': 0.14298926156497807, 'n_estimators': 95, 'min_child_weight': 4, 'gamma': 0.6446628396784585, 'subsample': 0.925308566320651, 'colsample_bytree': 0.9215599879015353, 'reg_alpha': 0.6577853627066188, 'reg_lambda': 0.010026919125089191}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:30:23,369 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.8550984197215405, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.6633044588353214, 'learning_rate': 0.20074565745636025, 'max_depth': 5, 'min_child_weight': 4, 'reg_alpha': 0.6212904036835192, 'reg_lambda': 0.11544979113299275, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9595421797875412, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 97}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:30:24,754 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:30:25] Task 1 got rank 1[16:30:25] Task 0 got rank 0\n",
      "\n",
      "[16:30:25] Task 2 got rank 2\n",
      "[16:30:25] Task 3 got rank 3\n",
      "[16:30:26] [0]\ttraining-aucpr:0.06225\n",
      "[16:30:26] [1]\ttraining-aucpr:0.07628\n",
      "[16:30:26] [2]\ttraining-aucpr:0.08401\n",
      "[16:30:26] [3]\ttraining-aucpr:0.09448\n",
      "[16:30:26] [4]\ttraining-aucpr:0.09832\n",
      "[16:30:26] [5]\ttraining-aucpr:0.10177\n",
      "[16:30:26] [6]\ttraining-aucpr:0.10879\n",
      "[16:30:26] [7]\ttraining-aucpr:0.11532\n",
      "[16:30:26] [8]\ttraining-aucpr:0.11945\n",
      "[16:30:26] [9]\ttraining-aucpr:0.12527\n",
      "[16:30:26] [10]\ttraining-aucpr:0.12920\n",
      "[16:30:26] [11]\ttraining-aucpr:0.13347\n",
      "[16:30:26] [12]\ttraining-aucpr:0.13836\n",
      "[16:30:26] [13]\ttraining-aucpr:0.14096\n",
      "[16:30:26] [14]\ttraining-aucpr:0.14273\n",
      "[16:30:26] [15]\ttraining-aucpr:0.14462\n",
      "[16:30:26] [16]\ttraining-aucpr:0.14592\n",
      "[16:30:26] [17]\ttraining-aucpr:0.14994\n",
      "[16:30:26] [18]\ttraining-aucpr:0.15097\n",
      "[16:30:26] [19]\ttraining-aucpr:0.15285\n",
      "[16:30:26] [20]\ttraining-aucpr:0.15509\n",
      "[16:30:26] [21]\ttraining-aucpr:0.15608\n",
      "[16:30:27] [22]\ttraining-aucpr:0.15747\n",
      "[16:30:27] [23]\ttraining-aucpr:0.15911\n",
      "[16:30:27] [24]\ttraining-aucpr:0.16048\n",
      "[16:30:27] [25]\ttraining-aucpr:0.16264\n",
      "[16:30:27] [26]\ttraining-aucpr:0.16398\n",
      "[16:30:27] [27]\ttraining-aucpr:0.16539\n",
      "[16:30:27] [28]\ttraining-aucpr:0.16553\n",
      "[16:30:27] [29]\ttraining-aucpr:0.16690\n",
      "[16:30:27] [30]\ttraining-aucpr:0.16739\n",
      "[16:30:27] [31]\ttraining-aucpr:0.16817\n",
      "[16:30:27] [32]\ttraining-aucpr:0.16820\n",
      "[16:30:27] [33]\ttraining-aucpr:0.16982\n",
      "[16:30:27] [34]\ttraining-aucpr:0.17104\n",
      "[16:30:27] [35]\ttraining-aucpr:0.17191\n",
      "[16:30:27] [36]\ttraining-aucpr:0.17278\n",
      "[16:30:27] [37]\ttraining-aucpr:0.17407\n",
      "[16:30:27] [38]\ttraining-aucpr:0.17489\n",
      "[16:30:27] [39]\ttraining-aucpr:0.17535\n",
      "[16:30:27] [40]\ttraining-aucpr:0.17633\n",
      "[16:30:27] [41]\ttraining-aucpr:0.17712\n",
      "[16:30:27] [42]\ttraining-aucpr:0.17750\n",
      "[16:30:27] [43]\ttraining-aucpr:0.17811\n",
      "[16:30:27] [44]\ttraining-aucpr:0.17912\n",
      "[16:30:27] [45]\ttraining-aucpr:0.17960\n",
      "[16:30:27] [46]\ttraining-aucpr:0.18057\n",
      "[16:30:27] [47]\ttraining-aucpr:0.18123\n",
      "[16:30:27] [48]\ttraining-aucpr:0.18184\n",
      "[16:30:27] [49]\ttraining-aucpr:0.18258\n",
      "[16:30:27] [50]\ttraining-aucpr:0.18317\n",
      "[16:30:27] [51]\ttraining-aucpr:0.18357\n",
      "[16:30:28] [52]\ttraining-aucpr:0.18417\n",
      "[16:30:28] [53]\ttraining-aucpr:0.18466\n",
      "[16:30:28] [54]\ttraining-aucpr:0.18478\n",
      "[16:30:28] [55]\ttraining-aucpr:0.18551\n",
      "[16:30:28] [56]\ttraining-aucpr:0.18581\n",
      "[16:30:28] [57]\ttraining-aucpr:0.18631\n",
      "[16:30:28] [58]\ttraining-aucpr:0.18651\n",
      "[16:30:28] [59]\ttraining-aucpr:0.18688\n",
      "[16:30:28] [60]\ttraining-aucpr:0.18737\n",
      "[16:30:28] [61]\ttraining-aucpr:0.18765\n",
      "[16:30:28] [62]\ttraining-aucpr:0.18845\n",
      "[16:30:28] [63]\ttraining-aucpr:0.18861\n",
      "[16:30:28] [64]\ttraining-aucpr:0.18920\n",
      "[16:30:28] [65]\ttraining-aucpr:0.18974\n",
      "[16:30:28] [66]\ttraining-aucpr:0.18991\n",
      "[16:30:28] [67]\ttraining-aucpr:0.19062\n",
      "[16:30:28] [68]\ttraining-aucpr:0.19099\n",
      "[16:30:28] [69]\ttraining-aucpr:0.19144\n",
      "[16:30:28] [70]\ttraining-aucpr:0.19193\n",
      "[16:30:28] [71]\ttraining-aucpr:0.19234\n",
      "[16:30:28] [72]\ttraining-aucpr:0.19279\n",
      "[16:30:28] [73]\ttraining-aucpr:0.19292\n",
      "[16:30:28] [74]\ttraining-aucpr:0.19321\n",
      "[16:30:28] [75]\ttraining-aucpr:0.19330\n",
      "[16:30:28] [76]\ttraining-aucpr:0.19361\n",
      "[16:30:28] [77]\ttraining-aucpr:0.19392\n",
      "[16:30:28] [78]\ttraining-aucpr:0.19434\n",
      "[16:30:28] [79]\ttraining-aucpr:0.19470\n",
      "[16:30:28] [80]\ttraining-aucpr:0.19498\n",
      "[16:30:28] [81]\ttraining-aucpr:0.19524\n",
      "[16:30:28] [82]\ttraining-aucpr:0.19627\n",
      "[16:30:28] [83]\ttraining-aucpr:0.19669\n",
      "[16:30:28] [84]\ttraining-aucpr:0.19725\n",
      "[16:30:28] [85]\ttraining-aucpr:0.19744\n",
      "[16:30:28] [86]\ttraining-aucpr:0.19812\n",
      "[16:30:29] [87]\ttraining-aucpr:0.19883\n",
      "[16:30:29] [88]\ttraining-aucpr:0.19904\n",
      "[16:30:29] [89]\ttraining-aucpr:0.19921\n",
      "[16:30:29] [90]\ttraining-aucpr:0.19929\n",
      "[16:30:29] [91]\ttraining-aucpr:0.19932\n",
      "[16:30:29] [92]\ttraining-aucpr:0.19939\n",
      "[16:30:29] [93]\ttraining-aucpr:0.19959\n",
      "[16:30:29] [94]\ttraining-aucpr:0.19976\n",
      "[16:30:29] [95]\ttraining-aucpr:0.19990\n",
      "[16:30:29] [96]\ttraining-aucpr:0.20026\n",
      "2025-07-28 16:30:30,299 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:30:30,389 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:30:31,096] Trial 22 finished with value: 0.17688533041037455 and parameters: {'max_depth': 5, 'learning_rate': 0.20074565745636025, 'n_estimators': 97, 'min_child_weight': 4, 'gamma': 0.6633044588353214, 'subsample': 0.9595421797875412, 'colsample_bytree': 0.8550984197215405, 'reg_alpha': 0.6212904036835192, 'reg_lambda': 0.11544979113299275}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:30:31,208 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.9427305882292083, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.782210484163186, 'learning_rate': 0.14426634926233622, 'max_depth': 6, 'min_child_weight': 5, 'reg_alpha': 0.3500707205856834, 'reg_lambda': 0.10731581327742756, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.8006103903804539, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 103}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:30:32,602 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:30:33] Task 1 got rank 1\n",
      "[16:30:33] Task 2 got rank 2\n",
      "[16:30:33] Task 0 got rank 0\n",
      "[16:30:33] Task 3 got rank 3\n",
      "[16:30:34] [0]\ttraining-aucpr:0.07221\n",
      "[16:30:34] [1]\ttraining-aucpr:0.08460\n",
      "[16:30:34] [2]\ttraining-aucpr:0.09469\n",
      "[16:30:34] [3]\ttraining-aucpr:0.10396\n",
      "[16:30:34] [4]\ttraining-aucpr:0.10834\n",
      "[16:30:34] [5]\ttraining-aucpr:0.10987\n",
      "[16:30:34] [6]\ttraining-aucpr:0.11386\n",
      "[16:30:34] [7]\ttraining-aucpr:0.11691\n",
      "[16:30:34] [8]\ttraining-aucpr:0.12076\n",
      "[16:30:34] [9]\ttraining-aucpr:0.12562\n",
      "[16:30:34] [10]\ttraining-aucpr:0.13153\n",
      "[16:30:34] [11]\ttraining-aucpr:0.13210\n",
      "[16:30:34] [12]\ttraining-aucpr:0.13728\n",
      "[16:30:34] [13]\ttraining-aucpr:0.14045\n",
      "[16:30:34] [14]\ttraining-aucpr:0.14323\n",
      "[16:30:34] [15]\ttraining-aucpr:0.14617\n",
      "[16:30:34] [16]\ttraining-aucpr:0.14899\n",
      "[16:30:34] [17]\ttraining-aucpr:0.15120\n",
      "[16:30:34] [18]\ttraining-aucpr:0.15421\n",
      "[16:30:34] [19]\ttraining-aucpr:0.15525\n",
      "[16:30:34] [20]\ttraining-aucpr:0.15766\n",
      "[16:30:34] [21]\ttraining-aucpr:0.15995\n",
      "[16:30:34] [22]\ttraining-aucpr:0.16250\n",
      "[16:30:34] [23]\ttraining-aucpr:0.16444\n",
      "[16:30:35] [24]\ttraining-aucpr:0.16640\n",
      "[16:30:35] [25]\ttraining-aucpr:0.16734\n",
      "[16:30:35] [26]\ttraining-aucpr:0.16958\n",
      "[16:30:35] [27]\ttraining-aucpr:0.17122\n",
      "[16:30:35] [28]\ttraining-aucpr:0.17194\n",
      "[16:30:35] [29]\ttraining-aucpr:0.17457\n",
      "[16:30:35] [30]\ttraining-aucpr:0.17597\n",
      "[16:30:35] [31]\ttraining-aucpr:0.17759\n",
      "[16:30:35] [32]\ttraining-aucpr:0.17868\n",
      "[16:30:35] [33]\ttraining-aucpr:0.17924\n",
      "[16:30:35] [34]\ttraining-aucpr:0.18039\n",
      "[16:30:35] [35]\ttraining-aucpr:0.18182\n",
      "[16:30:35] [36]\ttraining-aucpr:0.18298\n",
      "[16:30:35] [37]\ttraining-aucpr:0.18373\n",
      "[16:30:35] [38]\ttraining-aucpr:0.18442\n",
      "[16:30:35] [39]\ttraining-aucpr:0.18478\n",
      "[16:30:35] [40]\ttraining-aucpr:0.18568\n",
      "[16:30:35] [41]\ttraining-aucpr:0.18667\n",
      "[16:30:35] [42]\ttraining-aucpr:0.18721\n",
      "[16:30:35] [43]\ttraining-aucpr:0.18818\n",
      "[16:30:35] [44]\ttraining-aucpr:0.18938\n",
      "[16:30:35] [45]\ttraining-aucpr:0.19010\n",
      "[16:30:35] [46]\ttraining-aucpr:0.19089\n",
      "[16:30:35] [47]\ttraining-aucpr:0.19187\n",
      "[16:30:35] [48]\ttraining-aucpr:0.19288\n",
      "[16:30:35] [49]\ttraining-aucpr:0.19341\n",
      "[16:30:35] [50]\ttraining-aucpr:0.19487\n",
      "[16:30:35] [51]\ttraining-aucpr:0.19525\n",
      "[16:30:35] [52]\ttraining-aucpr:0.19633\n",
      "[16:30:35] [53]\ttraining-aucpr:0.19675\n",
      "[16:30:35] [54]\ttraining-aucpr:0.19738\n",
      "[16:30:36] [55]\ttraining-aucpr:0.19777\n",
      "[16:30:36] [56]\ttraining-aucpr:0.19810\n",
      "[16:30:36] [57]\ttraining-aucpr:0.19869\n",
      "[16:30:36] [58]\ttraining-aucpr:0.19942\n",
      "[16:30:36] [59]\ttraining-aucpr:0.20009\n",
      "[16:30:36] [60]\ttraining-aucpr:0.20090\n",
      "[16:30:36] [61]\ttraining-aucpr:0.20143\n",
      "[16:30:36] [62]\ttraining-aucpr:0.20185\n",
      "[16:30:36] [63]\ttraining-aucpr:0.20213\n",
      "[16:30:36] [64]\ttraining-aucpr:0.20277\n",
      "[16:30:36] [65]\ttraining-aucpr:0.20316\n",
      "[16:30:36] [66]\ttraining-aucpr:0.20346\n",
      "[16:30:36] [67]\ttraining-aucpr:0.20405\n",
      "[16:30:36] [68]\ttraining-aucpr:0.20464\n",
      "[16:30:36] [69]\ttraining-aucpr:0.20515\n",
      "[16:30:36] [70]\ttraining-aucpr:0.20579\n",
      "[16:30:36] [71]\ttraining-aucpr:0.20631\n",
      "[16:30:36] [72]\ttraining-aucpr:0.20690\n",
      "[16:30:36] [73]\ttraining-aucpr:0.20719\n",
      "[16:30:36] [74]\ttraining-aucpr:0.20793\n",
      "[16:30:36] [75]\ttraining-aucpr:0.20839\n",
      "[16:30:36] [76]\ttraining-aucpr:0.20921\n",
      "[16:30:36] [77]\ttraining-aucpr:0.20933\n",
      "[16:30:36] [78]\ttraining-aucpr:0.20996\n",
      "[16:30:36] [79]\ttraining-aucpr:0.21010\n",
      "[16:30:36] [80]\ttraining-aucpr:0.21051\n",
      "[16:30:36] [81]\ttraining-aucpr:0.21094\n",
      "[16:30:36] [82]\ttraining-aucpr:0.21132\n",
      "[16:30:36] [83]\ttraining-aucpr:0.21229\n",
      "[16:30:36] [84]\ttraining-aucpr:0.21301\n",
      "[16:30:36] [85]\ttraining-aucpr:0.21338\n",
      "[16:30:36] [86]\ttraining-aucpr:0.21396\n",
      "[16:30:37] [87]\ttraining-aucpr:0.21433\n",
      "[16:30:37] [88]\ttraining-aucpr:0.21564\n",
      "[16:30:37] [89]\ttraining-aucpr:0.21603\n",
      "[16:30:37] [90]\ttraining-aucpr:0.21613\n",
      "[16:30:37] [91]\ttraining-aucpr:0.21688\n",
      "[16:30:37] [92]\ttraining-aucpr:0.21711\n",
      "[16:30:37] [93]\ttraining-aucpr:0.21740\n",
      "[16:30:37] [94]\ttraining-aucpr:0.21758\n",
      "[16:30:37] [95]\ttraining-aucpr:0.21808\n",
      "[16:30:37] [96]\ttraining-aucpr:0.21876\n",
      "[16:30:37] [97]\ttraining-aucpr:0.21946\n",
      "[16:30:37] [98]\ttraining-aucpr:0.22008\n",
      "[16:30:37] [99]\ttraining-aucpr:0.22075\n",
      "[16:30:37] [100]\ttraining-aucpr:0.22104\n",
      "[16:30:37] [101]\ttraining-aucpr:0.22139\n",
      "[16:30:37] [102]\ttraining-aucpr:0.22185\n",
      "2025-07-28 16:30:38,495 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:30:38,588 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:30:39,313] Trial 23 finished with value: 0.1608438987409169 and parameters: {'max_depth': 6, 'learning_rate': 0.14426634926233622, 'n_estimators': 103, 'min_child_weight': 5, 'gamma': 0.782210484163186, 'subsample': 0.8006103903804539, 'colsample_bytree': 0.9427305882292083, 'reg_alpha': 0.3500707205856834, 'reg_lambda': 0.10731581327742756}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:30:39,441 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.7547681063746714, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.4382927217993571, 'learning_rate': 0.04087971200804147, 'max_depth': 4, 'min_child_weight': 3, 'reg_alpha': 0.16448804475298373, 'reg_lambda': 0.4428529824328987, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9121907767598478, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 152}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:30:40,824 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:30:41] Task 1 got rank 1\n",
      "[16:30:41] Task 2 got rank 2\n",
      "[16:30:41] Task 0 got rank 0\n",
      "[16:30:41] Task 3 got rank 3\n",
      "[16:30:42] [0]\ttraining-aucpr:0.05578\n",
      "[16:30:42] [1]\ttraining-aucpr:0.06016\n",
      "[16:30:42] [2]\ttraining-aucpr:0.06099\n",
      "[16:30:42] [3]\ttraining-aucpr:0.06124\n",
      "[16:30:42] [4]\ttraining-aucpr:0.06606\n",
      "[16:30:42] [5]\ttraining-aucpr:0.07465\n",
      "[16:30:42] [6]\ttraining-aucpr:0.07887\n",
      "[16:30:42] [7]\ttraining-aucpr:0.07907\n",
      "[16:30:42] [8]\ttraining-aucpr:0.07959\n",
      "[16:30:42] [9]\ttraining-aucpr:0.08113\n",
      "[16:30:42] [10]\ttraining-aucpr:0.08129\n",
      "[16:30:42] [11]\ttraining-aucpr:0.08127\n",
      "[16:30:42] [12]\ttraining-aucpr:0.08209\n",
      "[16:30:42] [13]\ttraining-aucpr:0.08447\n",
      "[16:30:42] [14]\ttraining-aucpr:0.08675\n",
      "[16:30:42] [15]\ttraining-aucpr:0.08661\n",
      "[16:30:42] [16]\ttraining-aucpr:0.08669\n",
      "[16:30:42] [17]\ttraining-aucpr:0.08815\n",
      "[16:30:42] [18]\ttraining-aucpr:0.08905\n",
      "[16:30:42] [19]\ttraining-aucpr:0.09520\n",
      "[16:30:42] [20]\ttraining-aucpr:0.09504\n",
      "[16:30:42] [21]\ttraining-aucpr:0.09493\n",
      "[16:30:42] [22]\ttraining-aucpr:0.09523\n",
      "[16:30:42] [23]\ttraining-aucpr:0.09515\n",
      "[16:30:42] [24]\ttraining-aucpr:0.09826\n",
      "[16:30:42] [25]\ttraining-aucpr:0.09871\n",
      "[16:30:42] [26]\ttraining-aucpr:0.10093\n",
      "[16:30:43] [27]\ttraining-aucpr:0.10215\n",
      "[16:30:43] [28]\ttraining-aucpr:0.10178\n",
      "[16:30:43] [29]\ttraining-aucpr:0.10255\n",
      "[16:30:43] [30]\ttraining-aucpr:0.10406\n",
      "[16:30:43] [31]\ttraining-aucpr:0.10587\n",
      "[16:30:43] [32]\ttraining-aucpr:0.10663\n",
      "[16:30:43] [33]\ttraining-aucpr:0.10629\n",
      "[16:30:43] [34]\ttraining-aucpr:0.10763\n",
      "[16:30:43] [35]\ttraining-aucpr:0.10836\n",
      "[16:30:43] [36]\ttraining-aucpr:0.10899\n",
      "[16:30:43] [37]\ttraining-aucpr:0.11130\n",
      "[16:30:43] [38]\ttraining-aucpr:0.11302\n",
      "[16:30:43] [39]\ttraining-aucpr:0.11459\n",
      "[16:30:43] [40]\ttraining-aucpr:0.11630\n",
      "[16:30:43] [41]\ttraining-aucpr:0.11674\n",
      "[16:30:43] [42]\ttraining-aucpr:0.11747\n",
      "[16:30:43] [43]\ttraining-aucpr:0.11751\n",
      "[16:30:43] [44]\ttraining-aucpr:0.11826\n",
      "[16:30:43] [45]\ttraining-aucpr:0.11971\n",
      "[16:30:43] [46]\ttraining-aucpr:0.11996\n",
      "[16:30:43] [47]\ttraining-aucpr:0.12057\n",
      "[16:30:43] [48]\ttraining-aucpr:0.12110\n",
      "[16:30:43] [49]\ttraining-aucpr:0.12173\n",
      "[16:30:43] [50]\ttraining-aucpr:0.12239\n",
      "[16:30:43] [51]\ttraining-aucpr:0.12258\n",
      "[16:30:43] [52]\ttraining-aucpr:0.12361\n",
      "[16:30:43] [53]\ttraining-aucpr:0.12394\n",
      "[16:30:43] [54]\ttraining-aucpr:0.12462\n",
      "[16:30:43] [55]\ttraining-aucpr:0.12485\n",
      "[16:30:43] [56]\ttraining-aucpr:0.12515\n",
      "[16:30:43] [57]\ttraining-aucpr:0.12579\n",
      "[16:30:43] [58]\ttraining-aucpr:0.12709\n",
      "[16:30:43] [59]\ttraining-aucpr:0.12800\n",
      "[16:30:43] [60]\ttraining-aucpr:0.12878\n",
      "[16:30:43] [61]\ttraining-aucpr:0.12911\n",
      "[16:30:43] [62]\ttraining-aucpr:0.12941\n",
      "[16:30:43] [63]\ttraining-aucpr:0.13025\n",
      "[16:30:43] [64]\ttraining-aucpr:0.13094\n",
      "[16:30:43] [65]\ttraining-aucpr:0.13174\n",
      "[16:30:44] [66]\ttraining-aucpr:0.13220\n",
      "[16:30:44] [67]\ttraining-aucpr:0.13234\n",
      "[16:30:44] [68]\ttraining-aucpr:0.13272\n",
      "[16:30:44] [69]\ttraining-aucpr:0.13341\n",
      "[16:30:44] [70]\ttraining-aucpr:0.13350\n",
      "[16:30:44] [71]\ttraining-aucpr:0.13399\n",
      "[16:30:44] [72]\ttraining-aucpr:0.13451\n",
      "[16:30:44] [73]\ttraining-aucpr:0.13453\n",
      "[16:30:44] [74]\ttraining-aucpr:0.13457\n",
      "[16:30:44] [75]\ttraining-aucpr:0.13514\n",
      "[16:30:44] [76]\ttraining-aucpr:0.13527\n",
      "[16:30:44] [77]\ttraining-aucpr:0.13592\n",
      "[16:30:44] [78]\ttraining-aucpr:0.13657\n",
      "[16:30:44] [79]\ttraining-aucpr:0.13746\n",
      "[16:30:44] [80]\ttraining-aucpr:0.13778\n",
      "[16:30:44] [81]\ttraining-aucpr:0.13805\n",
      "[16:30:44] [82]\ttraining-aucpr:0.13830\n",
      "[16:30:44] [83]\ttraining-aucpr:0.13920\n",
      "[16:30:44] [84]\ttraining-aucpr:0.13956\n",
      "[16:30:44] [85]\ttraining-aucpr:0.13976\n",
      "[16:30:44] [86]\ttraining-aucpr:0.14021\n",
      "[16:30:44] [87]\ttraining-aucpr:0.14104\n",
      "[16:30:44] [88]\ttraining-aucpr:0.14119\n",
      "[16:30:44] [89]\ttraining-aucpr:0.14131\n",
      "[16:30:44] [90]\ttraining-aucpr:0.14184\n",
      "[16:30:44] [91]\ttraining-aucpr:0.14233\n",
      "[16:30:44] [92]\ttraining-aucpr:0.14255\n",
      "[16:30:44] [93]\ttraining-aucpr:0.14293\n",
      "[16:30:44] [94]\ttraining-aucpr:0.14337\n",
      "[16:30:44] [95]\ttraining-aucpr:0.14346\n",
      "[16:30:44] [96]\ttraining-aucpr:0.14383\n",
      "[16:30:44] [97]\ttraining-aucpr:0.14413\n",
      "[16:30:44] [98]\ttraining-aucpr:0.14441\n",
      "[16:30:44] [99]\ttraining-aucpr:0.14490\n",
      "[16:30:44] [100]\ttraining-aucpr:0.14516\n",
      "[16:30:44] [101]\ttraining-aucpr:0.14576\n",
      "[16:30:45] [102]\ttraining-aucpr:0.14582\n",
      "[16:30:45] [103]\ttraining-aucpr:0.14640\n",
      "[16:30:45] [104]\ttraining-aucpr:0.14699\n",
      "[16:30:45] [105]\ttraining-aucpr:0.14758\n",
      "[16:30:45] [106]\ttraining-aucpr:0.14782\n",
      "[16:30:45] [107]\ttraining-aucpr:0.14827\n",
      "[16:30:45] [108]\ttraining-aucpr:0.14867\n",
      "[16:30:45] [109]\ttraining-aucpr:0.14892\n",
      "[16:30:45] [110]\ttraining-aucpr:0.14913\n",
      "[16:30:45] [111]\ttraining-aucpr:0.14937\n",
      "[16:30:45] [112]\ttraining-aucpr:0.14991\n",
      "[16:30:45] [113]\ttraining-aucpr:0.15012\n",
      "[16:30:45] [114]\ttraining-aucpr:0.15036\n",
      "[16:30:45] [115]\ttraining-aucpr:0.15074\n",
      "[16:30:45] [116]\ttraining-aucpr:0.15101\n",
      "[16:30:45] [117]\ttraining-aucpr:0.15157\n",
      "[16:30:45] [118]\ttraining-aucpr:0.15173\n",
      "[16:30:45] [119]\ttraining-aucpr:0.15172\n",
      "[16:30:45] [120]\ttraining-aucpr:0.15206\n",
      "[16:30:45] [121]\ttraining-aucpr:0.15240\n",
      "[16:30:45] [122]\ttraining-aucpr:0.15271\n",
      "[16:30:45] [123]\ttraining-aucpr:0.15269\n",
      "[16:30:45] [124]\ttraining-aucpr:0.15302\n",
      "[16:30:45] [125]\ttraining-aucpr:0.15342\n",
      "[16:30:45] [126]\ttraining-aucpr:0.15376\n",
      "[16:30:45] [127]\ttraining-aucpr:0.15407\n",
      "[16:30:45] [128]\ttraining-aucpr:0.15443\n",
      "[16:30:45] [129]\ttraining-aucpr:0.15470\n",
      "[16:30:45] [130]\ttraining-aucpr:0.15515\n",
      "[16:30:45] [131]\ttraining-aucpr:0.15549\n",
      "[16:30:45] [132]\ttraining-aucpr:0.15585\n",
      "[16:30:45] [133]\ttraining-aucpr:0.15593\n",
      "[16:30:45] [134]\ttraining-aucpr:0.15631\n",
      "[16:30:45] [135]\ttraining-aucpr:0.15664\n",
      "[16:30:45] [136]\ttraining-aucpr:0.15685\n",
      "[16:30:45] [137]\ttraining-aucpr:0.15702\n",
      "[16:30:45] [138]\ttraining-aucpr:0.15722\n",
      "[16:30:46] [139]\ttraining-aucpr:0.15762\n",
      "[16:30:46] [140]\ttraining-aucpr:0.15792\n",
      "[16:30:46] [141]\ttraining-aucpr:0.15795\n",
      "[16:30:46] [142]\ttraining-aucpr:0.15833\n",
      "[16:30:46] [143]\ttraining-aucpr:0.15857\n",
      "[16:30:46] [144]\ttraining-aucpr:0.15892\n",
      "[16:30:46] [145]\ttraining-aucpr:0.15902\n",
      "[16:30:46] [146]\ttraining-aucpr:0.15923\n",
      "[16:30:46] [147]\ttraining-aucpr:0.15952\n",
      "[16:30:46] [148]\ttraining-aucpr:0.15980\n",
      "[16:30:46] [149]\ttraining-aucpr:0.16010\n",
      "[16:30:46] [150]\ttraining-aucpr:0.16030\n",
      "[16:30:46] [151]\ttraining-aucpr:0.16057\n",
      "2025-07-28 16:30:47,348 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:30:47,434 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:30:48,154] Trial 24 finished with value: 0.18027781784561572 and parameters: {'max_depth': 4, 'learning_rate': 0.04087971200804147, 'n_estimators': 152, 'min_child_weight': 3, 'gamma': 0.4382927217993571, 'subsample': 0.9121907767598478, 'colsample_bytree': 0.7547681063746714, 'reg_alpha': 0.16448804475298373, 'reg_lambda': 0.4428529824328987}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:30:48,275 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.8296451697182724, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.5899447471862365, 'learning_rate': 0.19261432873717874, 'max_depth': 3, 'min_child_weight': 1, 'reg_alpha': 0.5674627951082698, 'reg_lambda': 0.0009665995680023989, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.8924342149127673, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 80}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:30:49,663 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:30:50] Task 1 got rank 1\n",
      "[16:30:50] Task 2 got rank 2\n",
      "[16:30:50] Task 0 got rank 0\n",
      "[16:30:50] Task 3 got rank 3\n",
      "[16:30:51] [0]\ttraining-aucpr:0.04710\n",
      "[16:30:51] [1]\ttraining-aucpr:0.04817\n",
      "[16:30:51] [2]\ttraining-aucpr:0.05532\n",
      "[16:30:51] [3]\ttraining-aucpr:0.05938\n",
      "[16:30:51] [4]\ttraining-aucpr:0.05998\n",
      "[16:30:51] [5]\ttraining-aucpr:0.06224\n",
      "[16:30:51] [6]\ttraining-aucpr:0.07561\n",
      "[16:30:51] [7]\ttraining-aucpr:0.08301\n",
      "[16:30:51] [8]\ttraining-aucpr:0.09308\n",
      "[16:30:51] [9]\ttraining-aucpr:0.10053\n",
      "[16:30:51] [10]\ttraining-aucpr:0.10454\n",
      "[16:30:51] [11]\ttraining-aucpr:0.10444\n",
      "[16:30:51] [12]\ttraining-aucpr:0.11318\n",
      "[16:30:51] [13]\ttraining-aucpr:0.11578\n",
      "[16:30:51] [14]\ttraining-aucpr:0.11729\n",
      "[16:30:51] [15]\ttraining-aucpr:0.11819\n",
      "[16:30:51] [16]\ttraining-aucpr:0.12128\n",
      "[16:30:51] [17]\ttraining-aucpr:0.12496\n",
      "[16:30:51] [18]\ttraining-aucpr:0.12698\n",
      "[16:30:51] [19]\ttraining-aucpr:0.12704\n",
      "[16:30:51] [20]\ttraining-aucpr:0.13232\n",
      "[16:30:51] [21]\ttraining-aucpr:0.13274\n",
      "[16:30:51] [22]\ttraining-aucpr:0.13409\n",
      "[16:30:52] [23]\ttraining-aucpr:0.13398\n",
      "[16:30:52] [24]\ttraining-aucpr:0.13479\n",
      "[16:30:52] [25]\ttraining-aucpr:0.13581\n",
      "[16:30:52] [26]\ttraining-aucpr:0.13836\n",
      "[16:30:52] [27]\ttraining-aucpr:0.14050\n",
      "[16:30:52] [28]\ttraining-aucpr:0.14051\n",
      "[16:30:52] [29]\ttraining-aucpr:0.14222\n",
      "[16:30:52] [30]\ttraining-aucpr:0.14437\n",
      "[16:30:52] [31]\ttraining-aucpr:0.14488\n",
      "[16:30:52] [32]\ttraining-aucpr:0.14517\n",
      "[16:30:52] [33]\ttraining-aucpr:0.14663\n",
      "[16:30:52] [34]\ttraining-aucpr:0.14773\n",
      "[16:30:52] [35]\ttraining-aucpr:0.14864\n",
      "[16:30:52] [36]\ttraining-aucpr:0.15011\n",
      "[16:30:52] [37]\ttraining-aucpr:0.15148\n",
      "[16:30:52] [38]\ttraining-aucpr:0.15275\n",
      "[16:30:52] [39]\ttraining-aucpr:0.15367\n",
      "[16:30:52] [40]\ttraining-aucpr:0.15393\n",
      "[16:30:52] [41]\ttraining-aucpr:0.15379\n",
      "[16:30:52] [42]\ttraining-aucpr:0.15463\n",
      "[16:30:52] [43]\ttraining-aucpr:0.15542\n",
      "[16:30:52] [44]\ttraining-aucpr:0.15544\n",
      "[16:30:52] [45]\ttraining-aucpr:0.15568\n",
      "[16:30:52] [46]\ttraining-aucpr:0.15668\n",
      "[16:30:52] [47]\ttraining-aucpr:0.15720\n",
      "[16:30:52] [48]\ttraining-aucpr:0.15786\n",
      "[16:30:52] [49]\ttraining-aucpr:0.15810\n",
      "[16:30:53] [50]\ttraining-aucpr:0.15842\n",
      "[16:30:53] [51]\ttraining-aucpr:0.15921\n",
      "[16:30:53] [52]\ttraining-aucpr:0.15981\n",
      "[16:30:53] [53]\ttraining-aucpr:0.16041\n",
      "[16:30:53] [54]\ttraining-aucpr:0.16055\n",
      "[16:30:53] [55]\ttraining-aucpr:0.16090\n",
      "[16:30:53] [56]\ttraining-aucpr:0.16119\n",
      "[16:30:53] [57]\ttraining-aucpr:0.16183\n",
      "[16:30:53] [58]\ttraining-aucpr:0.16218\n",
      "[16:30:53] [59]\ttraining-aucpr:0.16238\n",
      "[16:30:53] [60]\ttraining-aucpr:0.16300\n",
      "[16:30:53] [61]\ttraining-aucpr:0.16359\n",
      "[16:30:53] [62]\ttraining-aucpr:0.16378\n",
      "[16:30:53] [63]\ttraining-aucpr:0.16402\n",
      "[16:30:53] [64]\ttraining-aucpr:0.16436\n",
      "[16:30:53] [65]\ttraining-aucpr:0.16442\n",
      "[16:30:53] [66]\ttraining-aucpr:0.16457\n",
      "[16:30:53] [67]\ttraining-aucpr:0.16494\n",
      "[16:30:53] [68]\ttraining-aucpr:0.16507\n",
      "[16:30:53] [69]\ttraining-aucpr:0.16546\n",
      "[16:30:53] [70]\ttraining-aucpr:0.16593\n",
      "[16:30:53] [71]\ttraining-aucpr:0.16615\n",
      "[16:30:53] [72]\ttraining-aucpr:0.16659\n",
      "[16:30:53] [73]\ttraining-aucpr:0.16669\n",
      "[16:30:53] [74]\ttraining-aucpr:0.16679\n",
      "[16:30:53] [75]\ttraining-aucpr:0.16724\n",
      "[16:30:53] [76]\ttraining-aucpr:0.16732\n",
      "[16:30:53] [77]\ttraining-aucpr:0.16745\n",
      "[16:30:54] [78]\ttraining-aucpr:0.16773\n",
      "[16:30:54] [79]\ttraining-aucpr:0.16807\n",
      "2025-07-28 16:30:55,073 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:30:55,162 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:30:55,797] Trial 25 finished with value: 0.18666292590029307 and parameters: {'max_depth': 3, 'learning_rate': 0.19261432873717874, 'n_estimators': 80, 'min_child_weight': 1, 'gamma': 0.5899447471862365, 'subsample': 0.8924342149127673, 'colsample_bytree': 0.8296451697182724, 'reg_alpha': 0.5674627951082698, 'reg_lambda': 0.0009665995680023989}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:30:55,924 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.922239330301282, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.7309998564273487, 'learning_rate': 0.07175088264153578, 'max_depth': 4, 'min_child_weight': 9, 'reg_alpha': 0.424787776422928, 'reg_lambda': 0.35776043590341905, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9594620581908287, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 55}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:30:57,342 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:30:58] Task 2 got rank 2[16:30:58] Task 1 got rank 1[16:30:58] Task 0 got rank 0\n",
      "\n",
      "\n",
      "[16:30:58] Task 3 got rank 3\n",
      "[16:30:58] [0]\ttraining-aucpr:0.05562\n",
      "[16:30:58] [1]\ttraining-aucpr:0.05981\n",
      "[16:30:58] [2]\ttraining-aucpr:0.06137\n",
      "[16:30:58] [3]\ttraining-aucpr:0.06710\n",
      "[16:30:58] [4]\ttraining-aucpr:0.07366\n",
      "[16:30:59] [5]\ttraining-aucpr:0.07356\n",
      "[16:30:59] [6]\ttraining-aucpr:0.07453\n",
      "[16:30:59] [7]\ttraining-aucpr:0.07514\n",
      "[16:30:59] [8]\ttraining-aucpr:0.08216\n",
      "[16:30:59] [9]\ttraining-aucpr:0.08247\n",
      "[16:30:59] [10]\ttraining-aucpr:0.08377\n",
      "[16:30:59] [11]\ttraining-aucpr:0.08374\n",
      "[16:30:59] [12]\ttraining-aucpr:0.08626\n",
      "[16:30:59] [13]\ttraining-aucpr:0.08996\n",
      "[16:30:59] [14]\ttraining-aucpr:0.08994\n",
      "[16:30:59] [15]\ttraining-aucpr:0.09061\n",
      "[16:30:59] [16]\ttraining-aucpr:0.09761\n",
      "[16:30:59] [17]\ttraining-aucpr:0.09861\n",
      "[16:30:59] [18]\ttraining-aucpr:0.10279\n",
      "[16:30:59] [19]\ttraining-aucpr:0.10568\n",
      "[16:30:59] [20]\ttraining-aucpr:0.10965\n",
      "[16:30:59] [21]\ttraining-aucpr:0.11135\n",
      "[16:30:59] [22]\ttraining-aucpr:0.11286\n",
      "[16:30:59] [23]\ttraining-aucpr:0.11493\n",
      "[16:30:59] [24]\ttraining-aucpr:0.11629\n",
      "[16:30:59] [25]\ttraining-aucpr:0.11791\n",
      "[16:30:59] [26]\ttraining-aucpr:0.11887\n",
      "[16:30:59] [27]\ttraining-aucpr:0.12046\n",
      "[16:30:59] [28]\ttraining-aucpr:0.12117\n",
      "[16:30:59] [29]\ttraining-aucpr:0.12171\n",
      "[16:30:59] [30]\ttraining-aucpr:0.12304\n",
      "[16:30:59] [31]\ttraining-aucpr:0.12398\n",
      "[16:30:59] [32]\ttraining-aucpr:0.12552\n",
      "[16:30:59] [33]\ttraining-aucpr:0.12656\n",
      "[16:30:59] [34]\ttraining-aucpr:0.12662\n",
      "[16:30:59] [35]\ttraining-aucpr:0.12665\n",
      "[16:30:59] [36]\ttraining-aucpr:0.12789\n",
      "[16:30:59] [37]\ttraining-aucpr:0.12949\n",
      "[16:30:59] [38]\ttraining-aucpr:0.13006\n",
      "[16:30:59] [39]\ttraining-aucpr:0.13167\n",
      "[16:30:59] [40]\ttraining-aucpr:0.13286\n",
      "[16:30:59] [41]\ttraining-aucpr:0.13357\n",
      "[16:30:59] [42]\ttraining-aucpr:0.13453\n",
      "[16:30:59] [43]\ttraining-aucpr:0.13524\n",
      "[16:31:00] [44]\ttraining-aucpr:0.13575\n",
      "[16:31:00] [45]\ttraining-aucpr:0.13735\n",
      "[16:31:00] [46]\ttraining-aucpr:0.13795\n",
      "[16:31:00] [47]\ttraining-aucpr:0.13890\n",
      "[16:31:00] [48]\ttraining-aucpr:0.13942\n",
      "[16:31:00] [49]\ttraining-aucpr:0.13976\n",
      "[16:31:00] [50]\ttraining-aucpr:0.14069\n",
      "[16:31:00] [51]\ttraining-aucpr:0.14067\n",
      "[16:31:00] [52]\ttraining-aucpr:0.14096\n",
      "[16:31:00] [53]\ttraining-aucpr:0.14211\n",
      "[16:31:00] [54]\ttraining-aucpr:0.14286\n",
      "2025-07-28 16:31:01,306 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:31:01,383 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:31:02,017] Trial 26 finished with value: 0.1672989723651935 and parameters: {'max_depth': 4, 'learning_rate': 0.07175088264153578, 'n_estimators': 55, 'min_child_weight': 9, 'gamma': 0.7309998564273487, 'subsample': 0.9594620581908287, 'colsample_bytree': 0.922239330301282, 'reg_alpha': 0.424787776422928, 'reg_lambda': 0.35776043590341905}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:31:02,145 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.7803536574893956, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.8543127733842326, 'learning_rate': 0.11116121271017705, 'max_depth': 6, 'min_child_weight': 6, 'reg_alpha': 0.3591584700384163, 'reg_lambda': 0.1928872809064608, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.7944450588347721, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 162}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:31:03,522 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:31:04] Task 3 got rank 3[16:31:04] Task 0 got rank 0\n",
      "\n",
      "[16:31:04] Task 1 got rank 1[16:31:04] Task 2 got rank 2\n",
      "\n",
      "[16:31:05] [0]\ttraining-aucpr:0.07246\n",
      "[16:31:05] [1]\ttraining-aucpr:0.09041\n",
      "[16:31:05] [2]\ttraining-aucpr:0.09753\n",
      "[16:31:05] [3]\ttraining-aucpr:0.10040\n",
      "[16:31:05] [4]\ttraining-aucpr:0.10282\n",
      "[16:31:05] [5]\ttraining-aucpr:0.10879\n",
      "[16:31:05] [6]\ttraining-aucpr:0.11177\n",
      "[16:31:05] [7]\ttraining-aucpr:0.11376\n",
      "[16:31:05] [8]\ttraining-aucpr:0.11491\n",
      "[16:31:05] [9]\ttraining-aucpr:0.12175\n",
      "[16:31:05] [10]\ttraining-aucpr:0.12313\n",
      "[16:31:05] [11]\ttraining-aucpr:0.12448\n",
      "[16:31:05] [12]\ttraining-aucpr:0.12658\n",
      "[16:31:05] [13]\ttraining-aucpr:0.12896\n",
      "[16:31:05] [14]\ttraining-aucpr:0.13138\n",
      "[16:31:05] [15]\ttraining-aucpr:0.13520\n",
      "[16:31:05] [16]\ttraining-aucpr:0.13865\n",
      "[16:31:05] [17]\ttraining-aucpr:0.14164\n",
      "[16:31:05] [18]\ttraining-aucpr:0.14358\n",
      "[16:31:05] [19]\ttraining-aucpr:0.14552\n",
      "[16:31:05] [20]\ttraining-aucpr:0.14733\n",
      "[16:31:05] [21]\ttraining-aucpr:0.14864\n",
      "[16:31:05] [22]\ttraining-aucpr:0.15067\n",
      "[16:31:05] [23]\ttraining-aucpr:0.15115\n",
      "[16:31:06] [24]\ttraining-aucpr:0.15360\n",
      "[16:31:06] [25]\ttraining-aucpr:0.15456\n",
      "[16:31:06] [26]\ttraining-aucpr:0.15660\n",
      "[16:31:06] [27]\ttraining-aucpr:0.15858\n",
      "[16:31:06] [28]\ttraining-aucpr:0.15954\n",
      "[16:31:06] [29]\ttraining-aucpr:0.16100\n",
      "[16:31:06] [30]\ttraining-aucpr:0.16242\n",
      "[16:31:06] [31]\ttraining-aucpr:0.16413\n",
      "[16:31:06] [32]\ttraining-aucpr:0.16577\n",
      "[16:31:06] [33]\ttraining-aucpr:0.16720\n",
      "[16:31:06] [34]\ttraining-aucpr:0.16791\n",
      "[16:31:06] [35]\ttraining-aucpr:0.16893\n",
      "[16:31:06] [36]\ttraining-aucpr:0.17080\n",
      "[16:31:06] [37]\ttraining-aucpr:0.17191\n",
      "[16:31:06] [38]\ttraining-aucpr:0.17240\n",
      "[16:31:06] [39]\ttraining-aucpr:0.17418\n",
      "[16:31:06] [40]\ttraining-aucpr:0.17510\n",
      "[16:31:06] [41]\ttraining-aucpr:0.17621\n",
      "[16:31:06] [42]\ttraining-aucpr:0.17728\n",
      "[16:31:06] [43]\ttraining-aucpr:0.17800\n",
      "[16:31:06] [44]\ttraining-aucpr:0.17854\n",
      "[16:31:06] [45]\ttraining-aucpr:0.17997\n",
      "[16:31:06] [46]\ttraining-aucpr:0.18137\n",
      "[16:31:06] [47]\ttraining-aucpr:0.18269\n",
      "[16:31:06] [48]\ttraining-aucpr:0.18360\n",
      "[16:31:06] [49]\ttraining-aucpr:0.18430\n",
      "[16:31:07] [50]\ttraining-aucpr:0.18536\n",
      "[16:31:07] [51]\ttraining-aucpr:0.18598\n",
      "[16:31:07] [52]\ttraining-aucpr:0.18682\n",
      "[16:31:07] [53]\ttraining-aucpr:0.18707\n",
      "[16:31:07] [54]\ttraining-aucpr:0.18766\n",
      "[16:31:07] [55]\ttraining-aucpr:0.18809\n",
      "[16:31:07] [56]\ttraining-aucpr:0.18879\n",
      "[16:31:07] [57]\ttraining-aucpr:0.18945\n",
      "[16:31:07] [58]\ttraining-aucpr:0.19027\n",
      "[16:31:07] [59]\ttraining-aucpr:0.19107\n",
      "[16:31:07] [60]\ttraining-aucpr:0.19144\n",
      "[16:31:07] [61]\ttraining-aucpr:0.19213\n",
      "[16:31:07] [62]\ttraining-aucpr:0.19294\n",
      "[16:31:07] [63]\ttraining-aucpr:0.19350\n",
      "[16:31:07] [64]\ttraining-aucpr:0.19418\n",
      "[16:31:07] [65]\ttraining-aucpr:0.19475\n",
      "[16:31:07] [66]\ttraining-aucpr:0.19519\n",
      "[16:31:07] [67]\ttraining-aucpr:0.19556\n",
      "[16:31:07] [68]\ttraining-aucpr:0.19587\n",
      "[16:31:07] [69]\ttraining-aucpr:0.19629\n",
      "[16:31:07] [70]\ttraining-aucpr:0.19714\n",
      "[16:31:07] [71]\ttraining-aucpr:0.19758\n",
      "[16:31:07] [72]\ttraining-aucpr:0.19846\n",
      "[16:31:07] [73]\ttraining-aucpr:0.19860\n",
      "[16:31:07] [74]\ttraining-aucpr:0.19918\n",
      "[16:31:07] [75]\ttraining-aucpr:0.19998\n",
      "[16:31:07] [76]\ttraining-aucpr:0.20028\n",
      "[16:31:07] [77]\ttraining-aucpr:0.20069\n",
      "[16:31:07] [78]\ttraining-aucpr:0.20148\n",
      "[16:31:07] [79]\ttraining-aucpr:0.20199\n",
      "[16:31:07] [80]\ttraining-aucpr:0.20233\n",
      "[16:31:07] [81]\ttraining-aucpr:0.20290\n",
      "[16:31:08] [82]\ttraining-aucpr:0.20335\n",
      "[16:31:08] [83]\ttraining-aucpr:0.20404\n",
      "[16:31:08] [84]\ttraining-aucpr:0.20434\n",
      "[16:31:08] [85]\ttraining-aucpr:0.20459\n",
      "[16:31:08] [86]\ttraining-aucpr:0.20499\n",
      "[16:31:08] [87]\ttraining-aucpr:0.20608\n",
      "[16:31:08] [88]\ttraining-aucpr:0.20638\n",
      "[16:31:08] [89]\ttraining-aucpr:0.20681\n",
      "[16:31:08] [90]\ttraining-aucpr:0.20750\n",
      "[16:31:08] [91]\ttraining-aucpr:0.20783\n",
      "[16:31:08] [92]\ttraining-aucpr:0.20796\n",
      "[16:31:08] [93]\ttraining-aucpr:0.20819\n",
      "[16:31:08] [94]\ttraining-aucpr:0.20847\n",
      "[16:31:08] [95]\ttraining-aucpr:0.20859\n",
      "[16:31:08] [96]\ttraining-aucpr:0.20872\n",
      "[16:31:08] [97]\ttraining-aucpr:0.20914\n",
      "[16:31:08] [98]\ttraining-aucpr:0.20934\n",
      "[16:31:08] [99]\ttraining-aucpr:0.20963\n",
      "[16:31:08] [100]\ttraining-aucpr:0.21006\n",
      "[16:31:08] [101]\ttraining-aucpr:0.21046\n",
      "[16:31:08] [102]\ttraining-aucpr:0.21053\n",
      "[16:31:08] [103]\ttraining-aucpr:0.21085\n",
      "[16:31:08] [104]\ttraining-aucpr:0.21159\n",
      "[16:31:08] [105]\ttraining-aucpr:0.21221\n",
      "[16:31:08] [106]\ttraining-aucpr:0.21282\n",
      "[16:31:08] [107]\ttraining-aucpr:0.21296\n",
      "[16:31:08] [108]\ttraining-aucpr:0.21339\n",
      "[16:31:08] [109]\ttraining-aucpr:0.21375\n",
      "[16:31:08] [110]\ttraining-aucpr:0.21439\n",
      "[16:31:08] [111]\ttraining-aucpr:0.21480\n",
      "[16:31:08] [112]\ttraining-aucpr:0.21515\n",
      "[16:31:08] [113]\ttraining-aucpr:0.21553\n",
      "[16:31:09] [114]\ttraining-aucpr:0.21592\n",
      "[16:31:09] [115]\ttraining-aucpr:0.21625\n",
      "[16:31:09] [116]\ttraining-aucpr:0.21676\n",
      "[16:31:09] [117]\ttraining-aucpr:0.21727\n",
      "[16:31:09] [118]\ttraining-aucpr:0.21754\n",
      "[16:31:09] [119]\ttraining-aucpr:0.21765\n",
      "[16:31:09] [120]\ttraining-aucpr:0.21802\n",
      "[16:31:09] [121]\ttraining-aucpr:0.21834\n",
      "[16:31:09] [122]\ttraining-aucpr:0.21904\n",
      "[16:31:09] [123]\ttraining-aucpr:0.21966\n",
      "[16:31:09] [124]\ttraining-aucpr:0.22029\n",
      "[16:31:09] [125]\ttraining-aucpr:0.22078\n",
      "[16:31:09] [126]\ttraining-aucpr:0.22116\n",
      "[16:31:09] [127]\ttraining-aucpr:0.22136\n",
      "[16:31:09] [128]\ttraining-aucpr:0.22165\n",
      "[16:31:09] [129]\ttraining-aucpr:0.22251\n",
      "[16:31:09] [130]\ttraining-aucpr:0.22298\n",
      "[16:31:09] [131]\ttraining-aucpr:0.22346\n",
      "[16:31:09] [132]\ttraining-aucpr:0.22363\n",
      "[16:31:09] [133]\ttraining-aucpr:0.22391\n",
      "[16:31:09] [134]\ttraining-aucpr:0.22408\n",
      "[16:31:09] [135]\ttraining-aucpr:0.22425\n",
      "[16:31:09] [136]\ttraining-aucpr:0.22453\n",
      "[16:31:09] [137]\ttraining-aucpr:0.22521\n",
      "[16:31:09] [138]\ttraining-aucpr:0.22524\n",
      "[16:31:09] [139]\ttraining-aucpr:0.22577\n",
      "[16:31:09] [140]\ttraining-aucpr:0.22608\n",
      "[16:31:09] [141]\ttraining-aucpr:0.22633\n",
      "[16:31:09] [142]\ttraining-aucpr:0.22664\n",
      "[16:31:09] [143]\ttraining-aucpr:0.22688\n",
      "[16:31:09] [144]\ttraining-aucpr:0.22732\n",
      "[16:31:09] [145]\ttraining-aucpr:0.22771\n",
      "[16:31:10] [146]\ttraining-aucpr:0.22799\n",
      "[16:31:10] [147]\ttraining-aucpr:0.22826\n",
      "[16:31:10] [148]\ttraining-aucpr:0.22845\n",
      "[16:31:10] [149]\ttraining-aucpr:0.22856\n",
      "[16:31:10] [150]\ttraining-aucpr:0.22911\n",
      "[16:31:10] [151]\ttraining-aucpr:0.22955\n",
      "[16:31:10] [152]\ttraining-aucpr:0.22965\n",
      "[16:31:10] [153]\ttraining-aucpr:0.22989\n",
      "[16:31:10] [154]\ttraining-aucpr:0.23006\n",
      "[16:31:10] [155]\ttraining-aucpr:0.23051\n",
      "[16:31:10] [156]\ttraining-aucpr:0.23081\n",
      "[16:31:10] [157]\ttraining-aucpr:0.23123\n",
      "[16:31:10] [158]\ttraining-aucpr:0.23152\n",
      "[16:31:10] [159]\ttraining-aucpr:0.23183\n",
      "[16:31:10] [160]\ttraining-aucpr:0.23212\n",
      "[16:31:10] [161]\ttraining-aucpr:0.23261\n",
      "2025-07-28 16:31:11,500 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:31:11,586 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:31:12,291] Trial 27 finished with value: 0.17379588292734266 and parameters: {'max_depth': 6, 'learning_rate': 0.11116121271017705, 'n_estimators': 162, 'min_child_weight': 6, 'gamma': 0.8543127733842326, 'subsample': 0.7944450588347721, 'colsample_bytree': 0.7803536574893956, 'reg_alpha': 0.3591584700384163, 'reg_lambda': 0.1928872809064608}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:31:12,394 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.7215514614076, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.4571091340412784, 'learning_rate': 0.04130154195919684, 'max_depth': 5, 'min_child_weight': 9, 'reg_alpha': 0.7049211433340475, 'reg_lambda': 0.07750272359330201, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9572120723414633, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 111}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:31:13,777 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:31:14] Task 0 got rank 0\n",
      "[16:31:14] Task 2 got rank 2\n",
      "[16:31:14] Task 3 got rank 3[16:31:14] Task 1 got rank 1\n",
      "\n",
      "[16:31:15] [0]\ttraining-aucpr:0.06262\n",
      "[16:31:15] [1]\ttraining-aucpr:0.07243\n",
      "[16:31:15] [2]\ttraining-aucpr:0.07266\n",
      "[16:31:15] [3]\ttraining-aucpr:0.08057\n",
      "[16:31:15] [4]\ttraining-aucpr:0.08092\n",
      "[16:31:15] [5]\ttraining-aucpr:0.08772\n",
      "[16:31:15] [6]\ttraining-aucpr:0.09499\n",
      "[16:31:15] [7]\ttraining-aucpr:0.09502\n",
      "[16:31:15] [8]\ttraining-aucpr:0.09600\n",
      "[16:31:15] [9]\ttraining-aucpr:0.09859\n",
      "[16:31:15] [10]\ttraining-aucpr:0.09881\n",
      "[16:31:15] [11]\ttraining-aucpr:0.09908\n",
      "[16:31:15] [12]\ttraining-aucpr:0.09971\n",
      "[16:31:15] [13]\ttraining-aucpr:0.10203\n",
      "[16:31:15] [14]\ttraining-aucpr:0.10206\n",
      "[16:31:15] [15]\ttraining-aucpr:0.10187\n",
      "[16:31:15] [16]\ttraining-aucpr:0.10293\n",
      "[16:31:15] [17]\ttraining-aucpr:0.10387\n",
      "[16:31:15] [18]\ttraining-aucpr:0.10418\n",
      "[16:31:15] [19]\ttraining-aucpr:0.10823\n",
      "[16:31:15] [20]\ttraining-aucpr:0.10935\n",
      "[16:31:15] [21]\ttraining-aucpr:0.10983\n",
      "[16:31:15] [22]\ttraining-aucpr:0.10997\n",
      "[16:31:16] [23]\ttraining-aucpr:0.11013\n",
      "[16:31:16] [24]\ttraining-aucpr:0.11194\n",
      "[16:31:16] [25]\ttraining-aucpr:0.11393\n",
      "[16:31:16] [26]\ttraining-aucpr:0.11524\n",
      "[16:31:16] [27]\ttraining-aucpr:0.11624\n",
      "[16:31:16] [28]\ttraining-aucpr:0.11679\n",
      "[16:31:16] [29]\ttraining-aucpr:0.11735\n",
      "[16:31:16] [30]\ttraining-aucpr:0.11777\n",
      "[16:31:16] [31]\ttraining-aucpr:0.11844\n",
      "[16:31:16] [32]\ttraining-aucpr:0.11983\n",
      "[16:31:16] [33]\ttraining-aucpr:0.12012\n",
      "[16:31:16] [34]\ttraining-aucpr:0.12008\n",
      "[16:31:16] [35]\ttraining-aucpr:0.12071\n",
      "[16:31:16] [36]\ttraining-aucpr:0.12125\n",
      "[16:31:16] [37]\ttraining-aucpr:0.12318\n",
      "[16:31:16] [38]\ttraining-aucpr:0.12466\n",
      "[16:31:16] [39]\ttraining-aucpr:0.12566\n",
      "[16:31:16] [40]\ttraining-aucpr:0.12630\n",
      "[16:31:16] [41]\ttraining-aucpr:0.12694\n",
      "[16:31:16] [42]\ttraining-aucpr:0.12763\n",
      "[16:31:16] [43]\ttraining-aucpr:0.12872\n",
      "[16:31:16] [44]\ttraining-aucpr:0.12916\n",
      "[16:31:16] [45]\ttraining-aucpr:0.13015\n",
      "[16:31:16] [46]\ttraining-aucpr:0.13091\n",
      "[16:31:16] [47]\ttraining-aucpr:0.13131\n",
      "[16:31:16] [48]\ttraining-aucpr:0.13247\n",
      "[16:31:16] [49]\ttraining-aucpr:0.13345\n",
      "[16:31:16] [50]\ttraining-aucpr:0.13403\n",
      "[16:31:16] [51]\ttraining-aucpr:0.13438\n",
      "[16:31:16] [52]\ttraining-aucpr:0.13476\n",
      "[16:31:16] [53]\ttraining-aucpr:0.13517\n",
      "[16:31:16] [54]\ttraining-aucpr:0.13658\n",
      "[16:31:17] [55]\ttraining-aucpr:0.13739\n",
      "[16:31:17] [56]\ttraining-aucpr:0.13793\n",
      "[16:31:17] [57]\ttraining-aucpr:0.13818\n",
      "[16:31:17] [58]\ttraining-aucpr:0.13867\n",
      "[16:31:17] [59]\ttraining-aucpr:0.13900\n",
      "[16:31:17] [60]\ttraining-aucpr:0.13958\n",
      "[16:31:17] [61]\ttraining-aucpr:0.14006\n",
      "[16:31:17] [62]\ttraining-aucpr:0.14109\n",
      "[16:31:17] [63]\ttraining-aucpr:0.14175\n",
      "[16:31:17] [64]\ttraining-aucpr:0.14213\n",
      "[16:31:17] [65]\ttraining-aucpr:0.14266\n",
      "[16:31:17] [66]\ttraining-aucpr:0.14363\n",
      "[16:31:17] [67]\ttraining-aucpr:0.14397\n",
      "[16:31:17] [68]\ttraining-aucpr:0.14427\n",
      "[16:31:17] [69]\ttraining-aucpr:0.14474\n",
      "[16:31:17] [70]\ttraining-aucpr:0.14548\n",
      "[16:31:17] [71]\ttraining-aucpr:0.14584\n",
      "[16:31:17] [72]\ttraining-aucpr:0.14666\n",
      "[16:31:17] [73]\ttraining-aucpr:0.14707\n",
      "[16:31:17] [74]\ttraining-aucpr:0.14774\n",
      "[16:31:17] [75]\ttraining-aucpr:0.14815\n",
      "[16:31:17] [76]\ttraining-aucpr:0.14879\n",
      "[16:31:17] [77]\ttraining-aucpr:0.14904\n",
      "[16:31:17] [78]\ttraining-aucpr:0.14933\n",
      "[16:31:17] [79]\ttraining-aucpr:0.14961\n",
      "[16:31:17] [80]\ttraining-aucpr:0.15002\n",
      "[16:31:17] [81]\ttraining-aucpr:0.15020\n",
      "[16:31:17] [82]\ttraining-aucpr:0.15072\n",
      "[16:31:17] [83]\ttraining-aucpr:0.15113\n",
      "[16:31:17] [84]\ttraining-aucpr:0.15157\n",
      "[16:31:17] [85]\ttraining-aucpr:0.15178\n",
      "[16:31:17] [86]\ttraining-aucpr:0.15252\n",
      "[16:31:18] [87]\ttraining-aucpr:0.15296\n",
      "[16:31:18] [88]\ttraining-aucpr:0.15326\n",
      "[16:31:18] [89]\ttraining-aucpr:0.15353\n",
      "[16:31:18] [90]\ttraining-aucpr:0.15373\n",
      "[16:31:18] [91]\ttraining-aucpr:0.15426\n",
      "[16:31:18] [92]\ttraining-aucpr:0.15474\n",
      "[16:31:18] [93]\ttraining-aucpr:0.15492\n",
      "[16:31:18] [94]\ttraining-aucpr:0.15537\n",
      "[16:31:18] [95]\ttraining-aucpr:0.15563\n",
      "[16:31:18] [96]\ttraining-aucpr:0.15615\n",
      "[16:31:18] [97]\ttraining-aucpr:0.15628\n",
      "[16:31:18] [98]\ttraining-aucpr:0.15655\n",
      "[16:31:18] [99]\ttraining-aucpr:0.15726\n",
      "[16:31:18] [100]\ttraining-aucpr:0.15758\n",
      "[16:31:18] [101]\ttraining-aucpr:0.15807\n",
      "[16:31:18] [102]\ttraining-aucpr:0.15814\n",
      "[16:31:18] [103]\ttraining-aucpr:0.15858\n",
      "[16:31:18] [104]\ttraining-aucpr:0.15863\n",
      "[16:31:18] [105]\ttraining-aucpr:0.15900\n",
      "[16:31:18] [106]\ttraining-aucpr:0.15938\n",
      "[16:31:18] [107]\ttraining-aucpr:0.15961\n",
      "[16:31:18] [108]\ttraining-aucpr:0.16013\n",
      "[16:31:18] [109]\ttraining-aucpr:0.16041\n",
      "[16:31:18] [110]\ttraining-aucpr:0.16092\n",
      "2025-07-28 16:31:19,755 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:31:19,849 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:31:20,572] Trial 28 finished with value: 0.1770167344677305 and parameters: {'max_depth': 5, 'learning_rate': 0.04130154195919684, 'n_estimators': 111, 'min_child_weight': 9, 'gamma': 0.4571091340412784, 'subsample': 0.9572120723414633, 'colsample_bytree': 0.7215514614076, 'reg_alpha': 0.7049211433340475, 'reg_lambda': 0.07750272359330201}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:31:20,700 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.8502932763565828, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.8929852013655546, 'learning_rate': 0.27468827469748175, 'max_depth': 5, 'min_child_weight': 6, 'reg_alpha': 0.5188184434248876, 'reg_lambda': 0.6002411647513859, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.8206519441663025, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 138}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:31:22,089 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:31:23] Task 1 got rank 1\n",
      "[16:31:23] Task 2 got rank 2\n",
      "[16:31:23] Task 0 got rank 0\n",
      "[16:31:23] Task 3 got rank 3\n",
      "[16:31:23] [0]\ttraining-aucpr:0.06143\n",
      "[16:31:23] [1]\ttraining-aucpr:0.07645\n",
      "[16:31:23] [2]\ttraining-aucpr:0.08934\n",
      "[16:31:23] [3]\ttraining-aucpr:0.09749\n",
      "[16:31:23] [4]\ttraining-aucpr:0.10646\n",
      "[16:31:23] [5]\ttraining-aucpr:0.11104\n",
      "[16:31:23] [6]\ttraining-aucpr:0.11868\n",
      "[16:31:23] [7]\ttraining-aucpr:0.12304\n",
      "[16:31:23] [8]\ttraining-aucpr:0.12521\n",
      "[16:31:23] [9]\ttraining-aucpr:0.13127\n",
      "[16:31:23] [10]\ttraining-aucpr:0.13457\n",
      "[16:31:23] [11]\ttraining-aucpr:0.13977\n",
      "[16:31:24] [12]\ttraining-aucpr:0.14296\n",
      "[16:31:24] [13]\ttraining-aucpr:0.14600\n",
      "[16:31:24] [14]\ttraining-aucpr:0.15019\n",
      "[16:31:24] [15]\ttraining-aucpr:0.15302\n",
      "[16:31:24] [16]\ttraining-aucpr:0.15492\n",
      "[16:31:24] [17]\ttraining-aucpr:0.15740\n",
      "[16:31:24] [18]\ttraining-aucpr:0.15861\n",
      "[16:31:24] [19]\ttraining-aucpr:0.15941\n",
      "[16:31:24] [20]\ttraining-aucpr:0.16036\n",
      "[16:31:24] [21]\ttraining-aucpr:0.16089\n",
      "[16:31:24] [22]\ttraining-aucpr:0.16212\n",
      "[16:31:24] [23]\ttraining-aucpr:0.16333\n",
      "[16:31:24] [24]\ttraining-aucpr:0.16487\n",
      "[16:31:24] [25]\ttraining-aucpr:0.16670\n",
      "[16:31:24] [26]\ttraining-aucpr:0.16860\n",
      "[16:31:24] [27]\ttraining-aucpr:0.16957\n",
      "[16:31:24] [28]\ttraining-aucpr:0.17033\n",
      "[16:31:24] [29]\ttraining-aucpr:0.17183\n",
      "[16:31:24] [30]\ttraining-aucpr:0.17366\n",
      "[16:31:24] [31]\ttraining-aucpr:0.17451\n",
      "[16:31:24] [32]\ttraining-aucpr:0.17532\n",
      "[16:31:24] [33]\ttraining-aucpr:0.17597\n",
      "[16:31:24] [34]\ttraining-aucpr:0.17605\n",
      "[16:31:24] [35]\ttraining-aucpr:0.17638\n",
      "[16:31:24] [36]\ttraining-aucpr:0.17694\n",
      "[16:31:24] [37]\ttraining-aucpr:0.17753\n",
      "[16:31:24] [38]\ttraining-aucpr:0.17759\n",
      "[16:31:24] [39]\ttraining-aucpr:0.17845\n",
      "[16:31:24] [40]\ttraining-aucpr:0.17933\n",
      "[16:31:24] [41]\ttraining-aucpr:0.18039\n",
      "[16:31:24] [42]\ttraining-aucpr:0.18084\n",
      "[16:31:24] [43]\ttraining-aucpr:0.18091\n",
      "[16:31:24] [44]\ttraining-aucpr:0.18119\n",
      "[16:31:24] [45]\ttraining-aucpr:0.18189\n",
      "[16:31:24] [46]\ttraining-aucpr:0.18255\n",
      "[16:31:24] [47]\ttraining-aucpr:0.18326\n",
      "[16:31:25] [48]\ttraining-aucpr:0.18356\n",
      "[16:31:25] [49]\ttraining-aucpr:0.18401\n",
      "[16:31:25] [50]\ttraining-aucpr:0.18464\n",
      "[16:31:25] [51]\ttraining-aucpr:0.18524\n",
      "[16:31:25] [52]\ttraining-aucpr:0.18564\n",
      "[16:31:25] [53]\ttraining-aucpr:0.18587\n",
      "[16:31:25] [54]\ttraining-aucpr:0.18651\n",
      "[16:31:25] [55]\ttraining-aucpr:0.18703\n",
      "[16:31:25] [56]\ttraining-aucpr:0.18727\n",
      "[16:31:25] [57]\ttraining-aucpr:0.18759\n",
      "[16:31:25] [58]\ttraining-aucpr:0.18813\n",
      "[16:31:25] [59]\ttraining-aucpr:0.18875\n",
      "[16:31:25] [60]\ttraining-aucpr:0.18929\n",
      "[16:31:25] [61]\ttraining-aucpr:0.18989\n",
      "[16:31:25] [62]\ttraining-aucpr:0.19030\n",
      "[16:31:25] [63]\ttraining-aucpr:0.19044\n",
      "[16:31:25] [64]\ttraining-aucpr:0.19062\n",
      "[16:31:25] [65]\ttraining-aucpr:0.19054\n",
      "[16:31:25] [66]\ttraining-aucpr:0.19075\n",
      "[16:31:25] [67]\ttraining-aucpr:0.19093\n",
      "[16:31:25] [68]\ttraining-aucpr:0.19110\n",
      "[16:31:25] [69]\ttraining-aucpr:0.19120\n",
      "[16:31:25] [70]\ttraining-aucpr:0.19165\n",
      "[16:31:25] [71]\ttraining-aucpr:0.19217\n",
      "[16:31:25] [72]\ttraining-aucpr:0.19308\n",
      "[16:31:25] [73]\ttraining-aucpr:0.19322\n",
      "[16:31:25] [74]\ttraining-aucpr:0.19341\n",
      "[16:31:25] [75]\ttraining-aucpr:0.19405\n",
      "[16:31:25] [76]\ttraining-aucpr:0.19400\n",
      "[16:31:25] [77]\ttraining-aucpr:0.19471\n",
      "[16:31:25] [78]\ttraining-aucpr:0.19507\n",
      "[16:31:25] [79]\ttraining-aucpr:0.19539\n",
      "[16:31:25] [80]\ttraining-aucpr:0.19539\n",
      "[16:31:25] [81]\ttraining-aucpr:0.19554\n",
      "[16:31:25] [82]\ttraining-aucpr:0.19610\n",
      "[16:31:25] [83]\ttraining-aucpr:0.19650\n",
      "[16:31:25] [84]\ttraining-aucpr:0.19682\n",
      "[16:31:26] [85]\ttraining-aucpr:0.19725\n",
      "[16:31:26] [86]\ttraining-aucpr:0.19733\n",
      "[16:31:26] [87]\ttraining-aucpr:0.19768\n",
      "[16:31:26] [88]\ttraining-aucpr:0.19807\n",
      "[16:31:26] [89]\ttraining-aucpr:0.19862\n",
      "[16:31:26] [90]\ttraining-aucpr:0.19873\n",
      "[16:31:26] [91]\ttraining-aucpr:0.19916\n",
      "[16:31:26] [92]\ttraining-aucpr:0.19921\n",
      "[16:31:26] [93]\ttraining-aucpr:0.19945\n",
      "[16:31:26] [94]\ttraining-aucpr:0.19994\n",
      "[16:31:26] [95]\ttraining-aucpr:0.20028\n",
      "[16:31:26] [96]\ttraining-aucpr:0.20072\n",
      "[16:31:26] [97]\ttraining-aucpr:0.20105\n",
      "[16:31:26] [98]\ttraining-aucpr:0.20130\n",
      "[16:31:26] [99]\ttraining-aucpr:0.20150\n",
      "[16:31:26] [100]\ttraining-aucpr:0.20183\n",
      "[16:31:26] [101]\ttraining-aucpr:0.20197\n",
      "[16:31:26] [102]\ttraining-aucpr:0.20229\n",
      "[16:31:26] [103]\ttraining-aucpr:0.20232\n",
      "[16:31:26] [104]\ttraining-aucpr:0.20272\n",
      "[16:31:26] [105]\ttraining-aucpr:0.20328\n",
      "[16:31:26] [106]\ttraining-aucpr:0.20316\n",
      "[16:31:26] [107]\ttraining-aucpr:0.20311\n",
      "[16:31:26] [108]\ttraining-aucpr:0.20341\n",
      "[16:31:26] [109]\ttraining-aucpr:0.20357\n",
      "[16:31:26] [110]\ttraining-aucpr:0.20408\n",
      "[16:31:26] [111]\ttraining-aucpr:0.20482\n",
      "[16:31:26] [112]\ttraining-aucpr:0.20535\n",
      "[16:31:26] [113]\ttraining-aucpr:0.20589\n",
      "[16:31:26] [114]\ttraining-aucpr:0.20593\n",
      "[16:31:26] [115]\ttraining-aucpr:0.20625\n",
      "[16:31:26] [116]\ttraining-aucpr:0.20662\n",
      "[16:31:26] [117]\ttraining-aucpr:0.20755\n",
      "[16:31:26] [118]\ttraining-aucpr:0.20769\n",
      "[16:31:26] [119]\ttraining-aucpr:0.20782\n",
      "[16:31:26] [120]\ttraining-aucpr:0.20827\n",
      "[16:31:26] [121]\ttraining-aucpr:0.20852\n",
      "[16:31:26] [122]\ttraining-aucpr:0.20900\n",
      "[16:31:26] [123]\ttraining-aucpr:0.20943\n",
      "[16:31:26] [124]\ttraining-aucpr:0.20974\n",
      "[16:31:27] [125]\ttraining-aucpr:0.20987\n",
      "[16:31:27] [126]\ttraining-aucpr:0.21020\n",
      "[16:31:27] [127]\ttraining-aucpr:0.21022\n",
      "[16:31:27] [128]\ttraining-aucpr:0.21048\n",
      "[16:31:27] [129]\ttraining-aucpr:0.21111\n",
      "[16:31:27] [130]\ttraining-aucpr:0.21186\n",
      "[16:31:27] [131]\ttraining-aucpr:0.21218\n",
      "[16:31:27] [132]\ttraining-aucpr:0.21255\n",
      "[16:31:27] [133]\ttraining-aucpr:0.21274\n",
      "[16:31:27] [134]\ttraining-aucpr:0.21319\n",
      "[16:31:27] [135]\ttraining-aucpr:0.21328\n",
      "[16:31:27] [136]\ttraining-aucpr:0.21338\n",
      "[16:31:27] [137]\ttraining-aucpr:0.21366\n",
      "2025-07-28 16:31:28,349 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:31:28,436 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:31:29,146] Trial 29 finished with value: 0.1580177932886629 and parameters: {'max_depth': 5, 'learning_rate': 0.27468827469748175, 'n_estimators': 138, 'min_child_weight': 6, 'gamma': 0.8929852013655546, 'subsample': 0.8206519441663025, 'colsample_bytree': 0.8502932763565828, 'reg_alpha': 0.5188184434248876, 'reg_lambda': 0.6002411647513859}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:31:29,268 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.8916132622922335, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.6997040734487521, 'learning_rate': 0.22063894014327562, 'max_depth': 3, 'min_child_weight': 5, 'reg_alpha': 0.24655117634611834, 'reg_lambda': 0.7140798053885995, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.8837591149849157, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 166}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:31:30,666 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:31:31] Task 1 got rank 1[16:31:31] Task 0 got rank 0[16:31:31] Task 3 got rank 3\n",
      "\n",
      "\n",
      "[16:31:31] Task 2 got rank 2\n",
      "[16:31:32] [0]\ttraining-aucpr:0.04710\n",
      "[16:31:32] [1]\ttraining-aucpr:0.04832\n",
      "[16:31:32] [2]\ttraining-aucpr:0.05546\n",
      "[16:31:32] [3]\ttraining-aucpr:0.06116\n",
      "[16:31:32] [4]\ttraining-aucpr:0.07020\n",
      "[16:31:32] [5]\ttraining-aucpr:0.07704\n",
      "[16:31:32] [6]\ttraining-aucpr:0.08829\n",
      "[16:31:32] [7]\ttraining-aucpr:0.09288\n",
      "[16:31:32] [8]\ttraining-aucpr:0.09474\n",
      "[16:31:32] [9]\ttraining-aucpr:0.10098\n",
      "[16:31:32] [10]\ttraining-aucpr:0.10937\n",
      "[16:31:32] [11]\ttraining-aucpr:0.11073\n",
      "[16:31:32] [12]\ttraining-aucpr:0.11674\n",
      "[16:31:32] [13]\ttraining-aucpr:0.11774\n",
      "[16:31:32] [14]\ttraining-aucpr:0.12100\n",
      "[16:31:32] [15]\ttraining-aucpr:0.12103\n",
      "[16:31:32] [16]\ttraining-aucpr:0.12238\n",
      "[16:31:32] [17]\ttraining-aucpr:0.12565\n",
      "[16:31:32] [18]\ttraining-aucpr:0.12743\n",
      "[16:31:32] [19]\ttraining-aucpr:0.12808\n",
      "[16:31:32] [20]\ttraining-aucpr:0.13112\n",
      "[16:31:32] [21]\ttraining-aucpr:0.13096\n",
      "[16:31:32] [22]\ttraining-aucpr:0.13227\n",
      "[16:31:32] [23]\ttraining-aucpr:0.13550\n",
      "[16:31:32] [24]\ttraining-aucpr:0.13707\n",
      "[16:31:32] [25]\ttraining-aucpr:0.13974\n",
      "[16:31:32] [26]\ttraining-aucpr:0.14023\n",
      "[16:31:32] [27]\ttraining-aucpr:0.14075\n",
      "[16:31:32] [28]\ttraining-aucpr:0.14272\n",
      "[16:31:32] [29]\ttraining-aucpr:0.14374\n",
      "[16:31:33] [30]\ttraining-aucpr:0.14527\n",
      "[16:31:33] [31]\ttraining-aucpr:0.14646\n",
      "[16:31:33] [32]\ttraining-aucpr:0.14753\n",
      "[16:31:33] [33]\ttraining-aucpr:0.14799\n",
      "[16:31:33] [34]\ttraining-aucpr:0.14905\n",
      "[16:31:33] [35]\ttraining-aucpr:0.14996\n",
      "[16:31:33] [36]\ttraining-aucpr:0.15091\n",
      "[16:31:33] [37]\ttraining-aucpr:0.15093\n",
      "[16:31:33] [38]\ttraining-aucpr:0.15176\n",
      "[16:31:33] [39]\ttraining-aucpr:0.15236\n",
      "[16:31:33] [40]\ttraining-aucpr:0.15317\n",
      "[16:31:33] [41]\ttraining-aucpr:0.15321\n",
      "[16:31:33] [42]\ttraining-aucpr:0.15341\n",
      "[16:31:33] [43]\ttraining-aucpr:0.15431\n",
      "[16:31:33] [44]\ttraining-aucpr:0.15463\n",
      "[16:31:33] [45]\ttraining-aucpr:0.15537\n",
      "[16:31:33] [46]\ttraining-aucpr:0.15641\n",
      "[16:31:33] [47]\ttraining-aucpr:0.15684\n",
      "[16:31:33] [48]\ttraining-aucpr:0.15737\n",
      "[16:31:33] [49]\ttraining-aucpr:0.15781\n",
      "[16:31:33] [50]\ttraining-aucpr:0.15819\n",
      "[16:31:33] [51]\ttraining-aucpr:0.15876\n",
      "[16:31:33] [52]\ttraining-aucpr:0.15932\n",
      "[16:31:33] [53]\ttraining-aucpr:0.15994\n",
      "[16:31:33] [54]\ttraining-aucpr:0.16072\n",
      "[16:31:33] [55]\ttraining-aucpr:0.16128\n",
      "[16:31:33] [56]\ttraining-aucpr:0.16139\n",
      "[16:31:33] [57]\ttraining-aucpr:0.16175\n",
      "[16:31:33] [58]\ttraining-aucpr:0.16197\n",
      "[16:31:33] [59]\ttraining-aucpr:0.16235\n",
      "[16:31:33] [60]\ttraining-aucpr:0.16265\n",
      "[16:31:33] [61]\ttraining-aucpr:0.16286\n",
      "[16:31:33] [62]\ttraining-aucpr:0.16351\n",
      "[16:31:33] [63]\ttraining-aucpr:0.16371\n",
      "[16:31:33] [64]\ttraining-aucpr:0.16408\n",
      "[16:31:33] [65]\ttraining-aucpr:0.16426\n",
      "[16:31:33] [66]\ttraining-aucpr:0.16456\n",
      "[16:31:33] [67]\ttraining-aucpr:0.16470\n",
      "[16:31:34] [68]\ttraining-aucpr:0.16489\n",
      "[16:31:34] [69]\ttraining-aucpr:0.16512\n",
      "[16:31:34] [70]\ttraining-aucpr:0.16506\n",
      "[16:31:34] [71]\ttraining-aucpr:0.16522\n",
      "[16:31:34] [72]\ttraining-aucpr:0.16556\n",
      "[16:31:34] [73]\ttraining-aucpr:0.16585\n",
      "[16:31:34] [74]\ttraining-aucpr:0.16583\n",
      "[16:31:34] [75]\ttraining-aucpr:0.16611\n",
      "[16:31:34] [76]\ttraining-aucpr:0.16651\n",
      "[16:31:34] [77]\ttraining-aucpr:0.16678\n",
      "[16:31:34] [78]\ttraining-aucpr:0.16691\n",
      "[16:31:34] [79]\ttraining-aucpr:0.16701\n",
      "[16:31:34] [80]\ttraining-aucpr:0.16718\n",
      "[16:31:34] [81]\ttraining-aucpr:0.16730\n",
      "[16:31:34] [82]\ttraining-aucpr:0.16742\n",
      "[16:31:34] [83]\ttraining-aucpr:0.16779\n",
      "[16:31:34] [84]\ttraining-aucpr:0.16790\n",
      "[16:31:34] [85]\ttraining-aucpr:0.16793\n",
      "[16:31:34] [86]\ttraining-aucpr:0.16819\n",
      "[16:31:34] [87]\ttraining-aucpr:0.16836\n",
      "[16:31:34] [88]\ttraining-aucpr:0.16878\n",
      "[16:31:34] [89]\ttraining-aucpr:0.16916\n",
      "[16:31:34] [90]\ttraining-aucpr:0.16935\n",
      "[16:31:34] [91]\ttraining-aucpr:0.16949\n",
      "[16:31:34] [92]\ttraining-aucpr:0.16967\n",
      "[16:31:34] [93]\ttraining-aucpr:0.16983\n",
      "[16:31:34] [94]\ttraining-aucpr:0.16997\n",
      "[16:31:34] [95]\ttraining-aucpr:0.17008\n",
      "[16:31:34] [96]\ttraining-aucpr:0.17017\n",
      "[16:31:34] [97]\ttraining-aucpr:0.17021\n",
      "[16:31:34] [98]\ttraining-aucpr:0.17025\n",
      "[16:31:34] [99]\ttraining-aucpr:0.17029\n",
      "[16:31:34] [100]\ttraining-aucpr:0.17030\n",
      "[16:31:34] [101]\ttraining-aucpr:0.17055\n",
      "[16:31:34] [102]\ttraining-aucpr:0.17055\n",
      "[16:31:34] [103]\ttraining-aucpr:0.17064\n",
      "[16:31:34] [104]\ttraining-aucpr:0.17074\n",
      "[16:31:34] [105]\ttraining-aucpr:0.17100\n",
      "[16:31:34] [106]\ttraining-aucpr:0.17125\n",
      "[16:31:34] [107]\ttraining-aucpr:0.17134\n",
      "[16:31:34] [108]\ttraining-aucpr:0.17147\n",
      "[16:31:34] [109]\ttraining-aucpr:0.17156\n",
      "[16:31:35] [110]\ttraining-aucpr:0.17172\n",
      "[16:31:35] [111]\ttraining-aucpr:0.17189\n",
      "[16:31:35] [112]\ttraining-aucpr:0.17212\n",
      "[16:31:35] [113]\ttraining-aucpr:0.17211\n",
      "[16:31:35] [114]\ttraining-aucpr:0.17213\n",
      "[16:31:35] [115]\ttraining-aucpr:0.17206\n",
      "[16:31:35] [116]\ttraining-aucpr:0.17213\n",
      "[16:31:35] [117]\ttraining-aucpr:0.17221\n",
      "[16:31:35] [118]\ttraining-aucpr:0.17223\n",
      "[16:31:35] [119]\ttraining-aucpr:0.17241\n",
      "[16:31:35] [120]\ttraining-aucpr:0.17237\n",
      "[16:31:35] [121]\ttraining-aucpr:0.17245\n",
      "[16:31:35] [122]\ttraining-aucpr:0.17263\n",
      "[16:31:35] [123]\ttraining-aucpr:0.17272\n",
      "[16:31:35] [124]\ttraining-aucpr:0.17275\n",
      "[16:31:35] [125]\ttraining-aucpr:0.17320\n",
      "[16:31:35] [126]\ttraining-aucpr:0.17337\n",
      "[16:31:35] [127]\ttraining-aucpr:0.17344\n",
      "[16:31:35] [128]\ttraining-aucpr:0.17346\n",
      "[16:31:35] [129]\ttraining-aucpr:0.17363\n",
      "[16:31:35] [130]\ttraining-aucpr:0.17376\n",
      "[16:31:35] [131]\ttraining-aucpr:0.17383\n",
      "[16:31:35] [132]\ttraining-aucpr:0.17386\n",
      "[16:31:35] [133]\ttraining-aucpr:0.17396\n",
      "[16:31:35] [134]\ttraining-aucpr:0.17400\n",
      "[16:31:35] [135]\ttraining-aucpr:0.17409\n",
      "[16:31:35] [136]\ttraining-aucpr:0.17407\n",
      "[16:31:35] [137]\ttraining-aucpr:0.17413\n",
      "[16:31:35] [138]\ttraining-aucpr:0.17413\n",
      "[16:31:35] [139]\ttraining-aucpr:0.17404\n",
      "[16:31:35] [140]\ttraining-aucpr:0.17415\n",
      "[16:31:35] [141]\ttraining-aucpr:0.17422\n",
      "[16:31:35] [142]\ttraining-aucpr:0.17439\n",
      "[16:31:35] [143]\ttraining-aucpr:0.17441\n",
      "[16:31:35] [144]\ttraining-aucpr:0.17448\n",
      "[16:31:35] [145]\ttraining-aucpr:0.17452\n",
      "[16:31:35] [146]\ttraining-aucpr:0.17476\n",
      "[16:31:36] [147]\ttraining-aucpr:0.17479\n",
      "[16:31:36] [148]\ttraining-aucpr:0.17498\n",
      "[16:31:36] [149]\ttraining-aucpr:0.17492\n",
      "[16:31:36] [150]\ttraining-aucpr:0.17489\n",
      "[16:31:36] [151]\ttraining-aucpr:0.17487\n",
      "[16:31:36] [152]\ttraining-aucpr:0.17498\n",
      "[16:31:36] [153]\ttraining-aucpr:0.17503\n",
      "[16:31:36] [154]\ttraining-aucpr:0.17500\n",
      "[16:31:36] [155]\ttraining-aucpr:0.17512\n",
      "[16:31:36] [156]\ttraining-aucpr:0.17518\n",
      "[16:31:36] [157]\ttraining-aucpr:0.17529\n",
      "[16:31:36] [158]\ttraining-aucpr:0.17527\n",
      "[16:31:36] [159]\ttraining-aucpr:0.17564\n",
      "[16:31:36] [160]\ttraining-aucpr:0.17582\n",
      "[16:31:36] [161]\ttraining-aucpr:0.17583\n",
      "[16:31:36] [162]\ttraining-aucpr:0.17602\n",
      "[16:31:36] [163]\ttraining-aucpr:0.17605\n",
      "[16:31:36] [164]\ttraining-aucpr:0.17614\n",
      "[16:31:36] [165]\ttraining-aucpr:0.17619\n",
      "2025-07-28 16:31:37,470 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:31:37,554 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:31:38,288] Trial 30 finished with value: 0.19192550443184236 and parameters: {'max_depth': 3, 'learning_rate': 0.22063894014327562, 'n_estimators': 166, 'min_child_weight': 5, 'gamma': 0.6997040734487521, 'subsample': 0.8837591149849157, 'colsample_bytree': 0.8916132622922335, 'reg_alpha': 0.24655117634611834, 'reg_lambda': 0.7140798053885995}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:31:38,401 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.7836643116562136, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.5544781653390556, 'learning_rate': 0.16773306851420086, 'max_depth': 4, 'min_child_weight': 8, 'reg_alpha': 0.4768716160587091, 'reg_lambda': 0.04666953660918871, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9991417930830016, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 99}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:31:39,788 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:31:40] Task 0 got rank 0\n",
      "[16:31:40] Task 1 got rank 1[16:31:40] Task 2 got rank 2\n",
      "\n",
      "[16:31:40] Task 3 got rank 3\n",
      "[16:31:41] [0]\ttraining-aucpr:0.05563\n",
      "[16:31:41] [1]\ttraining-aucpr:0.06056\n",
      "[16:31:41] [2]\ttraining-aucpr:0.06854\n",
      "[16:31:41] [3]\ttraining-aucpr:0.07616\n",
      "[16:31:41] [4]\ttraining-aucpr:0.07767\n",
      "[16:31:41] [5]\ttraining-aucpr:0.08508\n",
      "[16:31:41] [6]\ttraining-aucpr:0.08619\n",
      "[16:31:41] [7]\ttraining-aucpr:0.09491\n",
      "[16:31:41] [8]\ttraining-aucpr:0.09955\n",
      "[16:31:41] [9]\ttraining-aucpr:0.10365\n",
      "[16:31:41] [10]\ttraining-aucpr:0.10818\n",
      "[16:31:41] [11]\ttraining-aucpr:0.11191\n",
      "[16:31:41] [12]\ttraining-aucpr:0.11613\n",
      "[16:31:41] [13]\ttraining-aucpr:0.12085\n",
      "[16:31:41] [14]\ttraining-aucpr:0.12267\n",
      "[16:31:41] [15]\ttraining-aucpr:0.12365\n",
      "[16:31:41] [16]\ttraining-aucpr:0.12726\n",
      "[16:31:41] [17]\ttraining-aucpr:0.13126\n",
      "[16:31:41] [18]\ttraining-aucpr:0.13217\n",
      "[16:31:41] [19]\ttraining-aucpr:0.13336\n",
      "[16:31:41] [20]\ttraining-aucpr:0.13616\n",
      "[16:31:41] [21]\ttraining-aucpr:0.13810\n",
      "[16:31:41] [22]\ttraining-aucpr:0.13899\n",
      "[16:31:42] [23]\ttraining-aucpr:0.13953\n",
      "[16:31:42] [24]\ttraining-aucpr:0.14143\n",
      "[16:31:42] [25]\ttraining-aucpr:0.14224\n",
      "[16:31:42] [26]\ttraining-aucpr:0.14466\n",
      "[16:31:42] [27]\ttraining-aucpr:0.14672\n",
      "[16:31:42] [28]\ttraining-aucpr:0.14721\n",
      "[16:31:42] [29]\ttraining-aucpr:0.14830\n",
      "[16:31:42] [30]\ttraining-aucpr:0.14979\n",
      "[16:31:42] [31]\ttraining-aucpr:0.15202\n",
      "[16:31:42] [32]\ttraining-aucpr:0.15296\n",
      "[16:31:42] [33]\ttraining-aucpr:0.15302\n",
      "[16:31:42] [34]\ttraining-aucpr:0.15426\n",
      "[16:31:42] [35]\ttraining-aucpr:0.15493\n",
      "[16:31:42] [36]\ttraining-aucpr:0.15606\n",
      "[16:31:42] [37]\ttraining-aucpr:0.15725\n",
      "[16:31:42] [38]\ttraining-aucpr:0.15808\n",
      "[16:31:42] [39]\ttraining-aucpr:0.15948\n",
      "[16:31:42] [40]\ttraining-aucpr:0.16027\n",
      "[16:31:42] [41]\ttraining-aucpr:0.16136\n",
      "[16:31:42] [42]\ttraining-aucpr:0.16188\n",
      "[16:31:42] [43]\ttraining-aucpr:0.16246\n",
      "[16:31:42] [44]\ttraining-aucpr:0.16320\n",
      "[16:31:42] [45]\ttraining-aucpr:0.16361\n",
      "[16:31:42] [46]\ttraining-aucpr:0.16444\n",
      "[16:31:42] [47]\ttraining-aucpr:0.16455\n",
      "[16:31:42] [48]\ttraining-aucpr:0.16508\n",
      "[16:31:42] [49]\ttraining-aucpr:0.16593\n",
      "[16:31:42] [50]\ttraining-aucpr:0.16676\n",
      "[16:31:42] [51]\ttraining-aucpr:0.16739\n",
      "[16:31:42] [52]\ttraining-aucpr:0.16763\n",
      "[16:31:42] [53]\ttraining-aucpr:0.16829\n",
      "[16:31:42] [54]\ttraining-aucpr:0.16957\n",
      "[16:31:42] [55]\ttraining-aucpr:0.17007\n",
      "[16:31:42] [56]\ttraining-aucpr:0.17020\n",
      "[16:31:42] [57]\ttraining-aucpr:0.17052\n",
      "[16:31:42] [58]\ttraining-aucpr:0.17089\n",
      "[16:31:42] [59]\ttraining-aucpr:0.17096\n",
      "[16:31:42] [60]\ttraining-aucpr:0.17170\n",
      "[16:31:43] [61]\ttraining-aucpr:0.17215\n",
      "[16:31:43] [62]\ttraining-aucpr:0.17246\n",
      "[16:31:43] [63]\ttraining-aucpr:0.17305\n",
      "[16:31:43] [64]\ttraining-aucpr:0.17312\n",
      "[16:31:43] [65]\ttraining-aucpr:0.17364\n",
      "[16:31:43] [66]\ttraining-aucpr:0.17400\n",
      "[16:31:43] [67]\ttraining-aucpr:0.17432\n",
      "[16:31:43] [68]\ttraining-aucpr:0.17457\n",
      "[16:31:43] [69]\ttraining-aucpr:0.17486\n",
      "[16:31:43] [70]\ttraining-aucpr:0.17539\n",
      "[16:31:43] [71]\ttraining-aucpr:0.17576\n",
      "[16:31:43] [72]\ttraining-aucpr:0.17576\n",
      "[16:31:43] [73]\ttraining-aucpr:0.17606\n",
      "[16:31:43] [74]\ttraining-aucpr:0.17617\n",
      "[16:31:43] [75]\ttraining-aucpr:0.17665\n",
      "[16:31:43] [76]\ttraining-aucpr:0.17689\n",
      "[16:31:43] [77]\ttraining-aucpr:0.17707\n",
      "[16:31:43] [78]\ttraining-aucpr:0.17713\n",
      "[16:31:43] [79]\ttraining-aucpr:0.17731\n",
      "[16:31:43] [80]\ttraining-aucpr:0.17753\n",
      "[16:31:43] [81]\ttraining-aucpr:0.17758\n",
      "[16:31:43] [82]\ttraining-aucpr:0.17780\n",
      "[16:31:43] [83]\ttraining-aucpr:0.17798\n",
      "[16:31:43] [84]\ttraining-aucpr:0.17827\n",
      "[16:31:43] [85]\ttraining-aucpr:0.17876\n",
      "[16:31:43] [86]\ttraining-aucpr:0.17902\n",
      "[16:31:43] [87]\ttraining-aucpr:0.17919\n",
      "[16:31:43] [88]\ttraining-aucpr:0.17925\n",
      "[16:31:43] [89]\ttraining-aucpr:0.17940\n",
      "[16:31:43] [90]\ttraining-aucpr:0.17957\n",
      "[16:31:43] [91]\ttraining-aucpr:0.17974\n",
      "[16:31:43] [92]\ttraining-aucpr:0.17999\n",
      "[16:31:43] [93]\ttraining-aucpr:0.18020\n",
      "[16:31:43] [94]\ttraining-aucpr:0.18042\n",
      "[16:31:43] [95]\ttraining-aucpr:0.18075\n",
      "[16:31:43] [96]\ttraining-aucpr:0.18084\n",
      "[16:31:43] [97]\ttraining-aucpr:0.18077\n",
      "[16:31:43] [98]\ttraining-aucpr:0.18093\n",
      "2025-07-28 16:31:44,951 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:31:45,028 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:31:45,742] Trial 31 finished with value: 0.1926092876155718 and parameters: {'max_depth': 4, 'learning_rate': 0.16773306851420086, 'n_estimators': 99, 'min_child_weight': 8, 'gamma': 0.5544781653390556, 'subsample': 0.9991417930830016, 'colsample_bytree': 0.7836643116562136, 'reg_alpha': 0.4768716160587091, 'reg_lambda': 0.04666953660918871}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:31:45,848 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.8156367309350179, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.5973363355441346, 'learning_rate': 0.13220606283176803, 'max_depth': 4, 'min_child_weight': 8, 'reg_alpha': 0.4444702632044215, 'reg_lambda': 0.14804674368144616, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9104356544171793, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 84}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:31:47,232 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:31:48] Task 0 got rank 0\n",
      "[16:31:48] Task 1 got rank 1\n",
      "[16:31:48] Task 2 got rank 2\n",
      "[16:31:48] Task 3 got rank 3\n",
      "[16:31:48] [0]\ttraining-aucpr:0.05563\n",
      "[16:31:48] [1]\ttraining-aucpr:0.06026\n",
      "[16:31:48] [2]\ttraining-aucpr:0.06741\n",
      "[16:31:48] [3]\ttraining-aucpr:0.06841\n",
      "[16:31:48] [4]\ttraining-aucpr:0.07629\n",
      "[16:31:48] [5]\ttraining-aucpr:0.07775\n",
      "[16:31:48] [6]\ttraining-aucpr:0.08232\n",
      "[16:31:48] [7]\ttraining-aucpr:0.08255\n",
      "[16:31:48] [8]\ttraining-aucpr:0.08440\n",
      "[16:31:48] [9]\ttraining-aucpr:0.09762\n",
      "[16:31:49] [10]\ttraining-aucpr:0.10505\n",
      "[16:31:49] [11]\ttraining-aucpr:0.10743\n",
      "[16:31:49] [12]\ttraining-aucpr:0.11308\n",
      "[16:31:49] [13]\ttraining-aucpr:0.11642\n",
      "[16:31:49] [14]\ttraining-aucpr:0.11908\n",
      "[16:31:49] [15]\ttraining-aucpr:0.12013\n",
      "[16:31:49] [16]\ttraining-aucpr:0.12131\n",
      "[16:31:49] [17]\ttraining-aucpr:0.12410\n",
      "[16:31:49] [18]\ttraining-aucpr:0.12656\n",
      "[16:31:49] [19]\ttraining-aucpr:0.12864\n",
      "[16:31:49] [20]\ttraining-aucpr:0.12795\n",
      "[16:31:49] [21]\ttraining-aucpr:0.12808\n",
      "[16:31:49] [22]\ttraining-aucpr:0.13157\n",
      "[16:31:49] [23]\ttraining-aucpr:0.13309\n",
      "[16:31:49] [24]\ttraining-aucpr:0.13485\n",
      "[16:31:49] [25]\ttraining-aucpr:0.13577\n",
      "[16:31:49] [26]\ttraining-aucpr:0.13714\n",
      "[16:31:49] [27]\ttraining-aucpr:0.13868\n",
      "[16:31:49] [28]\ttraining-aucpr:0.13935\n",
      "[16:31:49] [29]\ttraining-aucpr:0.13992\n",
      "[16:31:49] [30]\ttraining-aucpr:0.14152\n",
      "[16:31:49] [31]\ttraining-aucpr:0.14289\n",
      "[16:31:49] [32]\ttraining-aucpr:0.14335\n",
      "[16:31:49] [33]\ttraining-aucpr:0.14547\n",
      "[16:31:49] [34]\ttraining-aucpr:0.14720\n",
      "[16:31:49] [35]\ttraining-aucpr:0.14727\n",
      "[16:31:49] [36]\ttraining-aucpr:0.14925\n",
      "[16:31:49] [37]\ttraining-aucpr:0.15006\n",
      "[16:31:49] [38]\ttraining-aucpr:0.15074\n",
      "[16:31:49] [39]\ttraining-aucpr:0.15130\n",
      "[16:31:49] [40]\ttraining-aucpr:0.15198\n",
      "[16:31:49] [41]\ttraining-aucpr:0.15316\n",
      "[16:31:49] [42]\ttraining-aucpr:0.15371\n",
      "[16:31:49] [43]\ttraining-aucpr:0.15429\n",
      "[16:31:49] [44]\ttraining-aucpr:0.15474\n",
      "[16:31:49] [45]\ttraining-aucpr:0.15549\n",
      "[16:31:49] [46]\ttraining-aucpr:0.15638\n",
      "[16:31:49] [47]\ttraining-aucpr:0.15738\n",
      "[16:31:50] [48]\ttraining-aucpr:0.15814\n",
      "[16:31:50] [49]\ttraining-aucpr:0.15895\n",
      "[16:31:50] [50]\ttraining-aucpr:0.15931\n",
      "[16:31:50] [51]\ttraining-aucpr:0.16002\n",
      "[16:31:50] [52]\ttraining-aucpr:0.16074\n",
      "[16:31:50] [53]\ttraining-aucpr:0.16125\n",
      "[16:31:50] [54]\ttraining-aucpr:0.16244\n",
      "[16:31:50] [55]\ttraining-aucpr:0.16321\n",
      "[16:31:50] [56]\ttraining-aucpr:0.16364\n",
      "[16:31:50] [57]\ttraining-aucpr:0.16398\n",
      "[16:31:50] [58]\ttraining-aucpr:0.16456\n",
      "[16:31:50] [59]\ttraining-aucpr:0.16498\n",
      "[16:31:50] [60]\ttraining-aucpr:0.16559\n",
      "[16:31:50] [61]\ttraining-aucpr:0.16595\n",
      "[16:31:50] [62]\ttraining-aucpr:0.16639\n",
      "[16:31:50] [63]\ttraining-aucpr:0.16632\n",
      "[16:31:50] [64]\ttraining-aucpr:0.16669\n",
      "[16:31:50] [65]\ttraining-aucpr:0.16703\n",
      "[16:31:50] [66]\ttraining-aucpr:0.16722\n",
      "[16:31:50] [67]\ttraining-aucpr:0.16731\n",
      "[16:31:50] [68]\ttraining-aucpr:0.16743\n",
      "[16:31:50] [69]\ttraining-aucpr:0.16797\n",
      "[16:31:50] [70]\ttraining-aucpr:0.16872\n",
      "[16:31:50] [71]\ttraining-aucpr:0.16902\n",
      "[16:31:50] [72]\ttraining-aucpr:0.16962\n",
      "[16:31:50] [73]\ttraining-aucpr:0.16978\n",
      "[16:31:50] [74]\ttraining-aucpr:0.17009\n",
      "[16:31:50] [75]\ttraining-aucpr:0.17044\n",
      "[16:31:50] [76]\ttraining-aucpr:0.17106\n",
      "[16:31:50] [77]\ttraining-aucpr:0.17113\n",
      "[16:31:50] [78]\ttraining-aucpr:0.17137\n",
      "[16:31:50] [79]\ttraining-aucpr:0.17155\n",
      "[16:31:50] [80]\ttraining-aucpr:0.17164\n",
      "[16:31:50] [81]\ttraining-aucpr:0.17190\n",
      "[16:31:50] [82]\ttraining-aucpr:0.17262\n",
      "[16:31:50] [83]\ttraining-aucpr:0.17283\n",
      "2025-07-28 16:31:51,972 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:31:52,056 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:31:52,790] Trial 32 finished with value: 0.19081496718347649 and parameters: {'max_depth': 4, 'learning_rate': 0.13220606283176803, 'n_estimators': 84, 'min_child_weight': 8, 'gamma': 0.5973363355441346, 'subsample': 0.9104356544171793, 'colsample_bytree': 0.8156367309350179, 'reg_alpha': 0.4444702632044215, 'reg_lambda': 0.14804674368144616}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:31:52,895 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.7266367682142773, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.49311859147246007, 'learning_rate': 0.17951986409453063, 'max_depth': 3, 'min_child_weight': 7, 'reg_alpha': 0.557344327616757, 'reg_lambda': 0.2672359866726552, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9457028130560903, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 86}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:31:54,268 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:31:55] Task 0 got rank 0\n",
      "[16:31:55] Task 1 got rank 1[16:31:55] Task 2 got rank 2\n",
      "\n",
      "[16:31:55] Task 3 got rank 3\n",
      "[16:31:55] [0]\ttraining-aucpr:0.04728\n",
      "[16:31:55] [1]\ttraining-aucpr:0.04839\n",
      "[16:31:55] [2]\ttraining-aucpr:0.05799\n",
      "[16:31:55] [3]\ttraining-aucpr:0.06351\n",
      "[16:31:55] [4]\ttraining-aucpr:0.06633\n",
      "[16:31:55] [5]\ttraining-aucpr:0.07197\n",
      "[16:31:55] [6]\ttraining-aucpr:0.08298\n",
      "[16:31:55] [7]\ttraining-aucpr:0.08719\n",
      "[16:31:56] [8]\ttraining-aucpr:0.08837\n",
      "[16:31:56] [9]\ttraining-aucpr:0.08899\n",
      "[16:31:56] [10]\ttraining-aucpr:0.09905\n",
      "[16:31:56] [11]\ttraining-aucpr:0.10351\n",
      "[16:31:56] [12]\ttraining-aucpr:0.10551\n",
      "[16:31:56] [13]\ttraining-aucpr:0.10872\n",
      "[16:31:56] [14]\ttraining-aucpr:0.10955\n",
      "[16:31:56] [15]\ttraining-aucpr:0.11122\n",
      "[16:31:56] [16]\ttraining-aucpr:0.11480\n",
      "[16:31:56] [17]\ttraining-aucpr:0.11869\n",
      "[16:31:56] [18]\ttraining-aucpr:0.12119\n",
      "[16:31:56] [19]\ttraining-aucpr:0.12433\n",
      "[16:31:56] [20]\ttraining-aucpr:0.12507\n",
      "[16:31:56] [21]\ttraining-aucpr:0.12583\n",
      "[16:31:56] [22]\ttraining-aucpr:0.12800\n",
      "[16:31:56] [23]\ttraining-aucpr:0.12804\n",
      "[16:31:56] [24]\ttraining-aucpr:0.13013\n",
      "[16:31:56] [25]\ttraining-aucpr:0.13169\n",
      "[16:31:56] [26]\ttraining-aucpr:0.13284\n",
      "[16:31:56] [27]\ttraining-aucpr:0.13547\n",
      "[16:31:56] [28]\ttraining-aucpr:0.13599\n",
      "[16:31:56] [29]\ttraining-aucpr:0.13752\n",
      "[16:31:56] [30]\ttraining-aucpr:0.13884\n",
      "[16:31:56] [31]\ttraining-aucpr:0.14016\n",
      "[16:31:56] [32]\ttraining-aucpr:0.14182\n",
      "[16:31:56] [33]\ttraining-aucpr:0.14307\n",
      "[16:31:56] [34]\ttraining-aucpr:0.14369\n",
      "[16:31:56] [35]\ttraining-aucpr:0.14419\n",
      "[16:31:56] [36]\ttraining-aucpr:0.14526\n",
      "[16:31:56] [37]\ttraining-aucpr:0.14587\n",
      "[16:31:56] [38]\ttraining-aucpr:0.14681\n",
      "[16:31:56] [39]\ttraining-aucpr:0.14707\n",
      "[16:31:56] [40]\ttraining-aucpr:0.14770\n",
      "[16:31:56] [41]\ttraining-aucpr:0.14770\n",
      "[16:31:56] [42]\ttraining-aucpr:0.14951\n",
      "[16:31:56] [43]\ttraining-aucpr:0.15005\n",
      "[16:31:56] [44]\ttraining-aucpr:0.15071\n",
      "[16:31:56] [45]\ttraining-aucpr:0.15165\n",
      "[16:31:56] [46]\ttraining-aucpr:0.15294\n",
      "[16:31:56] [47]\ttraining-aucpr:0.15390\n",
      "[16:31:56] [48]\ttraining-aucpr:0.15468\n",
      "[16:31:57] [49]\ttraining-aucpr:0.15516\n",
      "[16:31:57] [50]\ttraining-aucpr:0.15588\n",
      "[16:31:57] [51]\ttraining-aucpr:0.15675\n",
      "[16:31:57] [52]\ttraining-aucpr:0.15678\n",
      "[16:31:57] [53]\ttraining-aucpr:0.15723\n",
      "[16:31:57] [54]\ttraining-aucpr:0.15762\n",
      "[16:31:57] [55]\ttraining-aucpr:0.15784\n",
      "[16:31:57] [56]\ttraining-aucpr:0.15848\n",
      "[16:31:57] [57]\ttraining-aucpr:0.15867\n",
      "[16:31:57] [58]\ttraining-aucpr:0.15898\n",
      "[16:31:57] [59]\ttraining-aucpr:0.15942\n",
      "[16:31:57] [60]\ttraining-aucpr:0.15992\n",
      "[16:31:57] [61]\ttraining-aucpr:0.16049\n",
      "[16:31:57] [62]\ttraining-aucpr:0.16107\n",
      "[16:31:57] [63]\ttraining-aucpr:0.16150\n",
      "[16:31:57] [64]\ttraining-aucpr:0.16155\n",
      "[16:31:57] [65]\ttraining-aucpr:0.16220\n",
      "[16:31:57] [66]\ttraining-aucpr:0.16224\n",
      "[16:31:57] [67]\ttraining-aucpr:0.16234\n",
      "[16:31:57] [68]\ttraining-aucpr:0.16250\n",
      "[16:31:57] [69]\ttraining-aucpr:0.16274\n",
      "[16:31:57] [70]\ttraining-aucpr:0.16306\n",
      "[16:31:57] [71]\ttraining-aucpr:0.16337\n",
      "[16:31:57] [72]\ttraining-aucpr:0.16366\n",
      "[16:31:57] [73]\ttraining-aucpr:0.16391\n",
      "[16:31:57] [74]\ttraining-aucpr:0.16426\n",
      "[16:31:57] [75]\ttraining-aucpr:0.16467\n",
      "[16:31:57] [76]\ttraining-aucpr:0.16506\n",
      "[16:31:57] [77]\ttraining-aucpr:0.16537\n",
      "[16:31:57] [78]\ttraining-aucpr:0.16586\n",
      "[16:31:57] [79]\ttraining-aucpr:0.16612\n",
      "[16:31:57] [80]\ttraining-aucpr:0.16615\n",
      "[16:31:57] [81]\ttraining-aucpr:0.16623\n",
      "[16:31:57] [82]\ttraining-aucpr:0.16668\n",
      "[16:31:57] [83]\ttraining-aucpr:0.16694\n",
      "[16:31:57] [84]\ttraining-aucpr:0.16727\n",
      "[16:31:57] [85]\ttraining-aucpr:0.16734\n",
      "2025-07-28 16:31:58,918 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:31:59,009 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:31:59,702] Trial 33 finished with value: 0.19168955815366906 and parameters: {'max_depth': 3, 'learning_rate': 0.17951986409453063, 'n_estimators': 86, 'min_child_weight': 7, 'gamma': 0.49311859147246007, 'subsample': 0.9457028130560903, 'colsample_bytree': 0.7266367682142773, 'reg_alpha': 0.557344327616757, 'reg_lambda': 0.2672359866726552}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:31:59,823 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.794961108226485, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.3936610271700302, 'learning_rate': 0.12596446879291956, 'max_depth': 6, 'min_child_weight': 9, 'reg_alpha': 0.76824708024221, 'reg_lambda': 0.05057047528611243, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.6866295836032222, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 51}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:32:01,221 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:32:02] Task 1 got rank 1\n",
      "[16:32:02] Task 0 got rank 0\n",
      "[16:32:02] Task 3 got rank 3\n",
      "[16:32:02] Task 2 got rank 2\n",
      "[16:32:02] [0]\ttraining-aucpr:0.07174\n",
      "[16:32:02] [1]\ttraining-aucpr:0.09045\n",
      "[16:32:02] [2]\ttraining-aucpr:0.10327\n",
      "[16:32:02] [3]\ttraining-aucpr:0.10707\n",
      "[16:32:02] [4]\ttraining-aucpr:0.10815\n",
      "[16:32:02] [5]\ttraining-aucpr:0.11272\n",
      "[16:32:02] [6]\ttraining-aucpr:0.11479\n",
      "[16:32:03] [7]\ttraining-aucpr:0.11672\n",
      "[16:32:03] [8]\ttraining-aucpr:0.11830\n",
      "[16:32:03] [9]\ttraining-aucpr:0.12255\n",
      "[16:32:03] [10]\ttraining-aucpr:0.12403\n",
      "[16:32:03] [11]\ttraining-aucpr:0.12568\n",
      "[16:32:03] [12]\ttraining-aucpr:0.12768\n",
      "[16:32:03] [13]\ttraining-aucpr:0.13065\n",
      "[16:32:03] [14]\ttraining-aucpr:0.13272\n",
      "[16:32:03] [15]\ttraining-aucpr:0.13409\n",
      "[16:32:03] [16]\ttraining-aucpr:0.13746\n",
      "[16:32:03] [17]\ttraining-aucpr:0.14119\n",
      "[16:32:03] [18]\ttraining-aucpr:0.14350\n",
      "[16:32:03] [19]\ttraining-aucpr:0.14506\n",
      "[16:32:03] [20]\ttraining-aucpr:0.14758\n",
      "[16:32:03] [21]\ttraining-aucpr:0.14904\n",
      "[16:32:03] [22]\ttraining-aucpr:0.15167\n",
      "[16:32:03] [23]\ttraining-aucpr:0.15274\n",
      "[16:32:03] [24]\ttraining-aucpr:0.15524\n",
      "[16:32:03] [25]\ttraining-aucpr:0.15645\n",
      "[16:32:03] [26]\ttraining-aucpr:0.15878\n",
      "[16:32:03] [27]\ttraining-aucpr:0.16079\n",
      "[16:32:03] [28]\ttraining-aucpr:0.16177\n",
      "[16:32:03] [29]\ttraining-aucpr:0.16321\n",
      "[16:32:03] [30]\ttraining-aucpr:0.16502\n",
      "[16:32:03] [31]\ttraining-aucpr:0.16631\n",
      "[16:32:03] [32]\ttraining-aucpr:0.16757\n",
      "[16:32:03] [33]\ttraining-aucpr:0.16977\n",
      "[16:32:04] [34]\ttraining-aucpr:0.17097\n",
      "[16:32:04] [35]\ttraining-aucpr:0.17175\n",
      "[16:32:04] [36]\ttraining-aucpr:0.17325\n",
      "[16:32:04] [37]\ttraining-aucpr:0.17481\n",
      "[16:32:04] [38]\ttraining-aucpr:0.17556\n",
      "[16:32:04] [39]\ttraining-aucpr:0.17610\n",
      "[16:32:04] [40]\ttraining-aucpr:0.17685\n",
      "[16:32:04] [41]\ttraining-aucpr:0.17766\n",
      "[16:32:04] [42]\ttraining-aucpr:0.17870\n",
      "[16:32:04] [43]\ttraining-aucpr:0.17948\n",
      "[16:32:04] [44]\ttraining-aucpr:0.17979\n",
      "[16:32:04] [45]\ttraining-aucpr:0.18102\n",
      "[16:32:04] [46]\ttraining-aucpr:0.18208\n",
      "[16:32:04] [47]\ttraining-aucpr:0.18297\n",
      "[16:32:04] [48]\ttraining-aucpr:0.18384\n",
      "[16:32:04] [49]\ttraining-aucpr:0.18494\n",
      "[16:32:04] [50]\ttraining-aucpr:0.18577\n",
      "2025-07-28 16:32:05,559 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:32:05,640 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:32:06,383] Trial 34 finished with value: 0.17121666199919167 and parameters: {'max_depth': 6, 'learning_rate': 0.12596446879291956, 'n_estimators': 51, 'min_child_weight': 9, 'gamma': 0.3936610271700302, 'subsample': 0.6866295836032222, 'colsample_bytree': 0.794961108226485, 'reg_alpha': 0.76824708024221, 'reg_lambda': 0.05057047528611243}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:32:06,504 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.8485612891342659, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.5222574247171057, 'learning_rate': 0.08038734016328382, 'max_depth': 9, 'min_child_weight': 9, 'reg_alpha': 0.33098555707788685, 'reg_lambda': 0.16566785053352698, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.8909969623905027, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 112}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:32:07,899 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:32:08] Task 1 got rank 1\n",
      "[16:32:08] Task 2 got rank 2\n",
      "[16:32:08] Task 3 got rank 3[16:32:08] Task 0 got rank 0\n",
      "\n",
      "[16:32:09] [0]\ttraining-aucpr:0.10398\n",
      "[16:32:09] [1]\ttraining-aucpr:0.12766\n",
      "[16:32:09] [2]\ttraining-aucpr:0.13588\n",
      "[16:32:09] [3]\ttraining-aucpr:0.14119\n",
      "[16:32:09] [4]\ttraining-aucpr:0.14611\n",
      "[16:32:09] [5]\ttraining-aucpr:0.14799\n",
      "[16:32:09] [6]\ttraining-aucpr:0.15304\n",
      "[16:32:09] [7]\ttraining-aucpr:0.15675\n",
      "[16:32:10] [8]\ttraining-aucpr:0.16055\n",
      "[16:32:10] [9]\ttraining-aucpr:0.16332\n",
      "[16:32:10] [10]\ttraining-aucpr:0.16658\n",
      "[16:32:10] [11]\ttraining-aucpr:0.17004\n",
      "[16:32:10] [12]\ttraining-aucpr:0.17389\n",
      "[16:32:10] [13]\ttraining-aucpr:0.18036\n",
      "[16:32:10] [14]\ttraining-aucpr:0.18375\n",
      "[16:32:10] [15]\ttraining-aucpr:0.18683\n",
      "[16:32:10] [16]\ttraining-aucpr:0.18966\n",
      "[16:32:10] [17]\ttraining-aucpr:0.19404\n",
      "[16:32:10] [18]\ttraining-aucpr:0.19939\n",
      "[16:32:10] [19]\ttraining-aucpr:0.20355\n",
      "[16:32:10] [20]\ttraining-aucpr:0.20580\n",
      "[16:32:10] [21]\ttraining-aucpr:0.20815\n",
      "[16:32:10] [22]\ttraining-aucpr:0.21330\n",
      "[16:32:10] [23]\ttraining-aucpr:0.21699\n",
      "[16:32:10] [24]\ttraining-aucpr:0.21919\n",
      "[16:32:11] [25]\ttraining-aucpr:0.22171\n",
      "[16:32:11] [26]\ttraining-aucpr:0.22543\n",
      "[16:32:11] [27]\ttraining-aucpr:0.23014\n",
      "[16:32:11] [28]\ttraining-aucpr:0.23232\n",
      "[16:32:11] [29]\ttraining-aucpr:0.23662\n",
      "[16:32:11] [30]\ttraining-aucpr:0.24035\n",
      "[16:32:11] [31]\ttraining-aucpr:0.24506\n",
      "[16:32:11] [32]\ttraining-aucpr:0.24681\n",
      "[16:32:11] [33]\ttraining-aucpr:0.25023\n",
      "[16:32:11] [34]\ttraining-aucpr:0.25310\n",
      "[16:32:11] [35]\ttraining-aucpr:0.25607\n",
      "[16:32:11] [36]\ttraining-aucpr:0.25957\n",
      "[16:32:11] [37]\ttraining-aucpr:0.26219\n",
      "[16:32:11] [38]\ttraining-aucpr:0.26455\n",
      "[16:32:11] [39]\ttraining-aucpr:0.26743\n",
      "[16:32:11] [40]\ttraining-aucpr:0.27161\n",
      "[16:32:11] [41]\ttraining-aucpr:0.27335\n",
      "[16:32:11] [42]\ttraining-aucpr:0.27703\n",
      "[16:32:12] [43]\ttraining-aucpr:0.27988\n",
      "[16:32:12] [44]\ttraining-aucpr:0.28214\n",
      "[16:32:12] [45]\ttraining-aucpr:0.28600\n",
      "[16:32:12] [46]\ttraining-aucpr:0.28769\n",
      "[16:32:12] [47]\ttraining-aucpr:0.29056\n",
      "[16:32:12] [48]\ttraining-aucpr:0.29277\n",
      "[16:32:12] [49]\ttraining-aucpr:0.29387\n",
      "[16:32:12] [50]\ttraining-aucpr:0.29755\n",
      "[16:32:12] [51]\ttraining-aucpr:0.30045\n",
      "[16:32:12] [52]\ttraining-aucpr:0.30361\n",
      "[16:32:12] [53]\ttraining-aucpr:0.30622\n",
      "[16:32:12] [54]\ttraining-aucpr:0.30861\n",
      "[16:32:12] [55]\ttraining-aucpr:0.31091\n",
      "[16:32:12] [56]\ttraining-aucpr:0.31369\n",
      "[16:32:12] [57]\ttraining-aucpr:0.31604\n",
      "[16:32:12] [58]\ttraining-aucpr:0.31889\n",
      "[16:32:12] [59]\ttraining-aucpr:0.32160\n",
      "[16:32:12] [60]\ttraining-aucpr:0.32344\n",
      "[16:32:12] [61]\ttraining-aucpr:0.32714\n",
      "[16:32:12] [62]\ttraining-aucpr:0.32884\n",
      "[16:32:13] [63]\ttraining-aucpr:0.32977\n",
      "[16:32:13] [64]\ttraining-aucpr:0.33182\n",
      "[16:32:13] [65]\ttraining-aucpr:0.33359\n",
      "[16:32:13] [66]\ttraining-aucpr:0.33460\n",
      "[16:32:13] [67]\ttraining-aucpr:0.33720\n",
      "[16:32:13] [68]\ttraining-aucpr:0.33862\n",
      "[16:32:13] [69]\ttraining-aucpr:0.34047\n",
      "[16:32:13] [70]\ttraining-aucpr:0.34440\n",
      "[16:32:13] [71]\ttraining-aucpr:0.34771\n",
      "[16:32:13] [72]\ttraining-aucpr:0.35067\n",
      "[16:32:13] [73]\ttraining-aucpr:0.35216\n",
      "[16:32:13] [74]\ttraining-aucpr:0.35452\n",
      "[16:32:13] [75]\ttraining-aucpr:0.35654\n",
      "[16:32:13] [76]\ttraining-aucpr:0.35800\n",
      "[16:32:13] [77]\ttraining-aucpr:0.35992\n",
      "[16:32:13] [78]\ttraining-aucpr:0.36203\n",
      "[16:32:13] [79]\ttraining-aucpr:0.36461\n",
      "[16:32:13] [80]\ttraining-aucpr:0.36651\n",
      "[16:32:13] [81]\ttraining-aucpr:0.36873\n",
      "[16:32:13] [82]\ttraining-aucpr:0.37097\n",
      "[16:32:13] [83]\ttraining-aucpr:0.37372\n",
      "[16:32:14] [84]\ttraining-aucpr:0.37508\n",
      "[16:32:14] [85]\ttraining-aucpr:0.37732\n",
      "[16:32:14] [86]\ttraining-aucpr:0.38008\n",
      "[16:32:14] [87]\ttraining-aucpr:0.38332\n",
      "[16:32:14] [88]\ttraining-aucpr:0.38401\n",
      "[16:32:14] [89]\ttraining-aucpr:0.38592\n",
      "[16:32:14] [90]\ttraining-aucpr:0.38794\n",
      "[16:32:14] [91]\ttraining-aucpr:0.38952\n",
      "[16:32:14] [92]\ttraining-aucpr:0.39115\n",
      "[16:32:14] [93]\ttraining-aucpr:0.39328\n",
      "[16:32:14] [94]\ttraining-aucpr:0.39553\n",
      "[16:32:14] [95]\ttraining-aucpr:0.39712\n",
      "[16:32:14] [96]\ttraining-aucpr:0.39888\n",
      "[16:32:14] [97]\ttraining-aucpr:0.40170\n",
      "[16:32:14] [98]\ttraining-aucpr:0.40365\n",
      "[16:32:14] [99]\ttraining-aucpr:0.40563\n",
      "[16:32:14] [100]\ttraining-aucpr:0.40887\n",
      "[16:32:14] [101]\ttraining-aucpr:0.41086\n",
      "[16:32:14] [102]\ttraining-aucpr:0.41241\n",
      "[16:32:14] [103]\ttraining-aucpr:0.41325\n",
      "[16:32:14] [104]\ttraining-aucpr:0.41463\n",
      "[16:32:15] [105]\ttraining-aucpr:0.41572\n",
      "[16:32:15] [106]\ttraining-aucpr:0.41876\n",
      "[16:32:15] [107]\ttraining-aucpr:0.42069\n",
      "[16:32:15] [108]\ttraining-aucpr:0.42323\n",
      "[16:32:15] [109]\ttraining-aucpr:0.42549\n",
      "[16:32:15] [110]\ttraining-aucpr:0.42716\n",
      "[16:32:15] [111]\ttraining-aucpr:0.43033\n",
      "2025-07-28 16:32:16,360 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:32:16,486 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:32:17,236] Trial 35 finished with value: 0.15163066935382888 and parameters: {'max_depth': 9, 'learning_rate': 0.08038734016328382, 'n_estimators': 112, 'min_child_weight': 9, 'gamma': 0.5222574247171057, 'subsample': 0.8909969623905027, 'colsample_bytree': 0.8485612891342659, 'reg_alpha': 0.33098555707788685, 'reg_lambda': 0.16566785053352698}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:32:17,344 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.9002217089272051, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.30717576089934273, 'learning_rate': 0.2538067452450865, 'max_depth': 5, 'min_child_weight': 7, 'reg_alpha': 0.8287931271359862, 'reg_lambda': 0.25694419601801377, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9690552806145217, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 295}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:32:18,747 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:32:19] Task 1 got rank 1\n",
      "[16:32:19] Task 2 got rank 2[16:32:19] Task 0 got rank 0\n",
      "\n",
      "[16:32:19] Task 3 got rank 3\n",
      "[16:32:20] [0]\ttraining-aucpr:0.06228\n",
      "[16:32:20] [1]\ttraining-aucpr:0.07647\n",
      "[16:32:20] [2]\ttraining-aucpr:0.08520\n",
      "[16:32:20] [3]\ttraining-aucpr:0.09773\n",
      "[16:32:20] [4]\ttraining-aucpr:0.10184\n",
      "[16:32:20] [5]\ttraining-aucpr:0.10978\n",
      "[16:32:20] [6]\ttraining-aucpr:0.11831\n",
      "[16:32:20] [7]\ttraining-aucpr:0.12335\n",
      "[16:32:20] [8]\ttraining-aucpr:0.12968\n",
      "[16:32:20] [9]\ttraining-aucpr:0.13200\n",
      "[16:32:20] [10]\ttraining-aucpr:0.13602\n",
      "[16:32:20] [11]\ttraining-aucpr:0.14217\n",
      "[16:32:20] [12]\ttraining-aucpr:0.14441\n",
      "[16:32:20] [13]\ttraining-aucpr:0.14656\n",
      "[16:32:20] [14]\ttraining-aucpr:0.14820\n",
      "[16:32:20] [15]\ttraining-aucpr:0.14946\n",
      "[16:32:20] [16]\ttraining-aucpr:0.15273\n",
      "[16:32:20] [17]\ttraining-aucpr:0.15539\n",
      "[16:32:20] [18]\ttraining-aucpr:0.15692\n",
      "[16:32:20] [19]\ttraining-aucpr:0.15772\n",
      "[16:32:20] [20]\ttraining-aucpr:0.15827\n",
      "[16:32:20] [21]\ttraining-aucpr:0.16038\n",
      "[16:32:20] [22]\ttraining-aucpr:0.16254\n",
      "[16:32:20] [23]\ttraining-aucpr:0.16370\n",
      "[16:32:20] [24]\ttraining-aucpr:0.16557\n",
      "[16:32:21] [25]\ttraining-aucpr:0.16733\n",
      "[16:32:21] [26]\ttraining-aucpr:0.16878\n",
      "[16:32:21] [27]\ttraining-aucpr:0.16933\n",
      "[16:32:21] [28]\ttraining-aucpr:0.17026\n",
      "[16:32:21] [29]\ttraining-aucpr:0.17085\n",
      "[16:32:21] [30]\ttraining-aucpr:0.17210\n",
      "[16:32:21] [31]\ttraining-aucpr:0.17293\n",
      "[16:32:21] [32]\ttraining-aucpr:0.17378\n",
      "[16:32:21] [33]\ttraining-aucpr:0.17500\n",
      "[16:32:21] [34]\ttraining-aucpr:0.17553\n",
      "[16:32:21] [35]\ttraining-aucpr:0.17593\n",
      "[16:32:21] [36]\ttraining-aucpr:0.17750\n",
      "[16:32:21] [37]\ttraining-aucpr:0.17765\n",
      "[16:32:21] [38]\ttraining-aucpr:0.17791\n",
      "[16:32:21] [39]\ttraining-aucpr:0.17838\n",
      "[16:32:21] [40]\ttraining-aucpr:0.17923\n",
      "[16:32:21] [41]\ttraining-aucpr:0.17943\n",
      "[16:32:21] [42]\ttraining-aucpr:0.18008\n",
      "[16:32:21] [43]\ttraining-aucpr:0.18059\n",
      "[16:32:21] [44]\ttraining-aucpr:0.18096\n",
      "[16:32:21] [45]\ttraining-aucpr:0.18139\n",
      "[16:32:21] [46]\ttraining-aucpr:0.18152\n",
      "[16:32:21] [47]\ttraining-aucpr:0.18234\n",
      "[16:32:21] [48]\ttraining-aucpr:0.18279\n",
      "[16:32:21] [49]\ttraining-aucpr:0.18346\n",
      "[16:32:21] [50]\ttraining-aucpr:0.18405\n",
      "[16:32:21] [51]\ttraining-aucpr:0.18442\n",
      "[16:32:21] [52]\ttraining-aucpr:0.18466\n",
      "[16:32:21] [53]\ttraining-aucpr:0.18524\n",
      "[16:32:21] [54]\ttraining-aucpr:0.18561\n",
      "[16:32:21] [55]\ttraining-aucpr:0.18597\n",
      "[16:32:21] [56]\ttraining-aucpr:0.18657\n",
      "[16:32:21] [57]\ttraining-aucpr:0.18700\n",
      "[16:32:21] [58]\ttraining-aucpr:0.18808\n",
      "[16:32:21] [59]\ttraining-aucpr:0.18837\n",
      "[16:32:21] [60]\ttraining-aucpr:0.18891\n",
      "[16:32:21] [61]\ttraining-aucpr:0.18912\n",
      "[16:32:22] [62]\ttraining-aucpr:0.18946\n",
      "[16:32:22] [63]\ttraining-aucpr:0.18976\n",
      "[16:32:22] [64]\ttraining-aucpr:0.19016\n",
      "[16:32:22] [65]\ttraining-aucpr:0.19047\n",
      "[16:32:22] [66]\ttraining-aucpr:0.19085\n",
      "[16:32:22] [67]\ttraining-aucpr:0.19215\n",
      "[16:32:22] [68]\ttraining-aucpr:0.19299\n",
      "[16:32:22] [69]\ttraining-aucpr:0.19316\n",
      "[16:32:22] [70]\ttraining-aucpr:0.19345\n",
      "[16:32:22] [71]\ttraining-aucpr:0.19384\n",
      "[16:32:22] [72]\ttraining-aucpr:0.19440\n",
      "[16:32:22] [73]\ttraining-aucpr:0.19465\n",
      "[16:32:22] [74]\ttraining-aucpr:0.19484\n",
      "[16:32:22] [75]\ttraining-aucpr:0.19547\n",
      "[16:32:22] [76]\ttraining-aucpr:0.19590\n",
      "[16:32:22] [77]\ttraining-aucpr:0.19633\n",
      "[16:32:22] [78]\ttraining-aucpr:0.19685\n",
      "[16:32:22] [79]\ttraining-aucpr:0.19748\n",
      "[16:32:22] [80]\ttraining-aucpr:0.19763\n",
      "[16:32:22] [81]\ttraining-aucpr:0.19811\n",
      "[16:32:22] [82]\ttraining-aucpr:0.19841\n",
      "[16:32:22] [83]\ttraining-aucpr:0.19934\n",
      "[16:32:22] [84]\ttraining-aucpr:0.19962\n",
      "[16:32:22] [85]\ttraining-aucpr:0.20003\n",
      "[16:32:22] [86]\ttraining-aucpr:0.20082\n",
      "[16:32:22] [87]\ttraining-aucpr:0.20113\n",
      "[16:32:22] [88]\ttraining-aucpr:0.20147\n",
      "[16:32:22] [89]\ttraining-aucpr:0.20266\n",
      "[16:32:22] [90]\ttraining-aucpr:0.20286\n",
      "[16:32:22] [91]\ttraining-aucpr:0.20308\n",
      "[16:32:22] [92]\ttraining-aucpr:0.20333\n",
      "[16:32:22] [93]\ttraining-aucpr:0.20356\n",
      "[16:32:22] [94]\ttraining-aucpr:0.20380\n",
      "[16:32:22] [95]\ttraining-aucpr:0.20411\n",
      "[16:32:22] [96]\ttraining-aucpr:0.20430\n",
      "[16:32:22] [97]\ttraining-aucpr:0.20464\n",
      "[16:32:22] [98]\ttraining-aucpr:0.20523\n",
      "[16:32:22] [99]\ttraining-aucpr:0.20513\n",
      "[16:32:22] [100]\ttraining-aucpr:0.20563\n",
      "[16:32:22] [101]\ttraining-aucpr:0.20582\n",
      "[16:32:23] [102]\ttraining-aucpr:0.20605\n",
      "[16:32:23] [103]\ttraining-aucpr:0.20640\n",
      "[16:32:23] [104]\ttraining-aucpr:0.20666\n",
      "[16:32:23] [105]\ttraining-aucpr:0.20696\n",
      "[16:32:23] [106]\ttraining-aucpr:0.20732\n",
      "[16:32:23] [107]\ttraining-aucpr:0.20766\n",
      "[16:32:23] [108]\ttraining-aucpr:0.20807\n",
      "[16:32:23] [109]\ttraining-aucpr:0.20851\n",
      "[16:32:23] [110]\ttraining-aucpr:0.20875\n",
      "[16:32:23] [111]\ttraining-aucpr:0.20924\n",
      "[16:32:23] [112]\ttraining-aucpr:0.20995\n",
      "[16:32:23] [113]\ttraining-aucpr:0.21035\n",
      "[16:32:23] [114]\ttraining-aucpr:0.21062\n",
      "[16:32:23] [115]\ttraining-aucpr:0.21109\n",
      "[16:32:23] [116]\ttraining-aucpr:0.21122\n",
      "[16:32:23] [117]\ttraining-aucpr:0.21151\n",
      "[16:32:23] [118]\ttraining-aucpr:0.21198\n",
      "[16:32:23] [119]\ttraining-aucpr:0.21223\n",
      "[16:32:23] [120]\ttraining-aucpr:0.21260\n",
      "[16:32:23] [121]\ttraining-aucpr:0.21317\n",
      "[16:32:23] [122]\ttraining-aucpr:0.21355\n",
      "[16:32:23] [123]\ttraining-aucpr:0.21382\n",
      "[16:32:23] [124]\ttraining-aucpr:0.21422\n",
      "[16:32:23] [125]\ttraining-aucpr:0.21538\n",
      "[16:32:23] [126]\ttraining-aucpr:0.21610\n",
      "[16:32:23] [127]\ttraining-aucpr:0.21654\n",
      "[16:32:23] [128]\ttraining-aucpr:0.21707\n",
      "[16:32:23] [129]\ttraining-aucpr:0.21730\n",
      "[16:32:23] [130]\ttraining-aucpr:0.21776\n",
      "[16:32:23] [131]\ttraining-aucpr:0.21840\n",
      "[16:32:23] [132]\ttraining-aucpr:0.21873\n",
      "[16:32:23] [133]\ttraining-aucpr:0.21896\n",
      "[16:32:23] [134]\ttraining-aucpr:0.21950\n",
      "[16:32:23] [135]\ttraining-aucpr:0.21986\n",
      "[16:32:23] [136]\ttraining-aucpr:0.22033\n",
      "[16:32:23] [137]\ttraining-aucpr:0.22107\n",
      "[16:32:23] [138]\ttraining-aucpr:0.22153\n",
      "[16:32:23] [139]\ttraining-aucpr:0.22173\n",
      "[16:32:24] [140]\ttraining-aucpr:0.22223\n",
      "[16:32:24] [141]\ttraining-aucpr:0.22230\n",
      "[16:32:24] [142]\ttraining-aucpr:0.22273\n",
      "[16:32:24] [143]\ttraining-aucpr:0.22287\n",
      "[16:32:24] [144]\ttraining-aucpr:0.22332\n",
      "[16:32:24] [145]\ttraining-aucpr:0.22404\n",
      "[16:32:24] [146]\ttraining-aucpr:0.22409\n",
      "[16:32:24] [147]\ttraining-aucpr:0.22448\n",
      "[16:32:24] [148]\ttraining-aucpr:0.22492\n",
      "[16:32:24] [149]\ttraining-aucpr:0.22536\n",
      "[16:32:24] [150]\ttraining-aucpr:0.22561\n",
      "[16:32:24] [151]\ttraining-aucpr:0.22576\n",
      "[16:32:24] [152]\ttraining-aucpr:0.22605\n",
      "[16:32:24] [153]\ttraining-aucpr:0.22667\n",
      "[16:32:24] [154]\ttraining-aucpr:0.22686\n",
      "[16:32:24] [155]\ttraining-aucpr:0.22700\n",
      "[16:32:24] [156]\ttraining-aucpr:0.22790\n",
      "[16:32:24] [157]\ttraining-aucpr:0.22825\n",
      "[16:32:24] [158]\ttraining-aucpr:0.22886\n",
      "[16:32:24] [159]\ttraining-aucpr:0.22954\n",
      "[16:32:24] [160]\ttraining-aucpr:0.22986\n",
      "[16:32:24] [161]\ttraining-aucpr:0.23019\n",
      "[16:32:24] [162]\ttraining-aucpr:0.23050\n",
      "[16:32:24] [163]\ttraining-aucpr:0.23078\n",
      "[16:32:24] [164]\ttraining-aucpr:0.23103\n",
      "[16:32:24] [165]\ttraining-aucpr:0.23140\n",
      "[16:32:24] [166]\ttraining-aucpr:0.23190\n",
      "[16:32:24] [167]\ttraining-aucpr:0.23208\n",
      "[16:32:24] [168]\ttraining-aucpr:0.23244\n",
      "[16:32:24] [169]\ttraining-aucpr:0.23323\n",
      "[16:32:24] [170]\ttraining-aucpr:0.23375\n",
      "[16:32:24] [171]\ttraining-aucpr:0.23420\n",
      "[16:32:24] [172]\ttraining-aucpr:0.23469\n",
      "[16:32:24] [173]\ttraining-aucpr:0.23471\n",
      "[16:32:24] [174]\ttraining-aucpr:0.23535\n",
      "[16:32:24] [175]\ttraining-aucpr:0.23565\n",
      "[16:32:24] [176]\ttraining-aucpr:0.23617\n",
      "[16:32:24] [177]\ttraining-aucpr:0.23675\n",
      "[16:32:24] [178]\ttraining-aucpr:0.23705\n",
      "[16:32:25] [179]\ttraining-aucpr:0.23783\n",
      "[16:32:25] [180]\ttraining-aucpr:0.23803\n",
      "[16:32:25] [181]\ttraining-aucpr:0.23853\n",
      "[16:32:25] [182]\ttraining-aucpr:0.23902\n",
      "[16:32:25] [183]\ttraining-aucpr:0.23954\n",
      "[16:32:25] [184]\ttraining-aucpr:0.23971\n",
      "[16:32:25] [185]\ttraining-aucpr:0.23983\n",
      "[16:32:25] [186]\ttraining-aucpr:0.24039\n",
      "[16:32:25] [187]\ttraining-aucpr:0.24083\n",
      "[16:32:25] [188]\ttraining-aucpr:0.24122\n",
      "[16:32:25] [189]\ttraining-aucpr:0.24155\n",
      "[16:32:25] [190]\ttraining-aucpr:0.24221\n",
      "[16:32:25] [191]\ttraining-aucpr:0.24273\n",
      "[16:32:25] [192]\ttraining-aucpr:0.24301\n",
      "[16:32:25] [193]\ttraining-aucpr:0.24376\n",
      "[16:32:25] [194]\ttraining-aucpr:0.24412\n",
      "[16:32:25] [195]\ttraining-aucpr:0.24447\n",
      "[16:32:25] [196]\ttraining-aucpr:0.24475\n",
      "[16:32:25] [197]\ttraining-aucpr:0.24503\n",
      "[16:32:25] [198]\ttraining-aucpr:0.24585\n",
      "[16:32:25] [199]\ttraining-aucpr:0.24638\n",
      "[16:32:25] [200]\ttraining-aucpr:0.24683\n",
      "[16:32:25] [201]\ttraining-aucpr:0.24731\n",
      "[16:32:25] [202]\ttraining-aucpr:0.24759\n",
      "[16:32:25] [203]\ttraining-aucpr:0.24801\n",
      "[16:32:25] [204]\ttraining-aucpr:0.24846\n",
      "[16:32:25] [205]\ttraining-aucpr:0.24872\n",
      "[16:32:25] [206]\ttraining-aucpr:0.24937\n",
      "[16:32:25] [207]\ttraining-aucpr:0.25004\n",
      "[16:32:25] [208]\ttraining-aucpr:0.25040\n",
      "[16:32:25] [209]\ttraining-aucpr:0.25098\n",
      "[16:32:25] [210]\ttraining-aucpr:0.25131\n",
      "[16:32:25] [211]\ttraining-aucpr:0.25167\n",
      "[16:32:25] [212]\ttraining-aucpr:0.25198\n",
      "[16:32:25] [213]\ttraining-aucpr:0.25220\n",
      "[16:32:25] [214]\ttraining-aucpr:0.25267\n",
      "[16:32:25] [215]\ttraining-aucpr:0.25298\n",
      "[16:32:26] [216]\ttraining-aucpr:0.25359\n",
      "[16:32:26] [217]\ttraining-aucpr:0.25400\n",
      "[16:32:26] [218]\ttraining-aucpr:0.25436\n",
      "[16:32:26] [219]\ttraining-aucpr:0.25510\n",
      "[16:32:26] [220]\ttraining-aucpr:0.25549\n",
      "[16:32:26] [221]\ttraining-aucpr:0.25603\n",
      "[16:32:26] [222]\ttraining-aucpr:0.25654\n",
      "[16:32:26] [223]\ttraining-aucpr:0.25699\n",
      "[16:32:26] [224]\ttraining-aucpr:0.25722\n",
      "[16:32:26] [225]\ttraining-aucpr:0.25754\n",
      "[16:32:26] [226]\ttraining-aucpr:0.25825\n",
      "[16:32:26] [227]\ttraining-aucpr:0.25863\n",
      "[16:32:26] [228]\ttraining-aucpr:0.25912\n",
      "[16:32:26] [229]\ttraining-aucpr:0.25941\n",
      "[16:32:26] [230]\ttraining-aucpr:0.25989\n",
      "[16:32:26] [231]\ttraining-aucpr:0.26006\n",
      "[16:32:26] [232]\ttraining-aucpr:0.26057\n",
      "[16:32:26] [233]\ttraining-aucpr:0.26085\n",
      "[16:32:26] [234]\ttraining-aucpr:0.26113\n",
      "[16:32:26] [235]\ttraining-aucpr:0.26154\n",
      "[16:32:26] [236]\ttraining-aucpr:0.26186\n",
      "[16:32:26] [237]\ttraining-aucpr:0.26227\n",
      "[16:32:26] [238]\ttraining-aucpr:0.26238\n",
      "[16:32:26] [239]\ttraining-aucpr:0.26284\n",
      "[16:32:26] [240]\ttraining-aucpr:0.26326\n",
      "[16:32:26] [241]\ttraining-aucpr:0.26372\n",
      "[16:32:26] [242]\ttraining-aucpr:0.26395\n",
      "[16:32:26] [243]\ttraining-aucpr:0.26483\n",
      "[16:32:26] [244]\ttraining-aucpr:0.26513\n",
      "[16:32:26] [245]\ttraining-aucpr:0.26549\n",
      "[16:32:26] [246]\ttraining-aucpr:0.26598\n",
      "[16:32:26] [247]\ttraining-aucpr:0.26652\n",
      "[16:32:26] [248]\ttraining-aucpr:0.26703\n",
      "[16:32:26] [249]\ttraining-aucpr:0.26746\n",
      "[16:32:26] [250]\ttraining-aucpr:0.26808\n",
      "[16:32:26] [251]\ttraining-aucpr:0.26834\n",
      "[16:32:26] [252]\ttraining-aucpr:0.26877\n",
      "[16:32:26] [253]\ttraining-aucpr:0.26908\n",
      "[16:32:27] [254]\ttraining-aucpr:0.26960\n",
      "[16:32:27] [255]\ttraining-aucpr:0.27002\n",
      "[16:32:27] [256]\ttraining-aucpr:0.27027\n",
      "[16:32:27] [257]\ttraining-aucpr:0.27070\n",
      "[16:32:27] [258]\ttraining-aucpr:0.27101\n",
      "[16:32:27] [259]\ttraining-aucpr:0.27141\n",
      "[16:32:27] [260]\ttraining-aucpr:0.27192\n",
      "[16:32:27] [261]\ttraining-aucpr:0.27227\n",
      "[16:32:27] [262]\ttraining-aucpr:0.27271\n",
      "[16:32:27] [263]\ttraining-aucpr:0.27328\n",
      "[16:32:27] [264]\ttraining-aucpr:0.27374\n",
      "[16:32:27] [265]\ttraining-aucpr:0.27408\n",
      "[16:32:27] [266]\ttraining-aucpr:0.27436\n",
      "[16:32:27] [267]\ttraining-aucpr:0.27530\n",
      "[16:32:27] [268]\ttraining-aucpr:0.27573\n",
      "[16:32:27] [269]\ttraining-aucpr:0.27604\n",
      "[16:32:27] [270]\ttraining-aucpr:0.27678\n",
      "[16:32:27] [271]\ttraining-aucpr:0.27696\n",
      "[16:32:27] [272]\ttraining-aucpr:0.27750\n",
      "[16:32:27] [273]\ttraining-aucpr:0.27790\n",
      "[16:32:27] [274]\ttraining-aucpr:0.27835\n",
      "[16:32:27] [275]\ttraining-aucpr:0.27951\n",
      "[16:32:27] [276]\ttraining-aucpr:0.27987\n",
      "[16:32:27] [277]\ttraining-aucpr:0.28001\n",
      "[16:32:27] [278]\ttraining-aucpr:0.28081\n",
      "[16:32:27] [279]\ttraining-aucpr:0.28164\n",
      "[16:32:27] [280]\ttraining-aucpr:0.28218\n",
      "[16:32:27] [281]\ttraining-aucpr:0.28296\n",
      "[16:32:27] [282]\ttraining-aucpr:0.28353\n",
      "[16:32:27] [283]\ttraining-aucpr:0.28396\n",
      "[16:32:27] [284]\ttraining-aucpr:0.28419\n",
      "[16:32:27] [285]\ttraining-aucpr:0.28467\n",
      "[16:32:27] [286]\ttraining-aucpr:0.28477\n",
      "[16:32:27] [287]\ttraining-aucpr:0.28507\n",
      "[16:32:27] [288]\ttraining-aucpr:0.28609\n",
      "[16:32:27] [289]\ttraining-aucpr:0.28659\n",
      "[16:32:27] [290]\ttraining-aucpr:0.28729\n",
      "[16:32:27] [291]\ttraining-aucpr:0.28759\n",
      "[16:32:27] [292]\ttraining-aucpr:0.28767\n",
      "[16:32:28] [293]\ttraining-aucpr:0.28811\n",
      "[16:32:28] [294]\ttraining-aucpr:0.28850\n",
      "2025-07-28 16:32:29,086 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "25/07/28 16:32:29 WARN DAGScheduler: Broadcasting large task binary with size 1013.4 KiB\n",
      "2025-07-28 16:32:29,172 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:32:29,854] Trial 36 finished with value: 0.14174822235555967 and parameters: {'max_depth': 5, 'learning_rate': 0.2538067452450865, 'n_estimators': 295, 'min_child_weight': 7, 'gamma': 0.30717576089934273, 'subsample': 0.9690552806145217, 'colsample_bytree': 0.9002217089272051, 'reg_alpha': 0.8287931271359862, 'reg_lambda': 0.25694419601801377}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:32:29,978 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.5567598685384797, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.5892064395941576, 'learning_rate': 0.16196623397857818, 'max_depth': 4, 'min_child_weight': 8, 'reg_alpha': 0.6681459340939641, 'reg_lambda': 0.5777434839578776, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9261216792331302, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 195}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:32:31,365 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:32:32] Task 0 got rank 0\n",
      "[16:32:32] Task 2 got rank 2\n",
      "[16:32:32] Task 1 got rank 1\n",
      "[16:32:32] Task 3 got rank 3\n",
      "[16:32:32] [0]\ttraining-aucpr:0.05530\n",
      "[16:32:32] [1]\ttraining-aucpr:0.06479\n",
      "[16:32:33] [2]\ttraining-aucpr:0.07911\n",
      "[16:32:33] [3]\ttraining-aucpr:0.07951\n",
      "[16:32:33] [4]\ttraining-aucpr:0.08049\n",
      "[16:32:33] [5]\ttraining-aucpr:0.08806\n",
      "[16:32:33] [6]\ttraining-aucpr:0.09624\n",
      "[16:32:33] [7]\ttraining-aucpr:0.09741\n",
      "[16:32:33] [8]\ttraining-aucpr:0.09953\n",
      "[16:32:33] [9]\ttraining-aucpr:0.11139\n",
      "[16:32:33] [10]\ttraining-aucpr:0.11542\n",
      "[16:32:33] [11]\ttraining-aucpr:0.12101\n",
      "[16:32:33] [12]\ttraining-aucpr:0.12110\n",
      "[16:32:33] [13]\ttraining-aucpr:0.12268\n",
      "[16:32:33] [14]\ttraining-aucpr:0.12582\n",
      "[16:32:33] [15]\ttraining-aucpr:0.12734\n",
      "[16:32:33] [16]\ttraining-aucpr:0.12959\n",
      "[16:32:33] [17]\ttraining-aucpr:0.13233\n",
      "[16:32:33] [18]\ttraining-aucpr:0.13421\n",
      "[16:32:33] [19]\ttraining-aucpr:0.13642\n",
      "[16:32:33] [20]\ttraining-aucpr:0.13688\n",
      "[16:32:33] [21]\ttraining-aucpr:0.13856\n",
      "[16:32:33] [22]\ttraining-aucpr:0.13937\n",
      "[16:32:33] [23]\ttraining-aucpr:0.13932\n",
      "[16:32:33] [24]\ttraining-aucpr:0.14097\n",
      "[16:32:33] [25]\ttraining-aucpr:0.14159\n",
      "[16:32:33] [26]\ttraining-aucpr:0.14315\n",
      "[16:32:34] [27]\ttraining-aucpr:0.14552\n",
      "[16:32:34] [28]\ttraining-aucpr:0.14592\n",
      "[16:32:34] [29]\ttraining-aucpr:0.14697\n",
      "[16:32:34] [30]\ttraining-aucpr:0.14809\n",
      "[16:32:34] [31]\ttraining-aucpr:0.14887\n",
      "[16:32:34] [32]\ttraining-aucpr:0.14962\n",
      "[16:32:34] [33]\ttraining-aucpr:0.15082\n",
      "[16:32:34] [34]\ttraining-aucpr:0.15187\n",
      "[16:32:34] [35]\ttraining-aucpr:0.15279\n",
      "[16:32:34] [36]\ttraining-aucpr:0.15374\n",
      "[16:32:34] [37]\ttraining-aucpr:0.15566\n",
      "[16:32:34] [38]\ttraining-aucpr:0.15646\n",
      "[16:32:34] [39]\ttraining-aucpr:0.15682\n",
      "[16:32:34] [40]\ttraining-aucpr:0.15723\n",
      "[16:32:34] [41]\ttraining-aucpr:0.15736\n",
      "[16:32:34] [42]\ttraining-aucpr:0.15875\n",
      "[16:32:34] [43]\ttraining-aucpr:0.16011\n",
      "[16:32:34] [44]\ttraining-aucpr:0.16086\n",
      "[16:32:34] [45]\ttraining-aucpr:0.16170\n",
      "[16:32:34] [46]\ttraining-aucpr:0.16243\n",
      "[16:32:34] [47]\ttraining-aucpr:0.16274\n",
      "[16:32:34] [48]\ttraining-aucpr:0.16324\n",
      "[16:32:34] [49]\ttraining-aucpr:0.16437\n",
      "[16:32:34] [50]\ttraining-aucpr:0.16527\n",
      "[16:32:34] [51]\ttraining-aucpr:0.16597\n",
      "[16:32:35] [52]\ttraining-aucpr:0.16602\n",
      "[16:32:35] [53]\ttraining-aucpr:0.16667\n",
      "[16:32:35] [54]\ttraining-aucpr:0.16741\n",
      "[16:32:35] [55]\ttraining-aucpr:0.16782\n",
      "[16:32:35] [56]\ttraining-aucpr:0.16832\n",
      "[16:32:35] [57]\ttraining-aucpr:0.16944\n",
      "[16:32:35] [58]\ttraining-aucpr:0.16977\n",
      "[16:32:35] [59]\ttraining-aucpr:0.16979\n",
      "[16:32:35] [60]\ttraining-aucpr:0.17012\n",
      "[16:32:35] [61]\ttraining-aucpr:0.17056\n",
      "[16:32:35] [62]\ttraining-aucpr:0.17104\n",
      "[16:32:35] [63]\ttraining-aucpr:0.17135\n",
      "[16:32:35] [64]\ttraining-aucpr:0.17135\n",
      "[16:32:35] [65]\ttraining-aucpr:0.17175\n",
      "[16:32:35] [66]\ttraining-aucpr:0.17200\n",
      "[16:32:35] [67]\ttraining-aucpr:0.17228\n",
      "[16:32:35] [68]\ttraining-aucpr:0.17272\n",
      "[16:32:35] [69]\ttraining-aucpr:0.17313\n",
      "[16:32:35] [70]\ttraining-aucpr:0.17352\n",
      "[16:32:35] [71]\ttraining-aucpr:0.17353\n",
      "[16:32:35] [72]\ttraining-aucpr:0.17391\n",
      "[16:32:35] [73]\ttraining-aucpr:0.17407\n",
      "[16:32:35] [74]\ttraining-aucpr:0.17431\n",
      "[16:32:35] [75]\ttraining-aucpr:0.17486\n",
      "[16:32:36] [76]\ttraining-aucpr:0.17518\n",
      "[16:32:36] [77]\ttraining-aucpr:0.17541\n",
      "[16:32:36] [78]\ttraining-aucpr:0.17553\n",
      "[16:32:36] [79]\ttraining-aucpr:0.17577\n",
      "[16:32:36] [80]\ttraining-aucpr:0.17607\n",
      "[16:32:36] [81]\ttraining-aucpr:0.17641\n",
      "[16:32:36] [82]\ttraining-aucpr:0.17651\n",
      "[16:32:36] [83]\ttraining-aucpr:0.17676\n",
      "[16:32:36] [84]\ttraining-aucpr:0.17687\n",
      "[16:32:36] [85]\ttraining-aucpr:0.17705\n",
      "[16:32:36] [86]\ttraining-aucpr:0.17740\n",
      "[16:32:36] [87]\ttraining-aucpr:0.17767\n",
      "[16:32:36] [88]\ttraining-aucpr:0.17788\n",
      "[16:32:36] [89]\ttraining-aucpr:0.17804\n",
      "[16:32:36] [90]\ttraining-aucpr:0.17811\n",
      "[16:32:36] [91]\ttraining-aucpr:0.17827\n",
      "[16:32:36] [92]\ttraining-aucpr:0.17835\n",
      "[16:32:36] [93]\ttraining-aucpr:0.17855\n",
      "[16:32:36] [94]\ttraining-aucpr:0.17870\n",
      "[16:32:36] [95]\ttraining-aucpr:0.17891\n",
      "[16:32:36] [96]\ttraining-aucpr:0.17896\n",
      "[16:32:36] [97]\ttraining-aucpr:0.17922\n",
      "[16:32:36] [98]\ttraining-aucpr:0.17942\n",
      "[16:32:36] [99]\ttraining-aucpr:0.17963\n",
      "[16:32:36] [100]\ttraining-aucpr:0.17961\n",
      "[16:32:36] [101]\ttraining-aucpr:0.17962\n",
      "[16:32:37] [102]\ttraining-aucpr:0.17976\n",
      "[16:32:37] [103]\ttraining-aucpr:0.17985\n",
      "[16:32:37] [104]\ttraining-aucpr:0.17991\n",
      "[16:32:37] [105]\ttraining-aucpr:0.18000\n",
      "[16:32:37] [106]\ttraining-aucpr:0.18027\n",
      "[16:32:37] [107]\ttraining-aucpr:0.18032\n",
      "[16:32:37] [108]\ttraining-aucpr:0.18052\n",
      "[16:32:37] [109]\ttraining-aucpr:0.18076\n",
      "[16:32:37] [110]\ttraining-aucpr:0.18078\n",
      "[16:32:37] [111]\ttraining-aucpr:0.18091\n",
      "[16:32:37] [112]\ttraining-aucpr:0.18131\n",
      "[16:32:37] [113]\ttraining-aucpr:0.18171\n",
      "[16:32:37] [114]\ttraining-aucpr:0.18191\n",
      "[16:32:37] [115]\ttraining-aucpr:0.18206\n",
      "[16:32:37] [116]\ttraining-aucpr:0.18218\n",
      "[16:32:37] [117]\ttraining-aucpr:0.18238\n",
      "[16:32:37] [118]\ttraining-aucpr:0.18247\n",
      "[16:32:37] [119]\ttraining-aucpr:0.18254\n",
      "[16:32:37] [120]\ttraining-aucpr:0.18268\n",
      "[16:32:37] [121]\ttraining-aucpr:0.18278\n",
      "[16:32:37] [122]\ttraining-aucpr:0.18308\n",
      "[16:32:37] [123]\ttraining-aucpr:0.18305\n",
      "[16:32:37] [124]\ttraining-aucpr:0.18326\n",
      "[16:32:37] [125]\ttraining-aucpr:0.18338\n",
      "[16:32:37] [126]\ttraining-aucpr:0.18351\n",
      "[16:32:38] [127]\ttraining-aucpr:0.18362\n",
      "[16:32:38] [128]\ttraining-aucpr:0.18384\n",
      "[16:32:38] [129]\ttraining-aucpr:0.18416\n",
      "[16:32:38] [130]\ttraining-aucpr:0.18420\n",
      "[16:32:38] [131]\ttraining-aucpr:0.18453\n",
      "[16:32:38] [132]\ttraining-aucpr:0.18463\n",
      "[16:32:38] [133]\ttraining-aucpr:0.18462\n",
      "[16:32:38] [134]\ttraining-aucpr:0.18472\n",
      "[16:32:38] [135]\ttraining-aucpr:0.18496\n",
      "[16:32:38] [136]\ttraining-aucpr:0.18499\n",
      "[16:32:38] [137]\ttraining-aucpr:0.18504\n",
      "[16:32:38] [138]\ttraining-aucpr:0.18499\n",
      "[16:32:38] [139]\ttraining-aucpr:0.18521\n",
      "[16:32:38] [140]\ttraining-aucpr:0.18546\n",
      "[16:32:38] [141]\ttraining-aucpr:0.18554\n",
      "[16:32:38] [142]\ttraining-aucpr:0.18569\n",
      "[16:32:38] [143]\ttraining-aucpr:0.18576\n",
      "[16:32:38] [144]\ttraining-aucpr:0.18584\n",
      "[16:32:38] [145]\ttraining-aucpr:0.18607\n",
      "[16:32:38] [146]\ttraining-aucpr:0.18633\n",
      "[16:32:38] [147]\ttraining-aucpr:0.18636\n",
      "[16:32:38] [148]\ttraining-aucpr:0.18642\n",
      "[16:32:38] [149]\ttraining-aucpr:0.18655\n",
      "[16:32:38] [150]\ttraining-aucpr:0.18655\n",
      "[16:32:38] [151]\ttraining-aucpr:0.18671\n",
      "[16:32:39] [152]\ttraining-aucpr:0.18677\n",
      "[16:32:39] [153]\ttraining-aucpr:0.18688\n",
      "[16:32:39] [154]\ttraining-aucpr:0.18704\n",
      "[16:32:39] [155]\ttraining-aucpr:0.18717\n",
      "[16:32:39] [156]\ttraining-aucpr:0.18741\n",
      "[16:32:39] [157]\ttraining-aucpr:0.18750\n",
      "[16:32:39] [158]\ttraining-aucpr:0.18775\n",
      "[16:32:39] [159]\ttraining-aucpr:0.18825\n",
      "[16:32:39] [160]\ttraining-aucpr:0.18831\n",
      "[16:32:39] [161]\ttraining-aucpr:0.18849\n",
      "[16:32:39] [162]\ttraining-aucpr:0.18892\n",
      "[16:32:39] [163]\ttraining-aucpr:0.18928\n",
      "[16:32:39] [164]\ttraining-aucpr:0.18938\n",
      "[16:32:39] [165]\ttraining-aucpr:0.18940\n",
      "[16:32:39] [166]\ttraining-aucpr:0.18953\n",
      "[16:32:39] [167]\ttraining-aucpr:0.18957\n",
      "[16:32:39] [168]\ttraining-aucpr:0.18960\n",
      "[16:32:39] [169]\ttraining-aucpr:0.18982\n",
      "[16:32:39] [170]\ttraining-aucpr:0.18980\n",
      "[16:32:39] [171]\ttraining-aucpr:0.18990\n",
      "[16:32:39] [172]\ttraining-aucpr:0.18990\n",
      "[16:32:39] [173]\ttraining-aucpr:0.18995\n",
      "[16:32:39] [174]\ttraining-aucpr:0.19008\n",
      "[16:32:39] [175]\ttraining-aucpr:0.19007\n",
      "[16:32:39] [176]\ttraining-aucpr:0.19032\n",
      "[16:32:39] [177]\ttraining-aucpr:0.19036\n",
      "[16:32:39] [178]\ttraining-aucpr:0.19016\n",
      "[16:32:40] [179]\ttraining-aucpr:0.19046\n",
      "[16:32:40] [180]\ttraining-aucpr:0.19064\n",
      "[16:32:40] [181]\ttraining-aucpr:0.19061\n",
      "[16:32:40] [182]\ttraining-aucpr:0.19092\n",
      "[16:32:40] [183]\ttraining-aucpr:0.19103\n",
      "[16:32:40] [184]\ttraining-aucpr:0.19113\n",
      "[16:32:40] [185]\ttraining-aucpr:0.19123\n",
      "[16:32:40] [186]\ttraining-aucpr:0.19130\n",
      "[16:32:40] [187]\ttraining-aucpr:0.19139\n",
      "[16:32:40] [188]\ttraining-aucpr:0.19175\n",
      "[16:32:40] [189]\ttraining-aucpr:0.19182\n",
      "[16:32:40] [190]\ttraining-aucpr:0.19194\n",
      "[16:32:40] [191]\ttraining-aucpr:0.19204\n",
      "[16:32:40] [192]\ttraining-aucpr:0.19215\n",
      "[16:32:40] [193]\ttraining-aucpr:0.19222\n",
      "[16:32:40] [194]\ttraining-aucpr:0.19234\n",
      "2025-07-28 16:32:41,616 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:32:41,701 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:32:42,472] Trial 37 finished with value: 0.18176130749587033 and parameters: {'max_depth': 4, 'learning_rate': 0.16196623397857818, 'n_estimators': 195, 'min_child_weight': 8, 'gamma': 0.5892064395941576, 'subsample': 0.9261216792331302, 'colsample_bytree': 0.5567598685384797, 'reg_alpha': 0.6681459340939641, 'reg_lambda': 0.5777434839578776}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:32:42,580 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.9558282502554007, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.6997987848399297, 'learning_rate': 0.09441602758331123, 'max_depth': 5, 'min_child_weight': 3, 'reg_alpha': 0.11780405100113238, 'reg_lambda': 0.0002774603834067753, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.8327965435862219, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 74}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:32:43,949 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:32:44] Task 0 got rank 0[16:32:44] Task 1 got rank 1\n",
      "\n",
      "[16:32:44] Task 2 got rank 2\n",
      "[16:32:44] Task 3 got rank 3\n",
      "[16:32:45] [0]\ttraining-aucpr:0.06244\n",
      "[16:32:45] [1]\ttraining-aucpr:0.07502\n",
      "[16:32:45] [2]\ttraining-aucpr:0.08198\n",
      "[16:32:45] [3]\ttraining-aucpr:0.08280\n",
      "[16:32:45] [4]\ttraining-aucpr:0.08891\n",
      "[16:32:45] [5]\ttraining-aucpr:0.09323\n",
      "[16:32:45] [6]\ttraining-aucpr:0.09320\n",
      "[16:32:45] [7]\ttraining-aucpr:0.09334\n",
      "[16:32:45] [8]\ttraining-aucpr:0.09889\n",
      "[16:32:45] [9]\ttraining-aucpr:0.10236\n",
      "[16:32:45] [10]\ttraining-aucpr:0.10318\n",
      "[16:32:45] [11]\ttraining-aucpr:0.10522\n",
      "[16:32:45] [12]\ttraining-aucpr:0.10933\n",
      "[16:32:45] [13]\ttraining-aucpr:0.11117\n",
      "[16:32:45] [14]\ttraining-aucpr:0.11351\n",
      "[16:32:45] [15]\ttraining-aucpr:0.11716\n",
      "[16:32:46] [16]\ttraining-aucpr:0.12005\n",
      "[16:32:46] [17]\ttraining-aucpr:0.12180\n",
      "[16:32:46] [18]\ttraining-aucpr:0.12335\n",
      "[16:32:46] [19]\ttraining-aucpr:0.12468\n",
      "[16:32:46] [20]\ttraining-aucpr:0.12668\n",
      "[16:32:46] [21]\ttraining-aucpr:0.12992\n",
      "[16:32:46] [22]\ttraining-aucpr:0.13152\n",
      "[16:32:46] [23]\ttraining-aucpr:0.13320\n",
      "[16:32:46] [24]\ttraining-aucpr:0.13461\n",
      "[16:32:46] [25]\ttraining-aucpr:0.13691\n",
      "[16:32:46] [26]\ttraining-aucpr:0.13827\n",
      "[16:32:46] [27]\ttraining-aucpr:0.13987\n",
      "[16:32:46] [28]\ttraining-aucpr:0.14084\n",
      "[16:32:46] [29]\ttraining-aucpr:0.14285\n",
      "[16:32:46] [30]\ttraining-aucpr:0.14364\n",
      "[16:32:46] [31]\ttraining-aucpr:0.14425\n",
      "[16:32:46] [32]\ttraining-aucpr:0.14528\n",
      "[16:32:46] [33]\ttraining-aucpr:0.14547\n",
      "[16:32:46] [34]\ttraining-aucpr:0.14712\n",
      "[16:32:46] [35]\ttraining-aucpr:0.14776\n",
      "[16:32:46] [36]\ttraining-aucpr:0.14887\n",
      "[16:32:46] [37]\ttraining-aucpr:0.14945\n",
      "[16:32:46] [38]\ttraining-aucpr:0.14972\n",
      "[16:32:46] [39]\ttraining-aucpr:0.15116\n",
      "[16:32:46] [40]\ttraining-aucpr:0.15208\n",
      "[16:32:46] [41]\ttraining-aucpr:0.15409\n",
      "[16:32:46] [42]\ttraining-aucpr:0.15475\n",
      "[16:32:46] [43]\ttraining-aucpr:0.15605\n",
      "[16:32:46] [44]\ttraining-aucpr:0.15638\n",
      "[16:32:46] [45]\ttraining-aucpr:0.15714\n",
      "[16:32:46] [46]\ttraining-aucpr:0.15805\n",
      "[16:32:46] [47]\ttraining-aucpr:0.15916\n",
      "[16:32:46] [48]\ttraining-aucpr:0.16008\n",
      "[16:32:47] [49]\ttraining-aucpr:0.16100\n",
      "[16:32:47] [50]\ttraining-aucpr:0.16132\n",
      "[16:32:47] [51]\ttraining-aucpr:0.16171\n",
      "[16:32:47] [52]\ttraining-aucpr:0.16295\n",
      "[16:32:47] [53]\ttraining-aucpr:0.16354\n",
      "[16:32:47] [54]\ttraining-aucpr:0.16432\n",
      "[16:32:47] [55]\ttraining-aucpr:0.16514\n",
      "[16:32:47] [56]\ttraining-aucpr:0.16543\n",
      "[16:32:47] [57]\ttraining-aucpr:0.16621\n",
      "[16:32:47] [58]\ttraining-aucpr:0.16720\n",
      "[16:32:47] [59]\ttraining-aucpr:0.16767\n",
      "[16:32:47] [60]\ttraining-aucpr:0.16832\n",
      "[16:32:47] [61]\ttraining-aucpr:0.16894\n",
      "[16:32:47] [62]\ttraining-aucpr:0.17015\n",
      "[16:32:47] [63]\ttraining-aucpr:0.17062\n",
      "[16:32:47] [64]\ttraining-aucpr:0.17103\n",
      "[16:32:47] [65]\ttraining-aucpr:0.17116\n",
      "[16:32:47] [66]\ttraining-aucpr:0.17179\n",
      "[16:32:47] [67]\ttraining-aucpr:0.17231\n",
      "[16:32:47] [68]\ttraining-aucpr:0.17279\n",
      "[16:32:47] [69]\ttraining-aucpr:0.17316\n",
      "[16:32:47] [70]\ttraining-aucpr:0.17408\n",
      "[16:32:47] [71]\ttraining-aucpr:0.17477\n",
      "[16:32:47] [72]\ttraining-aucpr:0.17517\n",
      "[16:32:47] [73]\ttraining-aucpr:0.17528\n",
      "2025-07-28 16:32:48,750 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:32:48,840 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:32:49,488] Trial 38 finished with value: 0.18461437348519233 and parameters: {'max_depth': 5, 'learning_rate': 0.09441602758331123, 'n_estimators': 74, 'min_child_weight': 3, 'gamma': 0.6997987848399297, 'subsample': 0.8327965435862219, 'colsample_bytree': 0.9558282502554007, 'reg_alpha': 0.11780405100113238, 'reg_lambda': 0.0002774603834067753}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:32:49,598 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.67011205136076, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.37672782146918204, 'learning_rate': 0.23053085935174336, 'max_depth': 3, 'min_child_weight': 6, 'reg_alpha': 0.541100165326118, 'reg_lambda': 0.2238614666491252, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.6285490700133742, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 134}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:32:50,985 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:32:52] Task 0 got rank 0\n",
      "[16:32:52] Task 1 got rank 1\n",
      "[16:32:52] Task 3 got rank 3\n",
      "[16:32:52] Task 2 got rank 2\n",
      "[16:32:52] [0]\ttraining-aucpr:0.04732\n",
      "[16:32:52] [1]\ttraining-aucpr:0.04839\n",
      "[16:32:52] [2]\ttraining-aucpr:0.05904\n",
      "[16:32:52] [3]\ttraining-aucpr:0.06431\n",
      "[16:32:52] [4]\ttraining-aucpr:0.06788\n",
      "[16:32:52] [5]\ttraining-aucpr:0.07427\n",
      "[16:32:52] [6]\ttraining-aucpr:0.08439\n",
      "[16:32:52] [7]\ttraining-aucpr:0.09198\n",
      "[16:32:52] [8]\ttraining-aucpr:0.09796\n",
      "[16:32:52] [9]\ttraining-aucpr:0.10579\n",
      "[16:32:52] [10]\ttraining-aucpr:0.11003\n",
      "[16:32:52] [11]\ttraining-aucpr:0.11137\n",
      "[16:32:52] [12]\ttraining-aucpr:0.11773\n",
      "[16:32:52] [13]\ttraining-aucpr:0.11996\n",
      "[16:32:52] [14]\ttraining-aucpr:0.12362\n",
      "[16:32:52] [15]\ttraining-aucpr:0.12230\n",
      "[16:32:52] [16]\ttraining-aucpr:0.12330\n",
      "[16:32:52] [17]\ttraining-aucpr:0.12648\n",
      "[16:32:52] [18]\ttraining-aucpr:0.12894\n",
      "[16:32:52] [19]\ttraining-aucpr:0.13034\n",
      "[16:32:52] [20]\ttraining-aucpr:0.13329\n",
      "[16:32:53] [21]\ttraining-aucpr:0.13345\n",
      "[16:32:53] [22]\ttraining-aucpr:0.13580\n",
      "[16:32:53] [23]\ttraining-aucpr:0.13721\n",
      "[16:32:53] [24]\ttraining-aucpr:0.13882\n",
      "[16:32:53] [25]\ttraining-aucpr:0.14066\n",
      "[16:32:53] [26]\ttraining-aucpr:0.14142\n",
      "[16:32:53] [27]\ttraining-aucpr:0.14263\n",
      "[16:32:53] [28]\ttraining-aucpr:0.14381\n",
      "[16:32:53] [29]\ttraining-aucpr:0.14553\n",
      "[16:32:53] [30]\ttraining-aucpr:0.14734\n",
      "[16:32:53] [31]\ttraining-aucpr:0.14788\n",
      "[16:32:53] [32]\ttraining-aucpr:0.14842\n",
      "[16:32:53] [33]\ttraining-aucpr:0.14961\n",
      "[16:32:53] [34]\ttraining-aucpr:0.15030\n",
      "[16:32:53] [35]\ttraining-aucpr:0.15134\n",
      "[16:32:53] [36]\ttraining-aucpr:0.15227\n",
      "[16:32:53] [37]\ttraining-aucpr:0.15374\n",
      "[16:32:53] [38]\ttraining-aucpr:0.15397\n",
      "[16:32:53] [39]\ttraining-aucpr:0.15484\n",
      "[16:32:53] [40]\ttraining-aucpr:0.15511\n",
      "[16:32:53] [41]\ttraining-aucpr:0.15569\n",
      "[16:32:53] [42]\ttraining-aucpr:0.15573\n",
      "[16:32:53] [43]\ttraining-aucpr:0.15666\n",
      "[16:32:53] [44]\ttraining-aucpr:0.15659\n",
      "[16:32:53] [45]\ttraining-aucpr:0.15718\n",
      "[16:32:53] [46]\ttraining-aucpr:0.15769\n",
      "[16:32:53] [47]\ttraining-aucpr:0.15835\n",
      "[16:32:53] [48]\ttraining-aucpr:0.15892\n",
      "[16:32:53] [49]\ttraining-aucpr:0.15917\n",
      "[16:32:53] [50]\ttraining-aucpr:0.15996\n",
      "[16:32:53] [51]\ttraining-aucpr:0.16031\n",
      "[16:32:53] [52]\ttraining-aucpr:0.16056\n",
      "[16:32:53] [53]\ttraining-aucpr:0.16056\n",
      "[16:32:53] [54]\ttraining-aucpr:0.16088\n",
      "[16:32:53] [55]\ttraining-aucpr:0.16106\n",
      "[16:32:53] [56]\ttraining-aucpr:0.16127\n",
      "[16:32:53] [57]\ttraining-aucpr:0.16153\n",
      "[16:32:53] [58]\ttraining-aucpr:0.16197\n",
      "[16:32:53] [59]\ttraining-aucpr:0.16241\n",
      "[16:32:53] [60]\ttraining-aucpr:0.16242\n",
      "[16:32:53] [61]\ttraining-aucpr:0.16235\n",
      "[16:32:53] [62]\ttraining-aucpr:0.16279\n",
      "[16:32:54] [63]\ttraining-aucpr:0.16336\n",
      "[16:32:54] [64]\ttraining-aucpr:0.16398\n",
      "[16:32:54] [65]\ttraining-aucpr:0.16420\n",
      "[16:32:54] [66]\ttraining-aucpr:0.16442\n",
      "[16:32:54] [67]\ttraining-aucpr:0.16475\n",
      "[16:32:54] [68]\ttraining-aucpr:0.16509\n",
      "[16:32:54] [69]\ttraining-aucpr:0.16538\n",
      "[16:32:54] [70]\ttraining-aucpr:0.16559\n",
      "[16:32:54] [71]\ttraining-aucpr:0.16559\n",
      "[16:32:54] [72]\ttraining-aucpr:0.16604\n",
      "[16:32:54] [73]\ttraining-aucpr:0.16603\n",
      "[16:32:54] [74]\ttraining-aucpr:0.16653\n",
      "[16:32:54] [75]\ttraining-aucpr:0.16682\n",
      "[16:32:54] [76]\ttraining-aucpr:0.16732\n",
      "[16:32:54] [77]\ttraining-aucpr:0.16780\n",
      "[16:32:54] [78]\ttraining-aucpr:0.16808\n",
      "[16:32:54] [79]\ttraining-aucpr:0.16823\n",
      "[16:32:54] [80]\ttraining-aucpr:0.16858\n",
      "[16:32:54] [81]\ttraining-aucpr:0.16884\n",
      "[16:32:54] [82]\ttraining-aucpr:0.16910\n",
      "[16:32:54] [83]\ttraining-aucpr:0.16966\n",
      "[16:32:54] [84]\ttraining-aucpr:0.16976\n",
      "[16:32:54] [85]\ttraining-aucpr:0.16999\n",
      "[16:32:54] [86]\ttraining-aucpr:0.17008\n",
      "[16:32:54] [87]\ttraining-aucpr:0.17021\n",
      "[16:32:54] [88]\ttraining-aucpr:0.17033\n",
      "[16:32:54] [89]\ttraining-aucpr:0.17042\n",
      "[16:32:54] [90]\ttraining-aucpr:0.17045\n",
      "[16:32:54] [91]\ttraining-aucpr:0.17061\n",
      "[16:32:54] [92]\ttraining-aucpr:0.17081\n",
      "[16:32:54] [93]\ttraining-aucpr:0.17098\n",
      "[16:32:54] [94]\ttraining-aucpr:0.17113\n",
      "[16:32:54] [95]\ttraining-aucpr:0.17119\n",
      "[16:32:54] [96]\ttraining-aucpr:0.17138\n",
      "[16:32:54] [97]\ttraining-aucpr:0.17145\n",
      "[16:32:54] [98]\ttraining-aucpr:0.17158\n",
      "[16:32:54] [99]\ttraining-aucpr:0.17154\n",
      "[16:32:54] [100]\ttraining-aucpr:0.17171\n",
      "[16:32:54] [101]\ttraining-aucpr:0.17188\n",
      "[16:32:54] [102]\ttraining-aucpr:0.17213\n",
      "[16:32:54] [103]\ttraining-aucpr:0.17206\n",
      "[16:32:54] [104]\ttraining-aucpr:0.17217\n",
      "[16:32:54] [105]\ttraining-aucpr:0.17232\n",
      "[16:32:55] [106]\ttraining-aucpr:0.17254\n",
      "[16:32:55] [107]\ttraining-aucpr:0.17247\n",
      "[16:32:55] [108]\ttraining-aucpr:0.17252\n",
      "[16:32:55] [109]\ttraining-aucpr:0.17270\n",
      "[16:32:55] [110]\ttraining-aucpr:0.17267\n",
      "[16:32:55] [111]\ttraining-aucpr:0.17287\n",
      "[16:32:55] [112]\ttraining-aucpr:0.17296\n",
      "[16:32:55] [113]\ttraining-aucpr:0.17307\n",
      "[16:32:55] [114]\ttraining-aucpr:0.17310\n",
      "[16:32:55] [115]\ttraining-aucpr:0.17321\n",
      "[16:32:55] [116]\ttraining-aucpr:0.17322\n",
      "[16:32:55] [117]\ttraining-aucpr:0.17333\n",
      "[16:32:55] [118]\ttraining-aucpr:0.17346\n",
      "[16:32:55] [119]\ttraining-aucpr:0.17351\n",
      "[16:32:55] [120]\ttraining-aucpr:0.17358\n",
      "[16:32:55] [121]\ttraining-aucpr:0.17368\n",
      "[16:32:55] [122]\ttraining-aucpr:0.17382\n",
      "[16:32:55] [123]\ttraining-aucpr:0.17397\n",
      "[16:32:55] [124]\ttraining-aucpr:0.17405\n",
      "[16:32:55] [125]\ttraining-aucpr:0.17416\n",
      "[16:32:55] [126]\ttraining-aucpr:0.17427\n",
      "[16:32:55] [127]\ttraining-aucpr:0.17427\n",
      "[16:32:55] [128]\ttraining-aucpr:0.17427\n",
      "[16:32:55] [129]\ttraining-aucpr:0.17447\n",
      "[16:32:55] [130]\ttraining-aucpr:0.17472\n",
      "[16:32:55] [131]\ttraining-aucpr:0.17479\n",
      "[16:32:55] [132]\ttraining-aucpr:0.17475\n",
      "[16:32:55] [133]\ttraining-aucpr:0.17471\n",
      "2025-07-28 16:32:56,723 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:32:56,804 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:32:57,485] Trial 39 finished with value: 0.1922565363393217 and parameters: {'max_depth': 3, 'learning_rate': 0.23053085935174336, 'n_estimators': 134, 'min_child_weight': 6, 'gamma': 0.37672782146918204, 'subsample': 0.6285490700133742, 'colsample_bytree': 0.67011205136076, 'reg_alpha': 0.541100165326118, 'reg_lambda': 0.2238614666491252}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:32:57,594 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.7434306925008358, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.48188760711966794, 'learning_rate': 0.018755037802199732, 'max_depth': 8, 'min_child_weight': 5, 'reg_alpha': 0.3029149515495939, 'reg_lambda': 0.531975847356918, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.7175091218326057, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 242}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:32:58,971 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:33:00] Task 2 got rank 2[16:33:00] Task 1 got rank 1[16:33:00] Task 0 got rank 0\n",
      "\n",
      "\n",
      "[16:33:00] Task 3 got rank 3\n",
      "[16:33:00] [0]\ttraining-aucpr:0.08608\n",
      "[16:33:00] [1]\ttraining-aucpr:0.11016\n",
      "[16:33:00] [2]\ttraining-aucpr:0.11505\n",
      "[16:33:00] [3]\ttraining-aucpr:0.11874\n",
      "[16:33:00] [4]\ttraining-aucpr:0.11871\n",
      "[16:33:00] [5]\ttraining-aucpr:0.12193\n",
      "[16:33:00] [6]\ttraining-aucpr:0.12641\n",
      "[16:33:01] [7]\ttraining-aucpr:0.12705\n",
      "[16:33:01] [8]\ttraining-aucpr:0.12880\n",
      "[16:33:01] [9]\ttraining-aucpr:0.12967\n",
      "[16:33:01] [10]\ttraining-aucpr:0.13135\n",
      "[16:33:01] [11]\ttraining-aucpr:0.13112\n",
      "[16:33:01] [12]\ttraining-aucpr:0.13145\n",
      "[16:33:01] [13]\ttraining-aucpr:0.13557\n",
      "[16:33:01] [14]\ttraining-aucpr:0.13666\n",
      "[16:33:01] [15]\ttraining-aucpr:0.13627\n",
      "[16:33:01] [16]\ttraining-aucpr:0.13617\n",
      "[16:33:01] [17]\ttraining-aucpr:0.13794\n",
      "[16:33:01] [18]\ttraining-aucpr:0.13959\n",
      "[16:33:01] [19]\ttraining-aucpr:0.14083\n",
      "[16:33:01] [20]\ttraining-aucpr:0.14127\n",
      "[16:33:01] [21]\ttraining-aucpr:0.14149\n",
      "[16:33:01] [22]\ttraining-aucpr:0.14333\n",
      "[16:33:01] [23]\ttraining-aucpr:0.14322\n",
      "[16:33:02] [24]\ttraining-aucpr:0.14301\n",
      "[16:33:02] [25]\ttraining-aucpr:0.14308\n",
      "[16:33:02] [26]\ttraining-aucpr:0.14352\n",
      "[16:33:02] [27]\ttraining-aucpr:0.14412\n",
      "[16:33:02] [28]\ttraining-aucpr:0.14487\n",
      "[16:33:02] [29]\ttraining-aucpr:0.14566\n",
      "[16:33:02] [30]\ttraining-aucpr:0.14613\n",
      "[16:33:02] [31]\ttraining-aucpr:0.14583\n",
      "[16:33:02] [32]\ttraining-aucpr:0.14581\n",
      "[16:33:02] [33]\ttraining-aucpr:0.14632\n",
      "[16:33:02] [34]\ttraining-aucpr:0.14626\n",
      "[16:33:02] [35]\ttraining-aucpr:0.14682\n",
      "[16:33:02] [36]\ttraining-aucpr:0.14770\n",
      "[16:33:02] [37]\ttraining-aucpr:0.14953\n",
      "[16:33:02] [38]\ttraining-aucpr:0.15126\n",
      "[16:33:02] [39]\ttraining-aucpr:0.15230\n",
      "[16:33:02] [40]\ttraining-aucpr:0.15359\n",
      "[16:33:02] [41]\ttraining-aucpr:0.15365\n",
      "[16:33:02] [42]\ttraining-aucpr:0.15445\n",
      "[16:33:02] [43]\ttraining-aucpr:0.15472\n",
      "[16:33:02] [44]\ttraining-aucpr:0.15536\n",
      "[16:33:03] [45]\ttraining-aucpr:0.15601\n",
      "[16:33:03] [46]\ttraining-aucpr:0.15580\n",
      "[16:33:03] [47]\ttraining-aucpr:0.15579\n",
      "[16:33:03] [48]\ttraining-aucpr:0.15613\n",
      "[16:33:03] [49]\ttraining-aucpr:0.15638\n",
      "[16:33:03] [50]\ttraining-aucpr:0.15706\n",
      "[16:33:03] [51]\ttraining-aucpr:0.15758\n",
      "[16:33:03] [52]\ttraining-aucpr:0.15819\n",
      "[16:33:03] [53]\ttraining-aucpr:0.15876\n",
      "[16:33:03] [54]\ttraining-aucpr:0.15948\n",
      "[16:33:03] [55]\ttraining-aucpr:0.15993\n",
      "[16:33:03] [56]\ttraining-aucpr:0.16043\n",
      "[16:33:03] [57]\ttraining-aucpr:0.16065\n",
      "[16:33:03] [58]\ttraining-aucpr:0.16131\n",
      "[16:33:03] [59]\ttraining-aucpr:0.16148\n",
      "[16:33:03] [60]\ttraining-aucpr:0.16177\n",
      "[16:33:03] [61]\ttraining-aucpr:0.16229\n",
      "[16:33:03] [62]\ttraining-aucpr:0.16275\n",
      "[16:33:03] [63]\ttraining-aucpr:0.16360\n",
      "[16:33:03] [64]\ttraining-aucpr:0.16400\n",
      "[16:33:03] [65]\ttraining-aucpr:0.16472\n",
      "[16:33:04] [66]\ttraining-aucpr:0.16582\n",
      "[16:33:04] [67]\ttraining-aucpr:0.16586\n",
      "[16:33:04] [68]\ttraining-aucpr:0.16622\n",
      "[16:33:04] [69]\ttraining-aucpr:0.16652\n",
      "[16:33:04] [70]\ttraining-aucpr:0.16662\n",
      "[16:33:04] [71]\ttraining-aucpr:0.16720\n",
      "[16:33:04] [72]\ttraining-aucpr:0.16794\n",
      "[16:33:04] [73]\ttraining-aucpr:0.16892\n",
      "[16:33:04] [74]\ttraining-aucpr:0.16953\n",
      "[16:33:04] [75]\ttraining-aucpr:0.17002\n",
      "[16:33:04] [76]\ttraining-aucpr:0.17043\n",
      "[16:33:04] [77]\ttraining-aucpr:0.17098\n",
      "[16:33:04] [78]\ttraining-aucpr:0.17156\n",
      "[16:33:04] [79]\ttraining-aucpr:0.17213\n",
      "[16:33:04] [80]\ttraining-aucpr:0.17277\n",
      "[16:33:04] [81]\ttraining-aucpr:0.17328\n",
      "[16:33:04] [82]\ttraining-aucpr:0.17371\n",
      "[16:33:04] [83]\ttraining-aucpr:0.17427\n",
      "[16:33:04] [84]\ttraining-aucpr:0.17492\n",
      "[16:33:04] [85]\ttraining-aucpr:0.17526\n",
      "[16:33:04] [86]\ttraining-aucpr:0.17610\n",
      "[16:33:04] [87]\ttraining-aucpr:0.17735\n",
      "[16:33:05] [88]\ttraining-aucpr:0.17788\n",
      "[16:33:05] [89]\ttraining-aucpr:0.17873\n",
      "[16:33:05] [90]\ttraining-aucpr:0.17940\n",
      "[16:33:05] [91]\ttraining-aucpr:0.17969\n",
      "[16:33:05] [92]\ttraining-aucpr:0.18050\n",
      "[16:33:05] [93]\ttraining-aucpr:0.18079\n",
      "[16:33:05] [94]\ttraining-aucpr:0.18136\n",
      "[16:33:05] [95]\ttraining-aucpr:0.18167\n",
      "[16:33:05] [96]\ttraining-aucpr:0.18292\n",
      "[16:33:05] [97]\ttraining-aucpr:0.18335\n",
      "[16:33:05] [98]\ttraining-aucpr:0.18371\n",
      "[16:33:05] [99]\ttraining-aucpr:0.18451\n",
      "[16:33:05] [100]\ttraining-aucpr:0.18508\n",
      "[16:33:05] [101]\ttraining-aucpr:0.18552\n",
      "[16:33:05] [102]\ttraining-aucpr:0.18572\n",
      "[16:33:05] [103]\ttraining-aucpr:0.18638\n",
      "[16:33:05] [104]\ttraining-aucpr:0.18676\n",
      "[16:33:05] [105]\ttraining-aucpr:0.18729\n",
      "[16:33:05] [106]\ttraining-aucpr:0.18798\n",
      "[16:33:05] [107]\ttraining-aucpr:0.18838\n",
      "[16:33:06] [108]\ttraining-aucpr:0.18918\n",
      "[16:33:06] [109]\ttraining-aucpr:0.18960\n",
      "[16:33:06] [110]\ttraining-aucpr:0.19012\n",
      "[16:33:06] [111]\ttraining-aucpr:0.19049\n",
      "[16:33:06] [112]\ttraining-aucpr:0.19109\n",
      "[16:33:06] [113]\ttraining-aucpr:0.19143\n",
      "[16:33:06] [114]\ttraining-aucpr:0.19205\n",
      "[16:33:06] [115]\ttraining-aucpr:0.19242\n",
      "[16:33:06] [116]\ttraining-aucpr:0.19318\n",
      "[16:33:06] [117]\ttraining-aucpr:0.19337\n",
      "[16:33:06] [118]\ttraining-aucpr:0.19376\n",
      "[16:33:06] [119]\ttraining-aucpr:0.19419\n",
      "[16:33:06] [120]\ttraining-aucpr:0.19491\n",
      "[16:33:06] [121]\ttraining-aucpr:0.19574\n",
      "[16:33:06] [122]\ttraining-aucpr:0.19631\n",
      "[16:33:06] [123]\ttraining-aucpr:0.19682\n",
      "[16:33:06] [124]\ttraining-aucpr:0.19765\n",
      "[16:33:06] [125]\ttraining-aucpr:0.19851\n",
      "[16:33:06] [126]\ttraining-aucpr:0.19941\n",
      "[16:33:06] [127]\ttraining-aucpr:0.20004\n",
      "[16:33:07] [128]\ttraining-aucpr:0.20038\n",
      "[16:33:07] [129]\ttraining-aucpr:0.20108\n",
      "[16:33:07] [130]\ttraining-aucpr:0.20150\n",
      "[16:33:07] [131]\ttraining-aucpr:0.20189\n",
      "[16:33:07] [132]\ttraining-aucpr:0.20236\n",
      "[16:33:07] [133]\ttraining-aucpr:0.20256\n",
      "[16:33:07] [134]\ttraining-aucpr:0.20309\n",
      "[16:33:07] [135]\ttraining-aucpr:0.20331\n",
      "[16:33:07] [136]\ttraining-aucpr:0.20351\n",
      "[16:33:07] [137]\ttraining-aucpr:0.20426\n",
      "[16:33:07] [138]\ttraining-aucpr:0.20507\n",
      "[16:33:07] [139]\ttraining-aucpr:0.20554\n",
      "[16:33:07] [140]\ttraining-aucpr:0.20606\n",
      "[16:33:07] [141]\ttraining-aucpr:0.20651\n",
      "[16:33:07] [142]\ttraining-aucpr:0.20707\n",
      "[16:33:07] [143]\ttraining-aucpr:0.20758\n",
      "[16:33:07] [144]\ttraining-aucpr:0.20820\n",
      "[16:33:07] [145]\ttraining-aucpr:0.20853\n",
      "[16:33:07] [146]\ttraining-aucpr:0.20889\n",
      "[16:33:07] [147]\ttraining-aucpr:0.20947\n",
      "[16:33:07] [148]\ttraining-aucpr:0.20956\n",
      "[16:33:08] [149]\ttraining-aucpr:0.20996\n",
      "[16:33:08] [150]\ttraining-aucpr:0.21060\n",
      "[16:33:08] [151]\ttraining-aucpr:0.21107\n",
      "[16:33:08] [152]\ttraining-aucpr:0.21140\n",
      "[16:33:08] [153]\ttraining-aucpr:0.21182\n",
      "[16:33:08] [154]\ttraining-aucpr:0.21243\n",
      "[16:33:08] [155]\ttraining-aucpr:0.21319\n",
      "[16:33:08] [156]\ttraining-aucpr:0.21380\n",
      "[16:33:08] [157]\ttraining-aucpr:0.21421\n",
      "[16:33:08] [158]\ttraining-aucpr:0.21473\n",
      "[16:33:08] [159]\ttraining-aucpr:0.21516\n",
      "[16:33:08] [160]\ttraining-aucpr:0.21593\n",
      "[16:33:08] [161]\ttraining-aucpr:0.21636\n",
      "[16:33:08] [162]\ttraining-aucpr:0.21691\n",
      "[16:33:08] [163]\ttraining-aucpr:0.21710\n",
      "[16:33:08] [164]\ttraining-aucpr:0.21727\n",
      "[16:33:08] [165]\ttraining-aucpr:0.21795\n",
      "[16:33:08] [166]\ttraining-aucpr:0.21835\n",
      "[16:33:08] [167]\ttraining-aucpr:0.21900\n",
      "[16:33:08] [168]\ttraining-aucpr:0.21934\n",
      "[16:33:08] [169]\ttraining-aucpr:0.22011\n",
      "[16:33:09] [170]\ttraining-aucpr:0.22045\n",
      "[16:33:09] [171]\ttraining-aucpr:0.22092\n",
      "[16:33:09] [172]\ttraining-aucpr:0.22124\n",
      "[16:33:09] [173]\ttraining-aucpr:0.22214\n",
      "[16:33:09] [174]\ttraining-aucpr:0.22251\n",
      "[16:33:09] [175]\ttraining-aucpr:0.22273\n",
      "[16:33:09] [176]\ttraining-aucpr:0.22351\n",
      "[16:33:09] [177]\ttraining-aucpr:0.22400\n",
      "[16:33:09] [178]\ttraining-aucpr:0.22476\n",
      "[16:33:09] [179]\ttraining-aucpr:0.22517\n",
      "[16:33:09] [180]\ttraining-aucpr:0.22575\n",
      "[16:33:09] [181]\ttraining-aucpr:0.22618\n",
      "[16:33:09] [182]\ttraining-aucpr:0.22652\n",
      "[16:33:09] [183]\ttraining-aucpr:0.22687\n",
      "[16:33:09] [184]\ttraining-aucpr:0.22720\n",
      "[16:33:09] [185]\ttraining-aucpr:0.22754\n",
      "[16:33:09] [186]\ttraining-aucpr:0.22808\n",
      "[16:33:09] [187]\ttraining-aucpr:0.22846\n",
      "[16:33:09] [188]\ttraining-aucpr:0.22874\n",
      "[16:33:09] [189]\ttraining-aucpr:0.22902\n",
      "[16:33:09] [190]\ttraining-aucpr:0.22939\n",
      "[16:33:09] [191]\ttraining-aucpr:0.22969\n",
      "[16:33:10] [192]\ttraining-aucpr:0.23028\n",
      "[16:33:10] [193]\ttraining-aucpr:0.23056\n",
      "[16:33:10] [194]\ttraining-aucpr:0.23107\n",
      "[16:33:10] [195]\ttraining-aucpr:0.23162\n",
      "[16:33:10] [196]\ttraining-aucpr:0.23205\n",
      "[16:33:10] [197]\ttraining-aucpr:0.23243\n",
      "[16:33:10] [198]\ttraining-aucpr:0.23272\n",
      "[16:33:10] [199]\ttraining-aucpr:0.23319\n",
      "[16:33:10] [200]\ttraining-aucpr:0.23358\n",
      "[16:33:10] [201]\ttraining-aucpr:0.23410\n",
      "[16:33:10] [202]\ttraining-aucpr:0.23451\n",
      "[16:33:10] [203]\ttraining-aucpr:0.23469\n",
      "[16:33:10] [204]\ttraining-aucpr:0.23489\n",
      "[16:33:10] [205]\ttraining-aucpr:0.23560\n",
      "[16:33:10] [206]\ttraining-aucpr:0.23591\n",
      "[16:33:10] [207]\ttraining-aucpr:0.23613\n",
      "[16:33:10] [208]\ttraining-aucpr:0.23654\n",
      "[16:33:10] [209]\ttraining-aucpr:0.23690\n",
      "[16:33:10] [210]\ttraining-aucpr:0.23711\n",
      "[16:33:10] [211]\ttraining-aucpr:0.23742\n",
      "[16:33:10] [212]\ttraining-aucpr:0.23778\n",
      "[16:33:10] [213]\ttraining-aucpr:0.23827\n",
      "[16:33:10] [214]\ttraining-aucpr:0.23869\n",
      "[16:33:10] [215]\ttraining-aucpr:0.23881\n",
      "[16:33:11] [216]\ttraining-aucpr:0.23926\n",
      "[16:33:11] [217]\ttraining-aucpr:0.23973\n",
      "[16:33:11] [218]\ttraining-aucpr:0.24005\n",
      "[16:33:11] [219]\ttraining-aucpr:0.24025\n",
      "[16:33:11] [220]\ttraining-aucpr:0.24062\n",
      "[16:33:11] [221]\ttraining-aucpr:0.24101\n",
      "[16:33:11] [222]\ttraining-aucpr:0.24113\n",
      "[16:33:11] [223]\ttraining-aucpr:0.24157\n",
      "[16:33:11] [224]\ttraining-aucpr:0.24187\n",
      "[16:33:11] [225]\ttraining-aucpr:0.24245\n",
      "[16:33:11] [226]\ttraining-aucpr:0.24283\n",
      "[16:33:11] [227]\ttraining-aucpr:0.24303\n",
      "[16:33:11] [228]\ttraining-aucpr:0.24332\n",
      "[16:33:11] [229]\ttraining-aucpr:0.24376\n",
      "[16:33:11] [230]\ttraining-aucpr:0.24427\n",
      "[16:33:11] [231]\ttraining-aucpr:0.24461\n",
      "[16:33:11] [232]\ttraining-aucpr:0.24479\n",
      "[16:33:11] [233]\ttraining-aucpr:0.24504\n",
      "[16:33:11] [234]\ttraining-aucpr:0.24520\n",
      "[16:33:11] [235]\ttraining-aucpr:0.24546\n",
      "[16:33:11] [236]\ttraining-aucpr:0.24579\n",
      "[16:33:11] [237]\ttraining-aucpr:0.24614\n",
      "[16:33:11] [238]\ttraining-aucpr:0.24646\n",
      "[16:33:12] [239]\ttraining-aucpr:0.24687\n",
      "[16:33:12] [240]\ttraining-aucpr:0.24728\n",
      "[16:33:12] [241]\ttraining-aucpr:0.24749\n",
      "2025-07-28 16:33:13,169 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:33:13,333 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:33:14,026] Trial 40 finished with value: 0.16664145093055904 and parameters: {'max_depth': 8, 'learning_rate': 0.018755037802199732, 'n_estimators': 242, 'min_child_weight': 5, 'gamma': 0.48188760711966794, 'subsample': 0.7175091218326057, 'colsample_bytree': 0.7434306925008358, 'reg_alpha': 0.3029149515495939, 'reg_lambda': 0.531975847356918}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:33:14,150 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.7787350608319897, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.5544571566351447, 'learning_rate': 0.17059076661753692, 'max_depth': 4, 'min_child_weight': 8, 'reg_alpha': 0.47036668648662666, 'reg_lambda': 0.056742932764326466, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9911469474809335, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 105}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:33:15,544 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:33:16] Task 1 got rank 1[16:33:16] Task 0 got rank 0\n",
      "\n",
      "[16:33:16] Task 2 got rank 2\n",
      "[16:33:16] Task 3 got rank 3\n",
      "[16:33:17] [0]\ttraining-aucpr:0.05563\n",
      "[16:33:17] [1]\ttraining-aucpr:0.06056\n",
      "[16:33:17] [2]\ttraining-aucpr:0.06871\n",
      "[16:33:17] [3]\ttraining-aucpr:0.07620\n",
      "[16:33:17] [4]\ttraining-aucpr:0.07767\n",
      "[16:33:17] [5]\ttraining-aucpr:0.08467\n",
      "[16:33:17] [6]\ttraining-aucpr:0.09211\n",
      "[16:33:17] [7]\ttraining-aucpr:0.09861\n",
      "[16:33:17] [8]\ttraining-aucpr:0.10139\n",
      "[16:33:17] [9]\ttraining-aucpr:0.10732\n",
      "[16:33:17] [10]\ttraining-aucpr:0.10921\n",
      "[16:33:17] [11]\ttraining-aucpr:0.11673\n",
      "[16:33:17] [12]\ttraining-aucpr:0.11870\n",
      "[16:33:17] [13]\ttraining-aucpr:0.12240\n",
      "[16:33:17] [14]\ttraining-aucpr:0.12485\n",
      "[16:33:17] [15]\ttraining-aucpr:0.12905\n",
      "[16:33:17] [16]\ttraining-aucpr:0.13119\n",
      "[16:33:17] [17]\ttraining-aucpr:0.13377\n",
      "[16:33:17] [18]\ttraining-aucpr:0.13747\n",
      "[16:33:17] [19]\ttraining-aucpr:0.13795\n",
      "[16:33:17] [20]\ttraining-aucpr:0.13966\n",
      "[16:33:17] [21]\ttraining-aucpr:0.14042\n",
      "[16:33:17] [22]\ttraining-aucpr:0.14161\n",
      "[16:33:17] [23]\ttraining-aucpr:0.14168\n",
      "[16:33:17] [24]\ttraining-aucpr:0.14203\n",
      "[16:33:17] [25]\ttraining-aucpr:0.14488\n",
      "[16:33:17] [26]\ttraining-aucpr:0.14731\n",
      "[16:33:17] [27]\ttraining-aucpr:0.14884\n",
      "[16:33:17] [28]\ttraining-aucpr:0.14920\n",
      "[16:33:17] [29]\ttraining-aucpr:0.15082\n",
      "[16:33:17] [30]\ttraining-aucpr:0.15191\n",
      "[16:33:17] [31]\ttraining-aucpr:0.15295\n",
      "[16:33:17] [32]\ttraining-aucpr:0.15356\n",
      "[16:33:17] [33]\ttraining-aucpr:0.15443\n",
      "[16:33:17] [34]\ttraining-aucpr:0.15440\n",
      "[16:33:17] [35]\ttraining-aucpr:0.15545\n",
      "[16:33:17] [36]\ttraining-aucpr:0.15670\n",
      "[16:33:18] [37]\ttraining-aucpr:0.15814\n",
      "[16:33:18] [38]\ttraining-aucpr:0.15855\n",
      "[16:33:18] [39]\ttraining-aucpr:0.15902\n",
      "[16:33:18] [40]\ttraining-aucpr:0.15909\n",
      "[16:33:18] [41]\ttraining-aucpr:0.15986\n",
      "[16:33:18] [42]\ttraining-aucpr:0.16050\n",
      "[16:33:18] [43]\ttraining-aucpr:0.16198\n",
      "[16:33:18] [44]\ttraining-aucpr:0.16312\n",
      "[16:33:18] [45]\ttraining-aucpr:0.16320\n",
      "[16:33:18] [46]\ttraining-aucpr:0.16378\n",
      "[16:33:18] [47]\ttraining-aucpr:0.16415\n",
      "[16:33:18] [48]\ttraining-aucpr:0.16456\n",
      "[16:33:18] [49]\ttraining-aucpr:0.16532\n",
      "[16:33:18] [50]\ttraining-aucpr:0.16583\n",
      "[16:33:18] [51]\ttraining-aucpr:0.16638\n",
      "[16:33:18] [52]\ttraining-aucpr:0.16692\n",
      "[16:33:18] [53]\ttraining-aucpr:0.16745\n",
      "[16:33:18] [54]\ttraining-aucpr:0.16782\n",
      "[16:33:18] [55]\ttraining-aucpr:0.16843\n",
      "[16:33:18] [56]\ttraining-aucpr:0.16932\n",
      "[16:33:18] [57]\ttraining-aucpr:0.16938\n",
      "[16:33:18] [58]\ttraining-aucpr:0.16998\n",
      "[16:33:18] [59]\ttraining-aucpr:0.17029\n",
      "[16:33:18] [60]\ttraining-aucpr:0.17084\n",
      "[16:33:18] [61]\ttraining-aucpr:0.17108\n",
      "[16:33:18] [62]\ttraining-aucpr:0.17138\n",
      "[16:33:18] [63]\ttraining-aucpr:0.17178\n",
      "[16:33:18] [64]\ttraining-aucpr:0.17217\n",
      "[16:33:18] [65]\ttraining-aucpr:0.17233\n",
      "[16:33:18] [66]\ttraining-aucpr:0.17266\n",
      "[16:33:18] [67]\ttraining-aucpr:0.17276\n",
      "[16:33:18] [68]\ttraining-aucpr:0.17309\n",
      "[16:33:18] [69]\ttraining-aucpr:0.17328\n",
      "[16:33:18] [70]\ttraining-aucpr:0.17391\n",
      "[16:33:18] [71]\ttraining-aucpr:0.17411\n",
      "[16:33:18] [72]\ttraining-aucpr:0.17413\n",
      "[16:33:18] [73]\ttraining-aucpr:0.17465\n",
      "[16:33:19] [74]\ttraining-aucpr:0.17490\n",
      "[16:33:19] [75]\ttraining-aucpr:0.17518\n",
      "[16:33:19] [76]\ttraining-aucpr:0.17534\n",
      "[16:33:19] [77]\ttraining-aucpr:0.17579\n",
      "[16:33:19] [78]\ttraining-aucpr:0.17636\n",
      "[16:33:19] [79]\ttraining-aucpr:0.17660\n",
      "[16:33:19] [80]\ttraining-aucpr:0.17666\n",
      "[16:33:19] [81]\ttraining-aucpr:0.17697\n",
      "[16:33:19] [82]\ttraining-aucpr:0.17740\n",
      "[16:33:19] [83]\ttraining-aucpr:0.17747\n",
      "[16:33:19] [84]\ttraining-aucpr:0.17757\n",
      "[16:33:19] [85]\ttraining-aucpr:0.17786\n",
      "[16:33:19] [86]\ttraining-aucpr:0.17810\n",
      "[16:33:19] [87]\ttraining-aucpr:0.17827\n",
      "[16:33:19] [88]\ttraining-aucpr:0.17833\n",
      "[16:33:19] [89]\ttraining-aucpr:0.17863\n",
      "[16:33:19] [90]\ttraining-aucpr:0.17880\n",
      "[16:33:19] [91]\ttraining-aucpr:0.17893\n",
      "[16:33:19] [92]\ttraining-aucpr:0.17911\n",
      "[16:33:19] [93]\ttraining-aucpr:0.17936\n",
      "[16:33:19] [94]\ttraining-aucpr:0.17954\n",
      "[16:33:19] [95]\ttraining-aucpr:0.17987\n",
      "[16:33:19] [96]\ttraining-aucpr:0.17999\n",
      "[16:33:19] [97]\ttraining-aucpr:0.18009\n",
      "[16:33:19] [98]\ttraining-aucpr:0.18017\n",
      "[16:33:19] [99]\ttraining-aucpr:0.18043\n",
      "[16:33:19] [100]\ttraining-aucpr:0.18048\n",
      "[16:33:19] [101]\ttraining-aucpr:0.18055\n",
      "[16:33:19] [102]\ttraining-aucpr:0.18062\n",
      "[16:33:19] [103]\ttraining-aucpr:0.18071\n",
      "[16:33:19] [104]\ttraining-aucpr:0.18090\n",
      "2025-07-28 16:33:20,777 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:33:20,861 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:33:21,551] Trial 41 finished with value: 0.18929853122491416 and parameters: {'max_depth': 4, 'learning_rate': 0.17059076661753692, 'n_estimators': 105, 'min_child_weight': 8, 'gamma': 0.5544571566351447, 'subsample': 0.9911469474809335, 'colsample_bytree': 0.7787350608319897, 'reg_alpha': 0.47036668648662666, 'reg_lambda': 0.056742932764326466}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:33:21,660 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.8312667484055034, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.6220629376470241, 'learning_rate': 0.15182143310637938, 'max_depth': 4, 'min_child_weight': 7, 'reg_alpha': 0.39077215905762225, 'reg_lambda': 0.08710573861898983, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9928033717798794, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 88}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:33:23,040 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:33:24] Task 1 got rank 1\n",
      "[16:33:24] Task 2 got rank 2\n",
      "[16:33:24] Task 3 got rank 3\n",
      "[16:33:24] Task 0 got rank 0\n",
      "[16:33:24] [0]\ttraining-aucpr:0.05563\n",
      "[16:33:24] [1]\ttraining-aucpr:0.06056\n",
      "[16:33:24] [2]\ttraining-aucpr:0.06191\n",
      "[16:33:24] [3]\ttraining-aucpr:0.07026\n",
      "[16:33:24] [4]\ttraining-aucpr:0.07941\n",
      "[16:33:24] [5]\ttraining-aucpr:0.08093\n",
      "[16:33:24] [6]\ttraining-aucpr:0.08117\n",
      "[16:33:24] [7]\ttraining-aucpr:0.09104\n",
      "[16:33:24] [8]\ttraining-aucpr:0.09994\n",
      "[16:33:24] [9]\ttraining-aucpr:0.10212\n",
      "[16:33:24] [10]\ttraining-aucpr:0.10711\n",
      "[16:33:24] [11]\ttraining-aucpr:0.11414\n",
      "[16:33:24] [12]\ttraining-aucpr:0.11828\n",
      "[16:33:24] [13]\ttraining-aucpr:0.12019\n",
      "[16:33:24] [14]\ttraining-aucpr:0.12006\n",
      "[16:33:24] [15]\ttraining-aucpr:0.12325\n",
      "[16:33:25] [16]\ttraining-aucpr:0.12624\n",
      "[16:33:25] [17]\ttraining-aucpr:0.12754\n",
      "[16:33:25] [18]\ttraining-aucpr:0.13027\n",
      "[16:33:25] [19]\ttraining-aucpr:0.13242\n",
      "[16:33:25] [20]\ttraining-aucpr:0.13440\n",
      "[16:33:25] [21]\ttraining-aucpr:0.13534\n",
      "[16:33:25] [22]\ttraining-aucpr:0.13708\n",
      "[16:33:25] [23]\ttraining-aucpr:0.13836\n",
      "[16:33:25] [24]\ttraining-aucpr:0.13983\n",
      "[16:33:25] [25]\ttraining-aucpr:0.13995\n",
      "[16:33:25] [26]\ttraining-aucpr:0.14144\n",
      "[16:33:25] [27]\ttraining-aucpr:0.14374\n",
      "[16:33:25] [28]\ttraining-aucpr:0.14485\n",
      "[16:33:25] [29]\ttraining-aucpr:0.14614\n",
      "[16:33:25] [30]\ttraining-aucpr:0.14779\n",
      "[16:33:25] [31]\ttraining-aucpr:0.14877\n",
      "[16:33:25] [32]\ttraining-aucpr:0.15046\n",
      "[16:33:25] [33]\ttraining-aucpr:0.15181\n",
      "[16:33:25] [34]\ttraining-aucpr:0.15250\n",
      "[16:33:25] [35]\ttraining-aucpr:0.15373\n",
      "[16:33:25] [36]\ttraining-aucpr:0.15538\n",
      "[16:33:25] [37]\ttraining-aucpr:0.15588\n",
      "[16:33:25] [38]\ttraining-aucpr:0.15727\n",
      "[16:33:25] [39]\ttraining-aucpr:0.15759\n",
      "[16:33:25] [40]\ttraining-aucpr:0.15800\n",
      "[16:33:25] [41]\ttraining-aucpr:0.15907\n",
      "[16:33:25] [42]\ttraining-aucpr:0.15964\n",
      "[16:33:25] [43]\ttraining-aucpr:0.16068\n",
      "[16:33:25] [44]\ttraining-aucpr:0.16083\n",
      "[16:33:25] [45]\ttraining-aucpr:0.16163\n",
      "[16:33:25] [46]\ttraining-aucpr:0.16180\n",
      "[16:33:25] [47]\ttraining-aucpr:0.16273\n",
      "[16:33:25] [48]\ttraining-aucpr:0.16344\n",
      "[16:33:25] [49]\ttraining-aucpr:0.16419\n",
      "[16:33:25] [50]\ttraining-aucpr:0.16483\n",
      "[16:33:25] [51]\ttraining-aucpr:0.16546\n",
      "[16:33:26] [52]\ttraining-aucpr:0.16589\n",
      "[16:33:26] [53]\ttraining-aucpr:0.16620\n",
      "[16:33:26] [54]\ttraining-aucpr:0.16687\n",
      "[16:33:26] [55]\ttraining-aucpr:0.16760\n",
      "[16:33:26] [56]\ttraining-aucpr:0.16805\n",
      "[16:33:26] [57]\ttraining-aucpr:0.16862\n",
      "[16:33:26] [58]\ttraining-aucpr:0.16884\n",
      "[16:33:26] [59]\ttraining-aucpr:0.16941\n",
      "[16:33:26] [60]\ttraining-aucpr:0.16971\n",
      "[16:33:26] [61]\ttraining-aucpr:0.17039\n",
      "[16:33:26] [62]\ttraining-aucpr:0.17069\n",
      "[16:33:26] [63]\ttraining-aucpr:0.17152\n",
      "[16:33:26] [64]\ttraining-aucpr:0.17175\n",
      "[16:33:26] [65]\ttraining-aucpr:0.17215\n",
      "[16:33:26] [66]\ttraining-aucpr:0.17254\n",
      "[16:33:26] [67]\ttraining-aucpr:0.17259\n",
      "[16:33:26] [68]\ttraining-aucpr:0.17299\n",
      "[16:33:26] [69]\ttraining-aucpr:0.17336\n",
      "[16:33:26] [70]\ttraining-aucpr:0.17377\n",
      "[16:33:26] [71]\ttraining-aucpr:0.17397\n",
      "[16:33:26] [72]\ttraining-aucpr:0.17419\n",
      "[16:33:26] [73]\ttraining-aucpr:0.17460\n",
      "[16:33:26] [74]\ttraining-aucpr:0.17490\n",
      "[16:33:26] [75]\ttraining-aucpr:0.17494\n",
      "[16:33:26] [76]\ttraining-aucpr:0.17514\n",
      "[16:33:26] [77]\ttraining-aucpr:0.17522\n",
      "[16:33:26] [78]\ttraining-aucpr:0.17550\n",
      "[16:33:26] [79]\ttraining-aucpr:0.17598\n",
      "[16:33:26] [80]\ttraining-aucpr:0.17596\n",
      "[16:33:26] [81]\ttraining-aucpr:0.17625\n",
      "[16:33:26] [82]\ttraining-aucpr:0.17650\n",
      "[16:33:26] [83]\ttraining-aucpr:0.17663\n",
      "[16:33:26] [84]\ttraining-aucpr:0.17692\n",
      "[16:33:26] [85]\ttraining-aucpr:0.17718\n",
      "[16:33:27] [86]\ttraining-aucpr:0.17742\n",
      "[16:33:27] [87]\ttraining-aucpr:0.17775\n",
      "2025-07-28 16:33:28,050 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:33:28,130 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:33:28,846] Trial 42 finished with value: 0.19011831257032952 and parameters: {'max_depth': 4, 'learning_rate': 0.15182143310637938, 'n_estimators': 88, 'min_child_weight': 7, 'gamma': 0.6220629376470241, 'subsample': 0.9928033717798794, 'colsample_bytree': 0.8312667484055034, 'reg_alpha': 0.39077215905762225, 'reg_lambda': 0.08710573861898983}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:33:28,966 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.7846404458190241, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.23913615606426492, 'learning_rate': 0.1216365881913747, 'max_depth': 5, 'min_child_weight': 8, 'reg_alpha': 0.7182727622530461, 'reg_lambda': 0.051310376362638264, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9435680061785408, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 73}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:33:30,342 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:33:31] Task 0 got rank 0\n",
      "[16:33:31] Task 1 got rank 1\n",
      "[16:33:31] Task 3 got rank 3\n",
      "[16:33:31] Task 2 got rank 2\n",
      "[16:33:31] [0]\ttraining-aucpr:0.06234\n",
      "[16:33:31] [1]\ttraining-aucpr:0.07173\n",
      "[16:33:31] [2]\ttraining-aucpr:0.08303\n",
      "[16:33:32] [3]\ttraining-aucpr:0.09126\n",
      "[16:33:32] [4]\ttraining-aucpr:0.09714\n",
      "[16:33:32] [5]\ttraining-aucpr:0.10221\n",
      "[16:33:32] [6]\ttraining-aucpr:0.10448\n",
      "[16:33:32] [7]\ttraining-aucpr:0.10538\n",
      "[16:33:32] [8]\ttraining-aucpr:0.10575\n",
      "[16:33:32] [9]\ttraining-aucpr:0.11002\n",
      "[16:33:32] [10]\ttraining-aucpr:0.11171\n",
      "[16:33:32] [11]\ttraining-aucpr:0.11346\n",
      "[16:33:32] [12]\ttraining-aucpr:0.11733\n",
      "[16:33:32] [13]\ttraining-aucpr:0.12147\n",
      "[16:33:32] [14]\ttraining-aucpr:0.12204\n",
      "[16:33:32] [15]\ttraining-aucpr:0.12399\n",
      "[16:33:32] [16]\ttraining-aucpr:0.12604\n",
      "[16:33:32] [17]\ttraining-aucpr:0.12972\n",
      "[16:33:32] [18]\ttraining-aucpr:0.13202\n",
      "[16:33:32] [19]\ttraining-aucpr:0.13416\n",
      "[16:33:32] [20]\ttraining-aucpr:0.13620\n",
      "[16:33:32] [21]\ttraining-aucpr:0.13824\n",
      "[16:33:32] [22]\ttraining-aucpr:0.13962\n",
      "[16:33:32] [23]\ttraining-aucpr:0.14084\n",
      "[16:33:32] [24]\ttraining-aucpr:0.14221\n",
      "[16:33:32] [25]\ttraining-aucpr:0.14426\n",
      "[16:33:32] [26]\ttraining-aucpr:0.14669\n",
      "[16:33:32] [27]\ttraining-aucpr:0.14833\n",
      "[16:33:32] [28]\ttraining-aucpr:0.14937\n",
      "[16:33:32] [29]\ttraining-aucpr:0.15091\n",
      "[16:33:32] [30]\ttraining-aucpr:0.15280\n",
      "[16:33:32] [31]\ttraining-aucpr:0.15409\n",
      "[16:33:32] [32]\ttraining-aucpr:0.15517\n",
      "[16:33:32] [33]\ttraining-aucpr:0.15587\n",
      "[16:33:32] [34]\ttraining-aucpr:0.15612\n",
      "[16:33:32] [35]\ttraining-aucpr:0.15733\n",
      "[16:33:32] [36]\ttraining-aucpr:0.15918\n",
      "[16:33:33] [37]\ttraining-aucpr:0.16030\n",
      "[16:33:33] [38]\ttraining-aucpr:0.16134\n",
      "[16:33:33] [39]\ttraining-aucpr:0.16189\n",
      "[16:33:33] [40]\ttraining-aucpr:0.16308\n",
      "[16:33:33] [41]\ttraining-aucpr:0.16336\n",
      "[16:33:33] [42]\ttraining-aucpr:0.16457\n",
      "[16:33:33] [43]\ttraining-aucpr:0.16549\n",
      "[16:33:33] [44]\ttraining-aucpr:0.16662\n",
      "[16:33:33] [45]\ttraining-aucpr:0.16740\n",
      "[16:33:33] [46]\ttraining-aucpr:0.16846\n",
      "[16:33:33] [47]\ttraining-aucpr:0.16883\n",
      "[16:33:33] [48]\ttraining-aucpr:0.16946\n",
      "[16:33:33] [49]\ttraining-aucpr:0.17019\n",
      "[16:33:33] [50]\ttraining-aucpr:0.17105\n",
      "[16:33:33] [51]\ttraining-aucpr:0.17203\n",
      "[16:33:33] [52]\ttraining-aucpr:0.17262\n",
      "[16:33:33] [53]\ttraining-aucpr:0.17274\n",
      "[16:33:33] [54]\ttraining-aucpr:0.17334\n",
      "[16:33:33] [55]\ttraining-aucpr:0.17353\n",
      "[16:33:33] [56]\ttraining-aucpr:0.17398\n",
      "[16:33:33] [57]\ttraining-aucpr:0.17511\n",
      "[16:33:33] [58]\ttraining-aucpr:0.17563\n",
      "[16:33:33] [59]\ttraining-aucpr:0.17597\n",
      "[16:33:33] [60]\ttraining-aucpr:0.17638\n",
      "[16:33:33] [61]\ttraining-aucpr:0.17707\n",
      "[16:33:33] [62]\ttraining-aucpr:0.17794\n",
      "[16:33:33] [63]\ttraining-aucpr:0.17820\n",
      "[16:33:33] [64]\ttraining-aucpr:0.17869\n",
      "[16:33:33] [65]\ttraining-aucpr:0.17915\n",
      "[16:33:33] [66]\ttraining-aucpr:0.17953\n",
      "[16:33:33] [67]\ttraining-aucpr:0.17991\n",
      "[16:33:33] [68]\ttraining-aucpr:0.18044\n",
      "[16:33:33] [69]\ttraining-aucpr:0.18080\n",
      "[16:33:33] [70]\ttraining-aucpr:0.18127\n",
      "[16:33:33] [71]\ttraining-aucpr:0.18180\n",
      "[16:33:33] [72]\ttraining-aucpr:0.18239\n",
      "2025-07-28 16:33:34,988 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:33:35,101 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:33:35,839] Trial 43 finished with value: 0.18520705376350452 and parameters: {'max_depth': 5, 'learning_rate': 0.1216365881913747, 'n_estimators': 73, 'min_child_weight': 8, 'gamma': 0.23913615606426492, 'subsample': 0.9435680061785408, 'colsample_bytree': 0.7846404458190241, 'reg_alpha': 0.7182727622530461, 'reg_lambda': 0.051310376362638264}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:33:35,959 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.8746637308309775, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.5558916209426832, 'learning_rate': 0.010661223528944603, 'max_depth': 3, 'min_child_weight': 9, 'reg_alpha': 0.4586854640611171, 'reg_lambda': 0.15195461933942073, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9999663192340178, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 111}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:33:37,346 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:33:38] Task 1 got rank 1[16:33:38] Task 2 got rank 2[16:33:38] Task 3 got rank 3[16:33:38] Task 0 got rank 0\n",
      "\n",
      "\n",
      "\n",
      "[16:33:38] [0]\ttraining-aucpr:0.04710\n",
      "[16:33:38] [1]\ttraining-aucpr:0.04772\n",
      "[16:33:38] [2]\ttraining-aucpr:0.05318\n",
      "[16:33:38] [3]\ttraining-aucpr:0.05318\n",
      "[16:33:38] [4]\ttraining-aucpr:0.05357\n",
      "[16:33:39] [5]\ttraining-aucpr:0.05357\n",
      "[16:33:39] [6]\ttraining-aucpr:0.05975\n",
      "[16:33:39] [7]\ttraining-aucpr:0.05965\n",
      "[16:33:39] [8]\ttraining-aucpr:0.06054\n",
      "[16:33:39] [9]\ttraining-aucpr:0.06368\n",
      "[16:33:39] [10]\ttraining-aucpr:0.06355\n",
      "[16:33:39] [11]\ttraining-aucpr:0.06363\n",
      "[16:33:39] [12]\ttraining-aucpr:0.06360\n",
      "[16:33:39] [13]\ttraining-aucpr:0.06898\n",
      "[16:33:39] [14]\ttraining-aucpr:0.06907\n",
      "[16:33:39] [15]\ttraining-aucpr:0.06900\n",
      "[16:33:39] [16]\ttraining-aucpr:0.06920\n",
      "[16:33:39] [17]\ttraining-aucpr:0.06932\n",
      "[16:33:39] [18]\ttraining-aucpr:0.06967\n",
      "[16:33:39] [19]\ttraining-aucpr:0.07366\n",
      "[16:33:39] [20]\ttraining-aucpr:0.07368\n",
      "[16:33:39] [21]\ttraining-aucpr:0.07372\n",
      "[16:33:39] [22]\ttraining-aucpr:0.07378\n",
      "[16:33:39] [23]\ttraining-aucpr:0.07381\n",
      "[16:33:39] [24]\ttraining-aucpr:0.07351\n",
      "[16:33:39] [25]\ttraining-aucpr:0.07351\n",
      "[16:33:39] [26]\ttraining-aucpr:0.07336\n",
      "[16:33:39] [27]\ttraining-aucpr:0.07335\n",
      "[16:33:39] [28]\ttraining-aucpr:0.07309\n",
      "[16:33:39] [29]\ttraining-aucpr:0.07295\n",
      "[16:33:39] [30]\ttraining-aucpr:0.07304\n",
      "[16:33:39] [31]\ttraining-aucpr:0.07310\n",
      "[16:33:39] [32]\ttraining-aucpr:0.07297\n",
      "[16:33:39] [33]\ttraining-aucpr:0.07300\n",
      "[16:33:39] [34]\ttraining-aucpr:0.07291\n",
      "[16:33:39] [35]\ttraining-aucpr:0.07353\n",
      "[16:33:39] [36]\ttraining-aucpr:0.07368\n",
      "[16:33:39] [37]\ttraining-aucpr:0.07363\n",
      "[16:33:39] [38]\ttraining-aucpr:0.07367\n",
      "[16:33:39] [39]\ttraining-aucpr:0.07378\n",
      "[16:33:39] [40]\ttraining-aucpr:0.07372\n",
      "[16:33:39] [41]\ttraining-aucpr:0.07378\n",
      "[16:33:39] [42]\ttraining-aucpr:0.07376\n",
      "[16:33:39] [43]\ttraining-aucpr:0.07371\n",
      "[16:33:39] [44]\ttraining-aucpr:0.07380\n",
      "[16:33:39] [45]\ttraining-aucpr:0.07377\n",
      "[16:33:39] [46]\ttraining-aucpr:0.07391\n",
      "[16:33:39] [47]\ttraining-aucpr:0.07406\n",
      "[16:33:39] [48]\ttraining-aucpr:0.07412\n",
      "[16:33:39] [49]\ttraining-aucpr:0.07405\n",
      "[16:33:39] [50]\ttraining-aucpr:0.07414\n",
      "[16:33:39] [51]\ttraining-aucpr:0.07418\n",
      "[16:33:39] [52]\ttraining-aucpr:0.07411\n",
      "[16:33:39] [53]\ttraining-aucpr:0.07424\n",
      "[16:33:39] [54]\ttraining-aucpr:0.07437\n",
      "[16:33:39] [55]\ttraining-aucpr:0.07435\n",
      "[16:33:39] [56]\ttraining-aucpr:0.07438\n",
      "[16:33:39] [57]\ttraining-aucpr:0.07451\n",
      "[16:33:40] [58]\ttraining-aucpr:0.07451\n",
      "[16:33:40] [59]\ttraining-aucpr:0.07460\n",
      "[16:33:40] [60]\ttraining-aucpr:0.07461\n",
      "[16:33:40] [61]\ttraining-aucpr:0.07470\n",
      "[16:33:40] [62]\ttraining-aucpr:0.07473\n",
      "[16:33:40] [63]\ttraining-aucpr:0.07925\n",
      "[16:33:40] [64]\ttraining-aucpr:0.07925\n",
      "[16:33:40] [65]\ttraining-aucpr:0.07950\n",
      "[16:33:40] [66]\ttraining-aucpr:0.07954\n",
      "[16:33:40] [67]\ttraining-aucpr:0.07955\n",
      "[16:33:40] [68]\ttraining-aucpr:0.07955\n",
      "[16:33:40] [69]\ttraining-aucpr:0.07952\n",
      "[16:33:40] [70]\ttraining-aucpr:0.07951\n",
      "[16:33:40] [71]\ttraining-aucpr:0.07951\n",
      "[16:33:40] [72]\ttraining-aucpr:0.07958\n",
      "[16:33:40] [73]\ttraining-aucpr:0.07969\n",
      "[16:33:40] [74]\ttraining-aucpr:0.07971\n",
      "[16:33:40] [75]\ttraining-aucpr:0.07909\n",
      "[16:33:40] [76]\ttraining-aucpr:0.07911\n",
      "[16:33:40] [77]\ttraining-aucpr:0.07976\n",
      "[16:33:40] [78]\ttraining-aucpr:0.07977\n",
      "[16:33:40] [79]\ttraining-aucpr:0.07916\n",
      "[16:33:40] [80]\ttraining-aucpr:0.07984\n",
      "[16:33:40] [81]\ttraining-aucpr:0.07997\n",
      "[16:33:40] [82]\ttraining-aucpr:0.07989\n",
      "[16:33:40] [83]\ttraining-aucpr:0.07998\n",
      "[16:33:40] [84]\ttraining-aucpr:0.07941\n",
      "[16:33:40] [85]\ttraining-aucpr:0.07942\n",
      "[16:33:40] [86]\ttraining-aucpr:0.07993\n",
      "[16:33:40] [87]\ttraining-aucpr:0.08113\n",
      "[16:33:40] [88]\ttraining-aucpr:0.08112\n",
      "[16:33:40] [89]\ttraining-aucpr:0.08119\n",
      "[16:33:40] [90]\ttraining-aucpr:0.08115\n",
      "[16:33:40] [91]\ttraining-aucpr:0.08123\n",
      "[16:33:40] [92]\ttraining-aucpr:0.08129\n",
      "[16:33:40] [93]\ttraining-aucpr:0.08130\n",
      "[16:33:40] [94]\ttraining-aucpr:0.08318\n",
      "[16:33:40] [95]\ttraining-aucpr:0.08324\n",
      "[16:33:40] [96]\ttraining-aucpr:0.08401\n",
      "[16:33:40] [97]\ttraining-aucpr:0.08538\n",
      "[16:33:40] [98]\ttraining-aucpr:0.08581\n",
      "[16:33:40] [99]\ttraining-aucpr:0.08590\n",
      "[16:33:40] [100]\ttraining-aucpr:0.08579\n",
      "[16:33:40] [101]\ttraining-aucpr:0.08589\n",
      "[16:33:40] [102]\ttraining-aucpr:0.08604\n",
      "[16:33:40] [103]\ttraining-aucpr:0.08590\n",
      "[16:33:40] [104]\ttraining-aucpr:0.08593\n",
      "[16:33:40] [105]\ttraining-aucpr:0.08575\n",
      "[16:33:40] [106]\ttraining-aucpr:0.08600\n",
      "[16:33:41] [107]\ttraining-aucpr:0.08610\n",
      "[16:33:41] [108]\ttraining-aucpr:0.08733\n",
      "[16:33:41] [109]\ttraining-aucpr:0.08804\n",
      "[16:33:41] [110]\ttraining-aucpr:0.08918\n",
      "2025-07-28 16:33:42,097 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:33:42,183 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:33:42,728] Trial 44 finished with value: 0.11714129564682323 and parameters: {'max_depth': 3, 'learning_rate': 0.010661223528944603, 'n_estimators': 111, 'min_child_weight': 9, 'gamma': 0.5558916209426832, 'subsample': 0.9999663192340178, 'colsample_bytree': 0.8746637308309775, 'reg_alpha': 0.4586854640611171, 'reg_lambda': 0.15195461933942073}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:33:42,848 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.6988851558349847, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.7377356916758544, 'learning_rate': 0.19471809993339279, 'max_depth': 4, 'min_child_weight': 8, 'reg_alpha': 0.6019526023487326, 'reg_lambda': 0.04137357078124859, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.8835047250151841, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 97}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:33:44,236 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:33:45] Task 0 got rank 0[16:33:45] Task 2 got rank 2\n",
      "\n",
      "[16:33:45] Task 3 got rank 3\n",
      "[16:33:45] Task 1 got rank 1\n",
      "[16:33:45] [0]\ttraining-aucpr:0.05580\n",
      "[16:33:45] [1]\ttraining-aucpr:0.06058\n",
      "[16:33:45] [2]\ttraining-aucpr:0.06862\n",
      "[16:33:45] [3]\ttraining-aucpr:0.07062\n",
      "[16:33:45] [4]\ttraining-aucpr:0.07726\n",
      "[16:33:45] [5]\ttraining-aucpr:0.08427\n",
      "[16:33:45] [6]\ttraining-aucpr:0.09570\n",
      "[16:33:46] [7]\ttraining-aucpr:0.09954\n",
      "[16:33:46] [8]\ttraining-aucpr:0.10866\n",
      "[16:33:46] [9]\ttraining-aucpr:0.11109\n",
      "[16:33:46] [10]\ttraining-aucpr:0.11352\n",
      "[16:33:46] [11]\ttraining-aucpr:0.11774\n",
      "[16:33:46] [12]\ttraining-aucpr:0.12335\n",
      "[16:33:46] [13]\ttraining-aucpr:0.12793\n",
      "[16:33:46] [14]\ttraining-aucpr:0.12902\n",
      "[16:33:46] [15]\ttraining-aucpr:0.13120\n",
      "[16:33:46] [16]\ttraining-aucpr:0.13271\n",
      "[16:33:46] [17]\ttraining-aucpr:0.13532\n",
      "[16:33:46] [18]\ttraining-aucpr:0.13604\n",
      "[16:33:46] [19]\ttraining-aucpr:0.13859\n",
      "[16:33:46] [20]\ttraining-aucpr:0.13853\n",
      "[16:33:46] [21]\ttraining-aucpr:0.14119\n",
      "[16:33:46] [22]\ttraining-aucpr:0.14262\n",
      "[16:33:46] [23]\ttraining-aucpr:0.14338\n",
      "[16:33:46] [24]\ttraining-aucpr:0.14535\n",
      "[16:33:46] [25]\ttraining-aucpr:0.14671\n",
      "[16:33:46] [26]\ttraining-aucpr:0.14839\n",
      "[16:33:46] [27]\ttraining-aucpr:0.15004\n",
      "[16:33:46] [28]\ttraining-aucpr:0.15145\n",
      "[16:33:46] [29]\ttraining-aucpr:0.15252\n",
      "[16:33:46] [30]\ttraining-aucpr:0.15384\n",
      "[16:33:46] [31]\ttraining-aucpr:0.15468\n",
      "[16:33:46] [32]\ttraining-aucpr:0.15536\n",
      "[16:33:46] [33]\ttraining-aucpr:0.15613\n",
      "[16:33:46] [34]\ttraining-aucpr:0.15667\n",
      "[16:33:46] [35]\ttraining-aucpr:0.15786\n",
      "[16:33:46] [36]\ttraining-aucpr:0.15904\n",
      "[16:33:46] [37]\ttraining-aucpr:0.16017\n",
      "[16:33:46] [38]\ttraining-aucpr:0.16054\n",
      "[16:33:46] [39]\ttraining-aucpr:0.16104\n",
      "[16:33:47] [40]\ttraining-aucpr:0.16176\n",
      "[16:33:47] [41]\ttraining-aucpr:0.16244\n",
      "[16:33:47] [42]\ttraining-aucpr:0.16329\n",
      "[16:33:47] [43]\ttraining-aucpr:0.16359\n",
      "[16:33:47] [44]\ttraining-aucpr:0.16375\n",
      "[16:33:47] [45]\ttraining-aucpr:0.16443\n",
      "[16:33:47] [46]\ttraining-aucpr:0.16511\n",
      "[16:33:47] [47]\ttraining-aucpr:0.16560\n",
      "[16:33:47] [48]\ttraining-aucpr:0.16596\n",
      "[16:33:47] [49]\ttraining-aucpr:0.16657\n",
      "[16:33:47] [50]\ttraining-aucpr:0.16761\n",
      "[16:33:47] [51]\ttraining-aucpr:0.16802\n",
      "[16:33:47] [52]\ttraining-aucpr:0.16831\n",
      "[16:33:47] [53]\ttraining-aucpr:0.16862\n",
      "[16:33:47] [54]\ttraining-aucpr:0.16907\n",
      "[16:33:47] [55]\ttraining-aucpr:0.16944\n",
      "[16:33:47] [56]\ttraining-aucpr:0.16962\n",
      "[16:33:47] [57]\ttraining-aucpr:0.16989\n",
      "[16:33:47] [58]\ttraining-aucpr:0.17065\n",
      "[16:33:47] [59]\ttraining-aucpr:0.17075\n",
      "[16:33:47] [60]\ttraining-aucpr:0.17131\n",
      "[16:33:47] [61]\ttraining-aucpr:0.17157\n",
      "[16:33:47] [62]\ttraining-aucpr:0.17191\n",
      "[16:33:47] [63]\ttraining-aucpr:0.17207\n",
      "[16:33:47] [64]\ttraining-aucpr:0.17250\n",
      "[16:33:47] [65]\ttraining-aucpr:0.17276\n",
      "[16:33:47] [66]\ttraining-aucpr:0.17286\n",
      "[16:33:47] [67]\ttraining-aucpr:0.17289\n",
      "[16:33:47] [68]\ttraining-aucpr:0.17320\n",
      "[16:33:47] [69]\ttraining-aucpr:0.17353\n",
      "[16:33:47] [70]\ttraining-aucpr:0.17365\n",
      "[16:33:47] [71]\ttraining-aucpr:0.17402\n",
      "[16:33:47] [72]\ttraining-aucpr:0.17441\n",
      "[16:33:47] [73]\ttraining-aucpr:0.17455\n",
      "[16:33:47] [74]\ttraining-aucpr:0.17492\n",
      "[16:33:47] [75]\ttraining-aucpr:0.17512\n",
      "[16:33:47] [76]\ttraining-aucpr:0.17543\n",
      "[16:33:47] [77]\ttraining-aucpr:0.17553\n",
      "[16:33:48] [78]\ttraining-aucpr:0.17585\n",
      "[16:33:48] [79]\ttraining-aucpr:0.17598\n",
      "[16:33:48] [80]\ttraining-aucpr:0.17635\n",
      "[16:33:48] [81]\ttraining-aucpr:0.17621\n",
      "[16:33:48] [82]\ttraining-aucpr:0.17658\n",
      "[16:33:48] [83]\ttraining-aucpr:0.17658\n",
      "[16:33:48] [84]\ttraining-aucpr:0.17673\n",
      "[16:33:48] [85]\ttraining-aucpr:0.17681\n",
      "[16:33:48] [86]\ttraining-aucpr:0.17708\n",
      "[16:33:48] [87]\ttraining-aucpr:0.17733\n",
      "[16:33:48] [88]\ttraining-aucpr:0.17740\n",
      "[16:33:48] [89]\ttraining-aucpr:0.17756\n",
      "[16:33:48] [90]\ttraining-aucpr:0.17762\n",
      "[16:33:48] [91]\ttraining-aucpr:0.17801\n",
      "[16:33:48] [92]\ttraining-aucpr:0.17832\n",
      "[16:33:48] [93]\ttraining-aucpr:0.17862\n",
      "[16:33:48] [94]\ttraining-aucpr:0.17912\n",
      "[16:33:48] [95]\ttraining-aucpr:0.17929\n",
      "[16:33:48] [96]\ttraining-aucpr:0.17938\n",
      "2025-07-28 16:33:49,486 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:33:49,576 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:33:50,282] Trial 45 finished with value: 0.1899867322748986 and parameters: {'max_depth': 4, 'learning_rate': 0.19471809993339279, 'n_estimators': 97, 'min_child_weight': 8, 'gamma': 0.7377356916758544, 'subsample': 0.8835047250151841, 'colsample_bytree': 0.6988851558349847, 'reg_alpha': 0.6019526023487326, 'reg_lambda': 0.04137357078124859}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:33:50,409 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.8068188337365046, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.8299925493366054, 'learning_rate': 0.22447585315779858, 'max_depth': 3, 'min_child_weight': 7, 'reg_alpha': 0.27034266611335195, 'reg_lambda': 0.41765341707038434, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9697139032879885, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 64}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:33:51,786 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:33:52] Task 0 got rank 0\n",
      "[16:33:52] Task 2 got rank 2\n",
      "[16:33:52] Task 1 got rank 1\n",
      "[16:33:52] Task 3 got rank 3\n",
      "[16:33:53] [0]\ttraining-aucpr:0.04710\n",
      "[16:33:53] [1]\ttraining-aucpr:0.04811\n",
      "[16:33:53] [2]\ttraining-aucpr:0.05847\n",
      "[16:33:53] [3]\ttraining-aucpr:0.06416\n",
      "[16:33:53] [4]\ttraining-aucpr:0.06917\n",
      "[16:33:53] [5]\ttraining-aucpr:0.07636\n",
      "[16:33:53] [6]\ttraining-aucpr:0.08689\n",
      "[16:33:53] [7]\ttraining-aucpr:0.09077\n",
      "[16:33:53] [8]\ttraining-aucpr:0.09798\n",
      "[16:33:53] [9]\ttraining-aucpr:0.09870\n",
      "[16:33:53] [10]\ttraining-aucpr:0.10149\n",
      "[16:33:53] [11]\ttraining-aucpr:0.10947\n",
      "[16:33:53] [12]\ttraining-aucpr:0.11148\n",
      "[16:33:53] [13]\ttraining-aucpr:0.11495\n",
      "[16:33:53] [14]\ttraining-aucpr:0.11824\n",
      "[16:33:53] [15]\ttraining-aucpr:0.12111\n",
      "[16:33:53] [16]\ttraining-aucpr:0.12213\n",
      "[16:33:53] [17]\ttraining-aucpr:0.12648\n",
      "[16:33:53] [18]\ttraining-aucpr:0.12875\n",
      "[16:33:53] [19]\ttraining-aucpr:0.12980\n",
      "[16:33:53] [20]\ttraining-aucpr:0.13103\n",
      "[16:33:53] [21]\ttraining-aucpr:0.13379\n",
      "[16:33:53] [22]\ttraining-aucpr:0.13509\n",
      "[16:33:53] [23]\ttraining-aucpr:0.13514\n",
      "[16:33:53] [24]\ttraining-aucpr:0.13644\n",
      "[16:33:53] [25]\ttraining-aucpr:0.13770\n",
      "[16:33:53] [26]\ttraining-aucpr:0.13893\n",
      "[16:33:53] [27]\ttraining-aucpr:0.13996\n",
      "[16:33:54] [28]\ttraining-aucpr:0.14011\n",
      "[16:33:54] [29]\ttraining-aucpr:0.14113\n",
      "[16:33:54] [30]\ttraining-aucpr:0.14334\n",
      "[16:33:54] [31]\ttraining-aucpr:0.14508\n",
      "[16:33:54] [32]\ttraining-aucpr:0.14610\n",
      "[16:33:54] [33]\ttraining-aucpr:0.14615\n",
      "[16:33:54] [34]\ttraining-aucpr:0.14789\n",
      "[16:33:54] [35]\ttraining-aucpr:0.14876\n",
      "[16:33:54] [36]\ttraining-aucpr:0.14970\n",
      "[16:33:54] [37]\ttraining-aucpr:0.15004\n",
      "[16:33:54] [38]\ttraining-aucpr:0.15041\n",
      "[16:33:54] [39]\ttraining-aucpr:0.15158\n",
      "[16:33:54] [40]\ttraining-aucpr:0.15236\n",
      "[16:33:54] [41]\ttraining-aucpr:0.15294\n",
      "[16:33:54] [42]\ttraining-aucpr:0.15386\n",
      "[16:33:54] [43]\ttraining-aucpr:0.15473\n",
      "[16:33:54] [44]\ttraining-aucpr:0.15554\n",
      "[16:33:54] [45]\ttraining-aucpr:0.15641\n",
      "[16:33:54] [46]\ttraining-aucpr:0.15703\n",
      "[16:33:54] [47]\ttraining-aucpr:0.15776\n",
      "[16:33:54] [48]\ttraining-aucpr:0.15870\n",
      "[16:33:54] [49]\ttraining-aucpr:0.15909\n",
      "[16:33:54] [50]\ttraining-aucpr:0.15926\n",
      "[16:33:54] [51]\ttraining-aucpr:0.15983\n",
      "[16:33:54] [52]\ttraining-aucpr:0.16033\n",
      "[16:33:54] [53]\ttraining-aucpr:0.16072\n",
      "[16:33:54] [54]\ttraining-aucpr:0.16096\n",
      "[16:33:54] [55]\ttraining-aucpr:0.16141\n",
      "[16:33:54] [56]\ttraining-aucpr:0.16154\n",
      "[16:33:54] [57]\ttraining-aucpr:0.16205\n",
      "[16:33:54] [58]\ttraining-aucpr:0.16221\n",
      "[16:33:54] [59]\ttraining-aucpr:0.16244\n",
      "[16:33:54] [60]\ttraining-aucpr:0.16274\n",
      "[16:33:54] [61]\ttraining-aucpr:0.16277\n",
      "[16:33:54] [62]\ttraining-aucpr:0.16324\n",
      "[16:33:54] [63]\ttraining-aucpr:0.16353\n",
      "2025-07-28 16:33:55,896 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:33:55,972 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:33:56,631] Trial 46 finished with value: 0.18988649565672008 and parameters: {'max_depth': 3, 'learning_rate': 0.22447585315779858, 'n_estimators': 64, 'min_child_weight': 7, 'gamma': 0.8299925493366054, 'subsample': 0.9697139032879885, 'colsample_bytree': 0.8068188337365046, 'reg_alpha': 0.27034266611335195, 'reg_lambda': 0.41765341707038434}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:33:56,764 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.7448013564167297, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.686762418100363, 'learning_rate': 0.1016977952254094, 'max_depth': 5, 'min_child_weight': 3, 'reg_alpha': 0.18738258212800038, 'reg_lambda': 0.29865786740624223, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9034661342618663, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 141}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:33:58,142 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:33:59] Task 0 got rank 0\n",
      "[16:33:59] Task 1 got rank 1\n",
      "[16:33:59] Task 2 got rank 2\n",
      "[16:33:59] Task 3 got rank 3\n",
      "[16:33:59] [0]\ttraining-aucpr:0.06246\n",
      "[16:33:59] [1]\ttraining-aucpr:0.07654\n",
      "[16:33:59] [2]\ttraining-aucpr:0.07723\n",
      "[16:33:59] [3]\ttraining-aucpr:0.08532\n",
      "[16:33:59] [4]\ttraining-aucpr:0.09131\n",
      "[16:33:59] [5]\ttraining-aucpr:0.09498\n",
      "[16:33:59] [6]\ttraining-aucpr:0.10111\n",
      "[16:33:59] [7]\ttraining-aucpr:0.10271\n",
      "[16:33:59] [8]\ttraining-aucpr:0.10397\n",
      "[16:33:59] [9]\ttraining-aucpr:0.10628\n",
      "[16:33:59] [10]\ttraining-aucpr:0.10789\n",
      "[16:34:00] [11]\ttraining-aucpr:0.10940\n",
      "[16:34:00] [12]\ttraining-aucpr:0.11049\n",
      "[16:34:00] [13]\ttraining-aucpr:0.11644\n",
      "[16:34:00] [14]\ttraining-aucpr:0.11827\n",
      "[16:34:00] [15]\ttraining-aucpr:0.12115\n",
      "[16:34:00] [16]\ttraining-aucpr:0.12428\n",
      "[16:34:00] [17]\ttraining-aucpr:0.12615\n",
      "[16:34:00] [18]\ttraining-aucpr:0.12800\n",
      "[16:34:00] [19]\ttraining-aucpr:0.12992\n",
      "[16:34:00] [20]\ttraining-aucpr:0.13085\n",
      "[16:34:00] [21]\ttraining-aucpr:0.13132\n",
      "[16:34:00] [22]\ttraining-aucpr:0.13314\n",
      "[16:34:00] [23]\ttraining-aucpr:0.13382\n",
      "[16:34:00] [24]\ttraining-aucpr:0.13535\n",
      "[16:34:00] [25]\ttraining-aucpr:0.13763\n",
      "[16:34:00] [26]\ttraining-aucpr:0.13867\n",
      "[16:34:00] [27]\ttraining-aucpr:0.14124\n",
      "[16:34:00] [28]\ttraining-aucpr:0.14203\n",
      "[16:34:00] [29]\ttraining-aucpr:0.14333\n",
      "[16:34:00] [30]\ttraining-aucpr:0.14505\n",
      "[16:34:00] [31]\ttraining-aucpr:0.14598\n",
      "[16:34:00] [32]\ttraining-aucpr:0.14635\n",
      "[16:34:00] [33]\ttraining-aucpr:0.14724\n",
      "[16:34:00] [34]\ttraining-aucpr:0.14821\n",
      "[16:34:00] [35]\ttraining-aucpr:0.15048\n",
      "[16:34:00] [36]\ttraining-aucpr:0.15276\n",
      "[16:34:00] [37]\ttraining-aucpr:0.15452\n",
      "[16:34:00] [38]\ttraining-aucpr:0.15481\n",
      "[16:34:00] [39]\ttraining-aucpr:0.15559\n",
      "[16:34:00] [40]\ttraining-aucpr:0.15680\n",
      "[16:34:00] [41]\ttraining-aucpr:0.15784\n",
      "[16:34:00] [42]\ttraining-aucpr:0.15865\n",
      "[16:34:00] [43]\ttraining-aucpr:0.15962\n",
      "[16:34:01] [44]\ttraining-aucpr:0.15968\n",
      "[16:34:01] [45]\ttraining-aucpr:0.16067\n",
      "[16:34:01] [46]\ttraining-aucpr:0.16126\n",
      "[16:34:01] [47]\ttraining-aucpr:0.16269\n",
      "[16:34:01] [48]\ttraining-aucpr:0.16364\n",
      "[16:34:01] [49]\ttraining-aucpr:0.16461\n",
      "[16:34:01] [50]\ttraining-aucpr:0.16535\n",
      "[16:34:01] [51]\ttraining-aucpr:0.16648\n",
      "[16:34:01] [52]\ttraining-aucpr:0.16668\n",
      "[16:34:01] [53]\ttraining-aucpr:0.16727\n",
      "[16:34:01] [54]\ttraining-aucpr:0.16841\n",
      "[16:34:01] [55]\ttraining-aucpr:0.16918\n",
      "[16:34:01] [56]\ttraining-aucpr:0.16941\n",
      "[16:34:01] [57]\ttraining-aucpr:0.16976\n",
      "[16:34:01] [58]\ttraining-aucpr:0.17070\n",
      "[16:34:01] [59]\ttraining-aucpr:0.17095\n",
      "[16:34:01] [60]\ttraining-aucpr:0.17100\n",
      "[16:34:01] [61]\ttraining-aucpr:0.17179\n",
      "[16:34:01] [62]\ttraining-aucpr:0.17248\n",
      "[16:34:01] [63]\ttraining-aucpr:0.17279\n",
      "[16:34:01] [64]\ttraining-aucpr:0.17317\n",
      "[16:34:01] [65]\ttraining-aucpr:0.17383\n",
      "[16:34:01] [66]\ttraining-aucpr:0.17427\n",
      "[16:34:01] [67]\ttraining-aucpr:0.17492\n",
      "[16:34:01] [68]\ttraining-aucpr:0.17528\n",
      "[16:34:01] [69]\ttraining-aucpr:0.17588\n",
      "[16:34:01] [70]\ttraining-aucpr:0.17644\n",
      "[16:34:01] [71]\ttraining-aucpr:0.17712\n",
      "[16:34:01] [72]\ttraining-aucpr:0.17759\n",
      "[16:34:01] [73]\ttraining-aucpr:0.17821\n",
      "[16:34:01] [74]\ttraining-aucpr:0.17878\n",
      "[16:34:01] [75]\ttraining-aucpr:0.17949\n",
      "[16:34:01] [76]\ttraining-aucpr:0.17989\n",
      "[16:34:01] [77]\ttraining-aucpr:0.18054\n",
      "[16:34:02] [78]\ttraining-aucpr:0.18110\n",
      "[16:34:02] [79]\ttraining-aucpr:0.18128\n",
      "[16:34:02] [80]\ttraining-aucpr:0.18142\n",
      "[16:34:02] [81]\ttraining-aucpr:0.18194\n",
      "[16:34:02] [82]\ttraining-aucpr:0.18228\n",
      "[16:34:02] [83]\ttraining-aucpr:0.18276\n",
      "[16:34:02] [84]\ttraining-aucpr:0.18321\n",
      "[16:34:02] [85]\ttraining-aucpr:0.18350\n",
      "[16:34:02] [86]\ttraining-aucpr:0.18406\n",
      "[16:34:02] [87]\ttraining-aucpr:0.18469\n",
      "[16:34:02] [88]\ttraining-aucpr:0.18480\n",
      "[16:34:02] [89]\ttraining-aucpr:0.18528\n",
      "[16:34:02] [90]\ttraining-aucpr:0.18560\n",
      "[16:34:02] [91]\ttraining-aucpr:0.18596\n",
      "[16:34:02] [92]\ttraining-aucpr:0.18625\n",
      "[16:34:02] [93]\ttraining-aucpr:0.18665\n",
      "[16:34:02] [94]\ttraining-aucpr:0.18691\n",
      "[16:34:02] [95]\ttraining-aucpr:0.18700\n",
      "[16:34:02] [96]\ttraining-aucpr:0.18736\n",
      "[16:34:02] [97]\ttraining-aucpr:0.18754\n",
      "[16:34:02] [98]\ttraining-aucpr:0.18768\n",
      "[16:34:02] [99]\ttraining-aucpr:0.18788\n",
      "[16:34:02] [100]\ttraining-aucpr:0.18805\n",
      "[16:34:02] [101]\ttraining-aucpr:0.18854\n",
      "[16:34:02] [102]\ttraining-aucpr:0.18870\n",
      "[16:34:02] [103]\ttraining-aucpr:0.18879\n",
      "[16:34:02] [104]\ttraining-aucpr:0.18921\n",
      "[16:34:02] [105]\ttraining-aucpr:0.18976\n",
      "[16:34:02] [106]\ttraining-aucpr:0.19024\n",
      "[16:34:02] [107]\ttraining-aucpr:0.19062\n",
      "[16:34:02] [108]\ttraining-aucpr:0.19078\n",
      "[16:34:02] [109]\ttraining-aucpr:0.19118\n",
      "[16:34:02] [110]\ttraining-aucpr:0.19123\n",
      "[16:34:02] [111]\ttraining-aucpr:0.19139\n",
      "[16:34:02] [112]\ttraining-aucpr:0.19168\n",
      "[16:34:02] [113]\ttraining-aucpr:0.19187\n",
      "[16:34:03] [114]\ttraining-aucpr:0.19213\n",
      "[16:34:03] [115]\ttraining-aucpr:0.19235\n",
      "[16:34:03] [116]\ttraining-aucpr:0.19275\n",
      "[16:34:03] [117]\ttraining-aucpr:0.19305\n",
      "[16:34:03] [118]\ttraining-aucpr:0.19327\n",
      "[16:34:03] [119]\ttraining-aucpr:0.19343\n",
      "[16:34:03] [120]\ttraining-aucpr:0.19389\n",
      "[16:34:03] [121]\ttraining-aucpr:0.19413\n",
      "[16:34:03] [122]\ttraining-aucpr:0.19424\n",
      "[16:34:03] [123]\ttraining-aucpr:0.19429\n",
      "[16:34:03] [124]\ttraining-aucpr:0.19436\n",
      "[16:34:03] [125]\ttraining-aucpr:0.19457\n",
      "[16:34:03] [126]\ttraining-aucpr:0.19477\n",
      "[16:34:03] [127]\ttraining-aucpr:0.19509\n",
      "[16:34:03] [128]\ttraining-aucpr:0.19507\n",
      "[16:34:03] [129]\ttraining-aucpr:0.19534\n",
      "[16:34:03] [130]\ttraining-aucpr:0.19552\n",
      "[16:34:03] [131]\ttraining-aucpr:0.19582\n",
      "[16:34:03] [132]\ttraining-aucpr:0.19584\n",
      "[16:34:03] [133]\ttraining-aucpr:0.19625\n",
      "[16:34:03] [134]\ttraining-aucpr:0.19633\n",
      "[16:34:03] [135]\ttraining-aucpr:0.19641\n",
      "[16:34:03] [136]\ttraining-aucpr:0.19659\n",
      "[16:34:03] [137]\ttraining-aucpr:0.19674\n",
      "[16:34:03] [138]\ttraining-aucpr:0.19693\n",
      "[16:34:03] [139]\ttraining-aucpr:0.19711\n",
      "[16:34:03] [140]\ttraining-aucpr:0.19734\n",
      "2025-07-28 16:34:04,736 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:34:04,827 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:34:05,525] Trial 47 finished with value: 0.1849781454746104 and parameters: {'max_depth': 5, 'learning_rate': 0.1016977952254094, 'n_estimators': 141, 'min_child_weight': 3, 'gamma': 0.686762418100363, 'subsample': 0.9034661342618663, 'colsample_bytree': 0.7448013564167297, 'reg_alpha': 0.18738258212800038, 'reg_lambda': 0.29865786740624223}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:34:05,648 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.8326967966187905, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.5013829967698382, 'learning_rate': 0.0727057990823747, 'max_depth': 4, 'min_child_weight': 10, 'reg_alpha': 0.4899338888526015, 'reg_lambda': 0.7109130181565528, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9391104221313802, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 126}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:34:07,022 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:34:08] Task 2 got rank 2\n",
      "[16:34:08] Task 0 got rank 0\n",
      "[16:34:08] Task 1 got rank 1\n",
      "[16:34:08] Task 3 got rank 3\n",
      "[16:34:08] [0]\ttraining-aucpr:0.05562\n",
      "[16:34:08] [1]\ttraining-aucpr:0.06010\n",
      "[16:34:08] [2]\ttraining-aucpr:0.06116\n",
      "[16:34:08] [3]\ttraining-aucpr:0.06633\n",
      "[16:34:08] [4]\ttraining-aucpr:0.07312\n",
      "[16:34:08] [5]\ttraining-aucpr:0.07381\n",
      "[16:34:08] [6]\ttraining-aucpr:0.07370\n",
      "[16:34:08] [7]\ttraining-aucpr:0.07510\n",
      "[16:34:08] [8]\ttraining-aucpr:0.07541\n",
      "[16:34:08] [9]\ttraining-aucpr:0.08265\n",
      "[16:34:08] [10]\ttraining-aucpr:0.08648\n",
      "[16:34:08] [11]\ttraining-aucpr:0.08894\n",
      "[16:34:08] [12]\ttraining-aucpr:0.08895\n",
      "[16:34:08] [13]\ttraining-aucpr:0.09268\n",
      "[16:34:08] [14]\ttraining-aucpr:0.09264\n",
      "[16:34:08] [15]\ttraining-aucpr:0.09261\n",
      "[16:34:08] [16]\ttraining-aucpr:0.09390\n",
      "[16:34:08] [17]\ttraining-aucpr:0.09844\n",
      "[16:34:09] [18]\ttraining-aucpr:0.10363\n",
      "[16:34:09] [19]\ttraining-aucpr:0.10754\n",
      "[16:34:09] [20]\ttraining-aucpr:0.10944\n",
      "[16:34:09] [21]\ttraining-aucpr:0.11039\n",
      "[16:34:09] [22]\ttraining-aucpr:0.11195\n",
      "[16:34:09] [23]\ttraining-aucpr:0.11426\n",
      "[16:34:09] [24]\ttraining-aucpr:0.11446\n",
      "[16:34:09] [25]\ttraining-aucpr:0.11552\n",
      "[16:34:09] [26]\ttraining-aucpr:0.11656\n",
      "[16:34:09] [27]\ttraining-aucpr:0.11863\n",
      "[16:34:09] [28]\ttraining-aucpr:0.12064\n",
      "[16:34:09] [29]\ttraining-aucpr:0.12244\n",
      "[16:34:09] [30]\ttraining-aucpr:0.12420\n",
      "[16:34:09] [31]\ttraining-aucpr:0.12558\n",
      "[16:34:09] [32]\ttraining-aucpr:0.12619\n",
      "[16:34:09] [33]\ttraining-aucpr:0.12656\n",
      "[16:34:09] [34]\ttraining-aucpr:0.12772\n",
      "[16:34:09] [35]\ttraining-aucpr:0.12840\n",
      "[16:34:09] [36]\ttraining-aucpr:0.12948\n",
      "[16:34:09] [37]\ttraining-aucpr:0.13117\n",
      "[16:34:09] [38]\ttraining-aucpr:0.13252\n",
      "[16:34:09] [39]\ttraining-aucpr:0.13377\n",
      "[16:34:09] [40]\ttraining-aucpr:0.13507\n",
      "[16:34:09] [41]\ttraining-aucpr:0.13553\n",
      "[16:34:09] [42]\ttraining-aucpr:0.13638\n",
      "[16:34:09] [43]\ttraining-aucpr:0.13663\n",
      "[16:34:09] [44]\ttraining-aucpr:0.13730\n",
      "[16:34:09] [45]\ttraining-aucpr:0.13876\n",
      "[16:34:09] [46]\ttraining-aucpr:0.13923\n",
      "[16:34:09] [47]\ttraining-aucpr:0.13984\n",
      "[16:34:09] [48]\ttraining-aucpr:0.14054\n",
      "[16:34:09] [49]\ttraining-aucpr:0.14056\n",
      "[16:34:09] [50]\ttraining-aucpr:0.14175\n",
      "[16:34:09] [51]\ttraining-aucpr:0.14273\n",
      "[16:34:09] [52]\ttraining-aucpr:0.14354\n",
      "[16:34:09] [53]\ttraining-aucpr:0.14442\n",
      "[16:34:09] [54]\ttraining-aucpr:0.14531\n",
      "[16:34:09] [55]\ttraining-aucpr:0.14574\n",
      "[16:34:09] [56]\ttraining-aucpr:0.14657\n",
      "[16:34:10] [57]\ttraining-aucpr:0.14728\n",
      "[16:34:10] [58]\ttraining-aucpr:0.14777\n",
      "[16:34:10] [59]\ttraining-aucpr:0.14795\n",
      "[16:34:10] [60]\ttraining-aucpr:0.14803\n",
      "[16:34:10] [61]\ttraining-aucpr:0.14850\n",
      "[16:34:10] [62]\ttraining-aucpr:0.14900\n",
      "[16:34:10] [63]\ttraining-aucpr:0.14951\n",
      "[16:34:10] [64]\ttraining-aucpr:0.14984\n",
      "[16:34:10] [65]\ttraining-aucpr:0.15035\n",
      "[16:34:10] [66]\ttraining-aucpr:0.15060\n",
      "[16:34:10] [67]\ttraining-aucpr:0.15079\n",
      "[16:34:10] [68]\ttraining-aucpr:0.15115\n",
      "[16:34:10] [69]\ttraining-aucpr:0.15144\n",
      "[16:34:10] [70]\ttraining-aucpr:0.15233\n",
      "[16:34:10] [71]\ttraining-aucpr:0.15303\n",
      "[16:34:10] [72]\ttraining-aucpr:0.15374\n",
      "[16:34:10] [73]\ttraining-aucpr:0.15443\n",
      "[16:34:10] [74]\ttraining-aucpr:0.15501\n",
      "[16:34:10] [75]\ttraining-aucpr:0.15553\n",
      "[16:34:10] [76]\ttraining-aucpr:0.15615\n",
      "[16:34:10] [77]\ttraining-aucpr:0.15634\n",
      "[16:34:10] [78]\ttraining-aucpr:0.15685\n",
      "[16:34:10] [79]\ttraining-aucpr:0.15743\n",
      "[16:34:10] [80]\ttraining-aucpr:0.15755\n",
      "[16:34:10] [81]\ttraining-aucpr:0.15761\n",
      "[16:34:10] [82]\ttraining-aucpr:0.15793\n",
      "[16:34:10] [83]\ttraining-aucpr:0.15842\n",
      "[16:34:10] [84]\ttraining-aucpr:0.15912\n",
      "[16:34:10] [85]\ttraining-aucpr:0.15945\n",
      "[16:34:10] [86]\ttraining-aucpr:0.15976\n",
      "[16:34:10] [87]\ttraining-aucpr:0.16006\n",
      "[16:34:10] [88]\ttraining-aucpr:0.16056\n",
      "[16:34:10] [89]\ttraining-aucpr:0.16074\n",
      "[16:34:10] [90]\ttraining-aucpr:0.16091\n",
      "[16:34:10] [91]\ttraining-aucpr:0.16154\n",
      "[16:34:10] [92]\ttraining-aucpr:0.16190\n",
      "[16:34:10] [93]\ttraining-aucpr:0.16216\n",
      "[16:34:10] [94]\ttraining-aucpr:0.16244\n",
      "[16:34:10] [95]\ttraining-aucpr:0.16269\n",
      "[16:34:10] [96]\ttraining-aucpr:0.16313\n",
      "[16:34:11] [97]\ttraining-aucpr:0.16326\n",
      "[16:34:11] [98]\ttraining-aucpr:0.16329\n",
      "[16:34:11] [99]\ttraining-aucpr:0.16358\n",
      "[16:34:11] [100]\ttraining-aucpr:0.16396\n",
      "[16:34:11] [101]\ttraining-aucpr:0.16427\n",
      "[16:34:11] [102]\ttraining-aucpr:0.16474\n",
      "[16:34:11] [103]\ttraining-aucpr:0.16500\n",
      "[16:34:11] [104]\ttraining-aucpr:0.16542\n",
      "[16:34:11] [105]\ttraining-aucpr:0.16575\n",
      "[16:34:11] [106]\ttraining-aucpr:0.16582\n",
      "[16:34:11] [107]\ttraining-aucpr:0.16621\n",
      "[16:34:11] [108]\ttraining-aucpr:0.16636\n",
      "[16:34:11] [109]\ttraining-aucpr:0.16665\n",
      "[16:34:11] [110]\ttraining-aucpr:0.16702\n",
      "[16:34:11] [111]\ttraining-aucpr:0.16722\n",
      "[16:34:11] [112]\ttraining-aucpr:0.16768\n",
      "[16:34:11] [113]\ttraining-aucpr:0.16783\n",
      "[16:34:11] [114]\ttraining-aucpr:0.16801\n",
      "[16:34:11] [115]\ttraining-aucpr:0.16818\n",
      "[16:34:11] [116]\ttraining-aucpr:0.16831\n",
      "[16:34:11] [117]\ttraining-aucpr:0.16846\n",
      "[16:34:11] [118]\ttraining-aucpr:0.16858\n",
      "[16:34:11] [119]\ttraining-aucpr:0.16867\n",
      "[16:34:11] [120]\ttraining-aucpr:0.16903\n",
      "[16:34:11] [121]\ttraining-aucpr:0.16935\n",
      "[16:34:11] [122]\ttraining-aucpr:0.16950\n",
      "[16:34:11] [123]\ttraining-aucpr:0.16978\n",
      "[16:34:11] [124]\ttraining-aucpr:0.17010\n",
      "[16:34:11] [125]\ttraining-aucpr:0.17032\n",
      "2025-07-28 16:34:12,715 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:34:12,805 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:34:13,480] Trial 48 finished with value: 0.18838517099804428 and parameters: {'max_depth': 4, 'learning_rate': 0.0727057990823747, 'n_estimators': 126, 'min_child_weight': 10, 'gamma': 0.5013829967698382, 'subsample': 0.9391104221313802, 'colsample_bytree': 0.8326967966187905, 'reg_alpha': 0.4899338888526015, 'reg_lambda': 0.7109130181565528}. Best is trial 14 with value: 0.19427317857446047.\n",
      "2025-07-28 16:34:13,590 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.7705077049207281, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.6138262892618663, 'learning_rate': 0.1355924526646326, 'max_depth': 3, 'min_child_weight': 8, 'reg_alpha': 0.5804793928477672, 'reg_lambda': 0.17671504264721596, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9789615757323006, 'verbosity': 0, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 177}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:34:14,977 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:34:16] Task 0 got rank 0\n",
      "[16:34:16] Task 3 got rank 3\n",
      "[16:34:16] Task 2 got rank 2\n",
      "[16:34:16] Task 1 got rank 1\n",
      "[16:34:16] [0]\ttraining-aucpr:0.04710\n",
      "[16:34:16] [1]\ttraining-aucpr:0.04817\n",
      "[16:34:16] [2]\ttraining-aucpr:0.05790\n",
      "[16:34:16] [3]\ttraining-aucpr:0.06307\n",
      "[16:34:16] [4]\ttraining-aucpr:0.06360\n",
      "[16:34:16] [5]\ttraining-aucpr:0.06896\n",
      "[16:34:16] [6]\ttraining-aucpr:0.07575\n",
      "[16:34:16] [7]\ttraining-aucpr:0.08255\n",
      "[16:34:16] [8]\ttraining-aucpr:0.08775\n",
      "[16:34:16] [9]\ttraining-aucpr:0.09390\n",
      "[16:34:16] [10]\ttraining-aucpr:0.09623\n",
      "[16:34:16] [11]\ttraining-aucpr:0.09678\n",
      "[16:34:16] [12]\ttraining-aucpr:0.10085\n",
      "[16:34:16] [13]\ttraining-aucpr:0.10664\n",
      "[16:34:16] [14]\ttraining-aucpr:0.10828\n",
      "[16:34:16] [15]\ttraining-aucpr:0.10937\n",
      "[16:34:16] [16]\ttraining-aucpr:0.11196\n",
      "[16:34:16] [17]\ttraining-aucpr:0.11377\n",
      "[16:34:16] [18]\ttraining-aucpr:0.11498\n",
      "[16:34:16] [19]\ttraining-aucpr:0.11553\n",
      "[16:34:16] [20]\ttraining-aucpr:0.11807\n",
      "[16:34:16] [21]\ttraining-aucpr:0.12032\n",
      "[16:34:16] [22]\ttraining-aucpr:0.12048\n",
      "[16:34:16] [23]\ttraining-aucpr:0.12229\n",
      "[16:34:16] [24]\ttraining-aucpr:0.12450\n",
      "[16:34:17] [25]\ttraining-aucpr:0.12471\n",
      "[16:34:17] [26]\ttraining-aucpr:0.12600\n",
      "[16:34:17] [27]\ttraining-aucpr:0.12754\n",
      "[16:34:17] [28]\ttraining-aucpr:0.12853\n",
      "[16:34:17] [29]\ttraining-aucpr:0.13046\n",
      "[16:34:17] [30]\ttraining-aucpr:0.13176\n",
      "[16:34:17] [31]\ttraining-aucpr:0.13286\n",
      "[16:34:17] [32]\ttraining-aucpr:0.13509\n",
      "[16:34:17] [33]\ttraining-aucpr:0.13512\n",
      "[16:34:17] [34]\ttraining-aucpr:0.13584\n",
      "[16:34:17] [35]\ttraining-aucpr:0.13710\n",
      "[16:34:17] [36]\ttraining-aucpr:0.13800\n",
      "[16:34:17] [37]\ttraining-aucpr:0.13874\n",
      "[16:34:17] [38]\ttraining-aucpr:0.14054\n",
      "[16:34:17] [39]\ttraining-aucpr:0.14200\n",
      "[16:34:17] [40]\ttraining-aucpr:0.14239\n",
      "[16:34:17] [41]\ttraining-aucpr:0.14237\n",
      "[16:34:17] [42]\ttraining-aucpr:0.14321\n",
      "[16:34:17] [43]\ttraining-aucpr:0.14438\n",
      "[16:34:17] [44]\ttraining-aucpr:0.14512\n",
      "[16:34:17] [45]\ttraining-aucpr:0.14593\n",
      "[16:34:17] [46]\ttraining-aucpr:0.14630\n",
      "[16:34:17] [47]\ttraining-aucpr:0.14633\n",
      "[16:34:17] [48]\ttraining-aucpr:0.14747\n",
      "[16:34:17] [49]\ttraining-aucpr:0.14805\n",
      "[16:34:17] [50]\ttraining-aucpr:0.14885\n",
      "[16:34:17] [51]\ttraining-aucpr:0.15013\n",
      "[16:34:17] [52]\ttraining-aucpr:0.15038\n",
      "[16:34:17] [53]\ttraining-aucpr:0.15127\n",
      "[16:34:17] [54]\ttraining-aucpr:0.15202\n",
      "[16:34:17] [55]\ttraining-aucpr:0.15234\n",
      "[16:34:17] [56]\ttraining-aucpr:0.15295\n",
      "[16:34:17] [57]\ttraining-aucpr:0.15345\n",
      "[16:34:17] [58]\ttraining-aucpr:0.15375\n",
      "[16:34:17] [59]\ttraining-aucpr:0.15438\n",
      "[16:34:17] [60]\ttraining-aucpr:0.15458\n",
      "[16:34:17] [61]\ttraining-aucpr:0.15485\n",
      "[16:34:17] [62]\ttraining-aucpr:0.15547\n",
      "[16:34:17] [63]\ttraining-aucpr:0.15604\n",
      "[16:34:17] [64]\ttraining-aucpr:0.15665\n",
      "[16:34:17] [65]\ttraining-aucpr:0.15726\n",
      "[16:34:17] [66]\ttraining-aucpr:0.15734\n",
      "[16:34:17] [67]\ttraining-aucpr:0.15776\n",
      "[16:34:17] [68]\ttraining-aucpr:0.15820\n",
      "[16:34:18] [69]\ttraining-aucpr:0.15821\n",
      "[16:34:18] [70]\ttraining-aucpr:0.15862\n",
      "[16:34:18] [71]\ttraining-aucpr:0.15933\n",
      "[16:34:18] [72]\ttraining-aucpr:0.15942\n",
      "[16:34:18] [73]\ttraining-aucpr:0.15995\n",
      "[16:34:18] [74]\ttraining-aucpr:0.16030\n",
      "[16:34:18] [75]\ttraining-aucpr:0.16088\n",
      "[16:34:18] [76]\ttraining-aucpr:0.16127\n",
      "[16:34:18] [77]\ttraining-aucpr:0.16147\n",
      "[16:34:18] [78]\ttraining-aucpr:0.16166\n",
      "[16:34:18] [79]\ttraining-aucpr:0.16162\n",
      "[16:34:18] [80]\ttraining-aucpr:0.16192\n",
      "[16:34:18] [81]\ttraining-aucpr:0.16210\n",
      "[16:34:18] [82]\ttraining-aucpr:0.16266\n",
      "[16:34:18] [83]\ttraining-aucpr:0.16302\n",
      "[16:34:18] [84]\ttraining-aucpr:0.16341\n",
      "[16:34:18] [85]\ttraining-aucpr:0.16341\n",
      "[16:34:18] [86]\ttraining-aucpr:0.16381\n",
      "[16:34:18] [87]\ttraining-aucpr:0.16407\n",
      "[16:34:18] [88]\ttraining-aucpr:0.16431\n",
      "[16:34:18] [89]\ttraining-aucpr:0.16459\n",
      "[16:34:18] [90]\ttraining-aucpr:0.16499\n",
      "[16:34:18] [91]\ttraining-aucpr:0.16526\n",
      "[16:34:18] [92]\ttraining-aucpr:0.16537\n",
      "[16:34:18] [93]\ttraining-aucpr:0.16536\n",
      "[16:34:18] [94]\ttraining-aucpr:0.16554\n",
      "[16:34:18] [95]\ttraining-aucpr:0.16567\n",
      "[16:34:18] [96]\ttraining-aucpr:0.16598\n",
      "[16:34:18] [97]\ttraining-aucpr:0.16628\n",
      "[16:34:18] [98]\ttraining-aucpr:0.16637\n",
      "[16:34:18] [99]\ttraining-aucpr:0.16657\n",
      "[16:34:18] [100]\ttraining-aucpr:0.16668\n",
      "[16:34:18] [101]\ttraining-aucpr:0.16691\n",
      "[16:34:18] [102]\ttraining-aucpr:0.16714\n",
      "[16:34:18] [103]\ttraining-aucpr:0.16737\n",
      "[16:34:18] [104]\ttraining-aucpr:0.16758\n",
      "[16:34:18] [105]\ttraining-aucpr:0.16775\n",
      "[16:34:18] [106]\ttraining-aucpr:0.16788\n",
      "[16:34:18] [107]\ttraining-aucpr:0.16822\n",
      "[16:34:18] [108]\ttraining-aucpr:0.16844\n",
      "[16:34:18] [109]\ttraining-aucpr:0.16865\n",
      "[16:34:18] [110]\ttraining-aucpr:0.16874\n",
      "[16:34:19] [111]\ttraining-aucpr:0.16904\n",
      "[16:34:19] [112]\ttraining-aucpr:0.16915\n",
      "[16:34:19] [113]\ttraining-aucpr:0.16915\n",
      "[16:34:19] [114]\ttraining-aucpr:0.16937\n",
      "[16:34:19] [115]\ttraining-aucpr:0.16964\n",
      "[16:34:19] [116]\ttraining-aucpr:0.16983\n",
      "[16:34:19] [117]\ttraining-aucpr:0.16995\n",
      "[16:34:19] [118]\ttraining-aucpr:0.17007\n",
      "[16:34:19] [119]\ttraining-aucpr:0.17031\n",
      "[16:34:19] [120]\ttraining-aucpr:0.17049\n",
      "[16:34:19] [121]\ttraining-aucpr:0.17065\n",
      "[16:34:19] [122]\ttraining-aucpr:0.17091\n",
      "[16:34:19] [123]\ttraining-aucpr:0.17105\n",
      "[16:34:19] [124]\ttraining-aucpr:0.17117\n",
      "[16:34:19] [125]\ttraining-aucpr:0.17129\n",
      "[16:34:19] [126]\ttraining-aucpr:0.17138\n",
      "[16:34:19] [127]\ttraining-aucpr:0.17138\n",
      "[16:34:19] [128]\ttraining-aucpr:0.17159\n",
      "[16:34:19] [129]\ttraining-aucpr:0.17171\n",
      "[16:34:19] [130]\ttraining-aucpr:0.17176\n",
      "[16:34:19] [131]\ttraining-aucpr:0.17182\n",
      "[16:34:19] [132]\ttraining-aucpr:0.17190\n",
      "[16:34:19] [133]\ttraining-aucpr:0.17212\n",
      "[16:34:19] [134]\ttraining-aucpr:0.17217\n",
      "[16:34:19] [135]\ttraining-aucpr:0.17230\n",
      "[16:34:19] [136]\ttraining-aucpr:0.17226\n",
      "[16:34:19] [137]\ttraining-aucpr:0.17239\n",
      "[16:34:19] [138]\ttraining-aucpr:0.17250\n",
      "[16:34:19] [139]\ttraining-aucpr:0.17269\n",
      "[16:34:19] [140]\ttraining-aucpr:0.17268\n",
      "[16:34:19] [141]\ttraining-aucpr:0.17271\n",
      "[16:34:19] [142]\ttraining-aucpr:0.17287\n",
      "[16:34:19] [143]\ttraining-aucpr:0.17300\n",
      "[16:34:19] [144]\ttraining-aucpr:0.17316\n",
      "[16:34:19] [145]\ttraining-aucpr:0.17327\n",
      "[16:34:19] [146]\ttraining-aucpr:0.17337\n",
      "[16:34:19] [147]\ttraining-aucpr:0.17356\n",
      "[16:34:19] [148]\ttraining-aucpr:0.17364\n",
      "[16:34:19] [149]\ttraining-aucpr:0.17371\n",
      "[16:34:20] [150]\ttraining-aucpr:0.17377\n",
      "[16:34:20] [151]\ttraining-aucpr:0.17383\n",
      "[16:34:20] [152]\ttraining-aucpr:0.17391\n",
      "[16:34:20] [153]\ttraining-aucpr:0.17397\n",
      "[16:34:20] [154]\ttraining-aucpr:0.17411\n",
      "[16:34:20] [155]\ttraining-aucpr:0.17412\n",
      "[16:34:20] [156]\ttraining-aucpr:0.17414\n",
      "[16:34:20] [157]\ttraining-aucpr:0.17416\n",
      "[16:34:20] [158]\ttraining-aucpr:0.17425\n",
      "[16:34:20] [159]\ttraining-aucpr:0.17435\n",
      "[16:34:20] [160]\ttraining-aucpr:0.17446\n",
      "[16:34:20] [161]\ttraining-aucpr:0.17450\n",
      "[16:34:20] [162]\ttraining-aucpr:0.17457\n",
      "[16:34:20] [163]\ttraining-aucpr:0.17466\n",
      "[16:34:20] [164]\ttraining-aucpr:0.17474\n",
      "[16:34:20] [165]\ttraining-aucpr:0.17479\n",
      "[16:34:20] [166]\ttraining-aucpr:0.17498\n",
      "[16:34:20] [167]\ttraining-aucpr:0.17499\n",
      "[16:34:20] [168]\ttraining-aucpr:0.17501\n",
      "[16:34:20] [169]\ttraining-aucpr:0.17512\n",
      "[16:34:20] [170]\ttraining-aucpr:0.17514\n",
      "[16:34:20] [171]\ttraining-aucpr:0.17523\n",
      "[16:34:20] [172]\ttraining-aucpr:0.17528\n",
      "[16:34:20] [173]\ttraining-aucpr:0.17524\n",
      "[16:34:20] [174]\ttraining-aucpr:0.17532\n",
      "[16:34:20] [175]\ttraining-aucpr:0.17541\n",
      "[16:34:20] [176]\ttraining-aucpr:0.17544\n",
      "2025-07-28 16:34:21,623 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:34:21,708 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[I 2025-07-28 16:34:22,389] Trial 49 finished with value: 0.1943670834898125 and parameters: {'max_depth': 3, 'learning_rate': 0.1355924526646326, 'n_estimators': 177, 'min_child_weight': 8, 'gamma': 0.6138262892618663, 'subsample': 0.9789615757323006, 'colsample_bytree': 0.7705077049207281, 'reg_alpha': 0.5804793928477672, 'reg_lambda': 0.17671504264721596}. Best is trial 49 with value: 0.1943670834898125.\n",
      "2025-07-28 16:34:22,489 INFO XGBoost-PySpark: _fit Running xgboost-3.0.2 on 4 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'colsample_bytree': 0.7705077049207281, 'device': 'cpu', 'eval_metric': 'aucpr', 'gamma': 0.6138262892618663, 'learning_rate': 0.1355924526646326, 'max_depth': 3, 'min_child_weight': 8, 'reg_alpha': 0.5804793928477672, 'reg_lambda': 0.17671504264721596, 'scale_pos_weight': 96.53269537480064, 'subsample': 0.9789615757323006, 'verbosity': 1, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 177}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-07-28 16:34:23,886 INFO XGBoost-PySpark: _train_booster Training on CPUs 4]\n",
      "[16:34:24] Task 0 got rank 0\n",
      "[16:34:24] Task 1 got rank 1\n",
      "[16:34:24] Task 2 got rank 2\n",
      "[16:34:24] Task 3 got rank 3\n",
      "[16:34:25] [0]\ttraining-aucpr:0.04710\n",
      "[16:34:25] [1]\ttraining-aucpr:0.04817\n",
      "[16:34:25] [2]\ttraining-aucpr:0.05790\n",
      "[16:34:25] [3]\ttraining-aucpr:0.06307\n",
      "[16:34:25] [4]\ttraining-aucpr:0.06360\n",
      "[16:34:25] [5]\ttraining-aucpr:0.06896\n",
      "[16:34:25] [6]\ttraining-aucpr:0.07575\n",
      "[16:34:25] [7]\ttraining-aucpr:0.08255\n",
      "[16:34:25] [8]\ttraining-aucpr:0.08775\n",
      "[16:34:25] [9]\ttraining-aucpr:0.09390\n",
      "[16:34:25] [10]\ttraining-aucpr:0.09623\n",
      "[16:34:25] [11]\ttraining-aucpr:0.09678\n",
      "[16:34:25] [12]\ttraining-aucpr:0.10085\n",
      "[16:34:25] [13]\ttraining-aucpr:0.10664\n",
      "[16:34:25] [14]\ttraining-aucpr:0.10828\n",
      "[16:34:25] [15]\ttraining-aucpr:0.10937\n",
      "[16:34:25] [16]\ttraining-aucpr:0.11196\n",
      "[16:34:25] [17]\ttraining-aucpr:0.11377\n",
      "[16:34:25] [18]\ttraining-aucpr:0.11498\n",
      "[16:34:26] [19]\ttraining-aucpr:0.11553\n",
      "[16:34:26] [20]\ttraining-aucpr:0.11807\n",
      "[16:34:26] [21]\ttraining-aucpr:0.12032\n",
      "[16:34:26] [22]\ttraining-aucpr:0.12048\n",
      "[16:34:26] [23]\ttraining-aucpr:0.12229\n",
      "[16:34:26] [24]\ttraining-aucpr:0.12450\n",
      "[16:34:26] [25]\ttraining-aucpr:0.12471\n",
      "[16:34:26] [26]\ttraining-aucpr:0.12600\n",
      "[16:34:26] [27]\ttraining-aucpr:0.12754\n",
      "[16:34:26] [28]\ttraining-aucpr:0.12853\n",
      "[16:34:26] [29]\ttraining-aucpr:0.13046\n",
      "[16:34:26] [30]\ttraining-aucpr:0.13176\n",
      "[16:34:26] [31]\ttraining-aucpr:0.13286\n",
      "[16:34:26] [32]\ttraining-aucpr:0.13509\n",
      "[16:34:26] [33]\ttraining-aucpr:0.13512\n",
      "[16:34:26] [34]\ttraining-aucpr:0.13584\n",
      "[16:34:26] [35]\ttraining-aucpr:0.13710\n",
      "[16:34:26] [36]\ttraining-aucpr:0.13800\n",
      "[16:34:26] [37]\ttraining-aucpr:0.13874\n",
      "[16:34:26] [38]\ttraining-aucpr:0.14054\n",
      "[16:34:26] [39]\ttraining-aucpr:0.14200\n",
      "[16:34:26] [40]\ttraining-aucpr:0.14239\n",
      "[16:34:26] [41]\ttraining-aucpr:0.14237\n",
      "[16:34:26] [42]\ttraining-aucpr:0.14321\n",
      "[16:34:26] [43]\ttraining-aucpr:0.14438\n",
      "[16:34:26] [44]\ttraining-aucpr:0.14512\n",
      "[16:34:26] [45]\ttraining-aucpr:0.14593\n",
      "[16:34:26] [46]\ttraining-aucpr:0.14630\n",
      "[16:34:26] [47]\ttraining-aucpr:0.14633\n",
      "[16:34:26] [48]\ttraining-aucpr:0.14747\n",
      "[16:34:27] [49]\ttraining-aucpr:0.14805\n",
      "[16:34:27] [50]\ttraining-aucpr:0.14885\n",
      "[16:34:27] [51]\ttraining-aucpr:0.15013\n",
      "[16:34:27] [52]\ttraining-aucpr:0.15038\n",
      "[16:34:27] [53]\ttraining-aucpr:0.15127\n",
      "[16:34:27] [54]\ttraining-aucpr:0.15202\n",
      "[16:34:27] [55]\ttraining-aucpr:0.15234\n",
      "[16:34:27] [56]\ttraining-aucpr:0.15295\n",
      "[16:34:27] [57]\ttraining-aucpr:0.15345\n",
      "[16:34:27] [58]\ttraining-aucpr:0.15375\n",
      "[16:34:27] [59]\ttraining-aucpr:0.15438\n",
      "[16:34:27] [60]\ttraining-aucpr:0.15458\n",
      "[16:34:27] [61]\ttraining-aucpr:0.15485\n",
      "[16:34:27] [62]\ttraining-aucpr:0.15547\n",
      "[16:34:27] [63]\ttraining-aucpr:0.15604\n",
      "[16:34:27] [64]\ttraining-aucpr:0.15665\n",
      "[16:34:27] [65]\ttraining-aucpr:0.15726\n",
      "[16:34:27] [66]\ttraining-aucpr:0.15734\n",
      "[16:34:27] [67]\ttraining-aucpr:0.15776\n",
      "[16:34:27] [68]\ttraining-aucpr:0.15820\n",
      "[16:34:27] [69]\ttraining-aucpr:0.15821\n",
      "[16:34:27] [70]\ttraining-aucpr:0.15862\n",
      "[16:34:27] [71]\ttraining-aucpr:0.15933\n",
      "[16:34:27] [72]\ttraining-aucpr:0.15942\n",
      "[16:34:27] [73]\ttraining-aucpr:0.15995\n",
      "[16:34:27] [74]\ttraining-aucpr:0.16030\n",
      "[16:34:27] [75]\ttraining-aucpr:0.16088\n",
      "[16:34:27] [76]\ttraining-aucpr:0.16127\n",
      "[16:34:27] [77]\ttraining-aucpr:0.16147\n",
      "[16:34:27] [78]\ttraining-aucpr:0.16166\n",
      "[16:34:28] [79]\ttraining-aucpr:0.16162\n",
      "[16:34:28] [80]\ttraining-aucpr:0.16192\n",
      "[16:34:28] [81]\ttraining-aucpr:0.16210\n",
      "[16:34:28] [82]\ttraining-aucpr:0.16266\n",
      "[16:34:28] [83]\ttraining-aucpr:0.16302\n",
      "[16:34:28] [84]\ttraining-aucpr:0.16341\n",
      "[16:34:28] [85]\ttraining-aucpr:0.16341\n",
      "[16:34:28] [86]\ttraining-aucpr:0.16381\n",
      "[16:34:28] [87]\ttraining-aucpr:0.16407\n",
      "[16:34:28] [88]\ttraining-aucpr:0.16431\n",
      "[16:34:28] [89]\ttraining-aucpr:0.16459\n",
      "[16:34:28] [90]\ttraining-aucpr:0.16499\n",
      "[16:34:28] [91]\ttraining-aucpr:0.16526\n",
      "[16:34:28] [92]\ttraining-aucpr:0.16537\n",
      "[16:34:28] [93]\ttraining-aucpr:0.16536\n",
      "[16:34:28] [94]\ttraining-aucpr:0.16554\n",
      "[16:34:28] [95]\ttraining-aucpr:0.16567\n",
      "[16:34:28] [96]\ttraining-aucpr:0.16598\n",
      "[16:34:28] [97]\ttraining-aucpr:0.16628\n",
      "[16:34:28] [98]\ttraining-aucpr:0.16637\n",
      "[16:34:28] [99]\ttraining-aucpr:0.16657\n",
      "[16:34:28] [100]\ttraining-aucpr:0.16668\n",
      "[16:34:28] [101]\ttraining-aucpr:0.16691\n",
      "[16:34:28] [102]\ttraining-aucpr:0.16714\n",
      "[16:34:28] [103]\ttraining-aucpr:0.16737\n",
      "[16:34:28] [104]\ttraining-aucpr:0.16758\n",
      "[16:34:28] [105]\ttraining-aucpr:0.16775\n",
      "[16:34:28] [106]\ttraining-aucpr:0.16788\n",
      "[16:34:28] [107]\ttraining-aucpr:0.16822\n",
      "[16:34:28] [108]\ttraining-aucpr:0.16844\n",
      "[16:34:29] [109]\ttraining-aucpr:0.16865\n",
      "[16:34:29] [110]\ttraining-aucpr:0.16874\n",
      "[16:34:29] [111]\ttraining-aucpr:0.16904\n",
      "[16:34:29] [112]\ttraining-aucpr:0.16915\n",
      "[16:34:29] [113]\ttraining-aucpr:0.16915\n",
      "[16:34:29] [114]\ttraining-aucpr:0.16937\n",
      "[16:34:29] [115]\ttraining-aucpr:0.16964\n",
      "[16:34:29] [116]\ttraining-aucpr:0.16983\n",
      "[16:34:29] [117]\ttraining-aucpr:0.16995\n",
      "[16:34:29] [118]\ttraining-aucpr:0.17007\n",
      "[16:34:29] [119]\ttraining-aucpr:0.17031\n",
      "[16:34:29] [120]\ttraining-aucpr:0.17049\n",
      "[16:34:29] [121]\ttraining-aucpr:0.17065\n",
      "[16:34:29] [122]\ttraining-aucpr:0.17091\n",
      "[16:34:29] [123]\ttraining-aucpr:0.17105\n",
      "[16:34:29] [124]\ttraining-aucpr:0.17117\n",
      "[16:34:29] [125]\ttraining-aucpr:0.17129\n",
      "[16:34:29] [126]\ttraining-aucpr:0.17138\n",
      "[16:34:29] [127]\ttraining-aucpr:0.17138\n",
      "[16:34:29] [128]\ttraining-aucpr:0.17159\n",
      "[16:34:29] [129]\ttraining-aucpr:0.17171\n",
      "[16:34:29] [130]\ttraining-aucpr:0.17176\n",
      "[16:34:29] [131]\ttraining-aucpr:0.17182\n",
      "[16:34:29] [132]\ttraining-aucpr:0.17190\n",
      "[16:34:29] [133]\ttraining-aucpr:0.17212\n",
      "[16:34:29] [134]\ttraining-aucpr:0.17217\n",
      "[16:34:29] [135]\ttraining-aucpr:0.17230\n",
      "[16:34:29] [136]\ttraining-aucpr:0.17226\n",
      "[16:34:29] [137]\ttraining-aucpr:0.17239\n",
      "[16:34:30] [138]\ttraining-aucpr:0.17250\n",
      "[16:34:30] [139]\ttraining-aucpr:0.17269\n",
      "[16:34:30] [140]\ttraining-aucpr:0.17268\n",
      "[16:34:30] [141]\ttraining-aucpr:0.17271\n",
      "[16:34:30] [142]\ttraining-aucpr:0.17287\n",
      "[16:34:30] [143]\ttraining-aucpr:0.17300\n",
      "[16:34:30] [144]\ttraining-aucpr:0.17316\n",
      "[16:34:30] [145]\ttraining-aucpr:0.17327\n",
      "[16:34:30] [146]\ttraining-aucpr:0.17337\n",
      "[16:34:30] [147]\ttraining-aucpr:0.17356\n",
      "[16:34:30] [148]\ttraining-aucpr:0.17364\n",
      "[16:34:30] [149]\ttraining-aucpr:0.17371\n",
      "[16:34:30] [150]\ttraining-aucpr:0.17377\n",
      "[16:34:30] [151]\ttraining-aucpr:0.17383\n",
      "[16:34:30] [152]\ttraining-aucpr:0.17391\n",
      "[16:34:30] [153]\ttraining-aucpr:0.17397\n",
      "[16:34:30] [154]\ttraining-aucpr:0.17411\n",
      "[16:34:30] [155]\ttraining-aucpr:0.17412\n",
      "[16:34:30] [156]\ttraining-aucpr:0.17414\n",
      "[16:34:30] [157]\ttraining-aucpr:0.17416\n",
      "[16:34:30] [158]\ttraining-aucpr:0.17425\n",
      "[16:34:30] [159]\ttraining-aucpr:0.17435\n",
      "[16:34:30] [160]\ttraining-aucpr:0.17446\n",
      "[16:34:30] [161]\ttraining-aucpr:0.17450\n",
      "[16:34:30] [162]\ttraining-aucpr:0.17457\n",
      "[16:34:30] [163]\ttraining-aucpr:0.17466\n",
      "[16:34:30] [164]\ttraining-aucpr:0.17474\n",
      "[16:34:30] [165]\ttraining-aucpr:0.17479\n",
      "[16:34:30] [166]\ttraining-aucpr:0.17498\n",
      "[16:34:30] [167]\ttraining-aucpr:0.17499\n",
      "[16:34:30] [168]\ttraining-aucpr:0.17501\n",
      "[16:34:30] [169]\ttraining-aucpr:0.17512\n",
      "[16:34:31] [170]\ttraining-aucpr:0.17514\n",
      "[16:34:31] [171]\ttraining-aucpr:0.17523\n",
      "[16:34:31] [172]\ttraining-aucpr:0.17528\n",
      "[16:34:31] [173]\ttraining-aucpr:0.17524\n",
      "[16:34:31] [174]\ttraining-aucpr:0.17532\n",
      "[16:34:31] [175]\ttraining-aucpr:0.17541\n",
      "[16:34:31] [176]\ttraining-aucpr:0.17544\n",
      "2025-07-28 16:34:32,251 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-07-28 16:34:32,340 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n"
     ]
    }
   ],
   "source": [
    "xgb_model, best_params, best_aucpr = train_model(train_df, test_df_processed, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92d42c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = make_predictions(xgb_model, test_df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4b8580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+--------------------+----------+--------------------+\n",
      "|            features|label|          id|       rawPrediction|prediction|         probability|\n",
      "+--------------------+-----+------------+--------------------+----------+--------------------+\n",
      "|[0.86605494464508...|    0|240518168576|[2.92565608024597...|       0.0|[0.94910025596618...|\n",
      "|[-1.5334994915883...|    0|240518168577|[-0.7727064490318...|       1.0|[0.31589394807815...|\n",
      "|[1.20884843553557...|    0|240518168578|[2.13984680175781...|       0.0|[0.89471620321273...|\n",
      "|[0.86605494464508...|    0|240518168579|[0.87860780954360...|       0.0|[0.70653367042541...|\n",
      "|[1.20884843553557...|    0|240518168580|[1.87006783485412...|       0.0|[0.86646616458892...|\n",
      "+--------------------+-----+------------+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 16:34:33,141 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18d61842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.BufferedWriter name=7>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/matheus/Documents/Projects/PersonalProjects/fraud-detection/.venv/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 16:34:33,350 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2025-07-28 16:34:34,556 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2025-07-28 16:34:35,295 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.8957\n",
      "Test AUPR: 0.1943\n",
      "Test F1 Score: 0.8841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'auc': 0.8956767879116823,\n",
       " 'aupr': 0.1943340851981518,\n",
       " 'f1': 0.8841424063706734}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = evaluate_model(predictions)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51fc24ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 16:34:35,851 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>240518168576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>240518168577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>240518168578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>240518168579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>240518168580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205006</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>206158431829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205007</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206158431830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205008</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206158431831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205009</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206158431832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205010</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206158431833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205011 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  prediction            id\n",
       "0           0         0.0  240518168576\n",
       "1           0         1.0  240518168577\n",
       "2           0         0.0  240518168578\n",
       "3           0         0.0  240518168579\n",
       "4           0         0.0  240518168580\n",
       "...       ...         ...           ...\n",
       "205006      0         1.0  206158431829\n",
       "205007      0         0.0  206158431830\n",
       "205008      0         0.0  206158431831\n",
       "205009      0         0.0  206158431832\n",
       "205010      0         0.0  206158431833\n",
       "\n",
       "[205011 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_pdf = predictions.select([\"label\", \"prediction\", \"id\"]).toPandas()\n",
    "results_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19fbe237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "y_true = results_pdf['label']\n",
    "y_pred = results_pdf['prediction']\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
    "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'{model_name} Confusion Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1786d569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Features by Weight:\n",
      "Feature f3: 82.0000\n",
      "Feature f1: 81.0000\n",
      "Feature f11: 74.0000\n",
      "Feature f13: 72.0000\n",
      "Feature f10: 69.0000\n",
      "Feature f5: 62.0000\n",
      "Feature f6: 59.0000\n",
      "Feature f7: 54.0000\n",
      "Feature f0: 50.0000\n",
      "Feature f9: 45.0000\n"
     ]
    }
   ],
   "source": [
    "xgb_model_native: xgb.Booster = xgb_model.get_booster()\n",
    "\n",
    "importances = xgb_model_native.get_score(importance_type='weight')\n",
    "print(\"\\nTop 10 Features by Weight:\")\n",
    "for feature, weight in sorted(importances.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"Feature {feature}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c180cd",
   "metadata": {},
   "source": [
    "Let's save the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36ee2f33",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/home/matheus/Documents/Projects/PersonalProjects/fraud-detection/models/xgb_fraud_detection_v1_0.spark/metadata already exists. Set mode as \"overwrite\" to overwrite the existing path. SQLSTATE: 42K04",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m xgb_model_native.save_model(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m../models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mxgb_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../models/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.spark\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/PersonalProjects/fraud-detection/.venv/lib/python3.13/site-packages/pyspark/ml/util.py:608\u001b[39m, in \u001b[36mMLWritable.save\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    607\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/PersonalProjects/fraud-detection/.venv/lib/python3.13/site-packages/pyspark/ml/util.py:502\u001b[39m, in \u001b[36mMLWriter.save\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.shouldOverwrite:\n\u001b[32m    501\u001b[39m     \u001b[38;5;28mself\u001b[39m._handleOverwrite(path)\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msaveImpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/PersonalProjects/fraud-detection/.venv/lib/python3.13/site-packages/xgboost/spark/core.py:1767\u001b[39m, in \u001b[36mSparkXGBModelWriter.saveImpl\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m   1765\u001b[39m xgb_model = \u001b[38;5;28mself\u001b[39m.instance._xgb_sklearn_model\n\u001b[32m   1766\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m xgb_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m \u001b[43m_SparkXGBSharedReadWrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43msaveMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m model_save_path = os.path.join(path, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1769\u001b[39m booster = xgb_model.get_booster().save_raw(\u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/PersonalProjects/fraud-detection/.venv/lib/python3.13/site-packages/xgboost/spark/core.py:1656\u001b[39m, in \u001b[36m_SparkXGBSharedReadWrite.saveMetadata\u001b[39m\u001b[34m(instance, path, sc, logger, extraMetadata)\u001b[39m\n\u001b[32m   1653\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m conf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1654\u001b[39m         extraMetadata[\u001b[33m\"\u001b[39m\u001b[33mcoll_cfg\u001b[39m\u001b[33m\"\u001b[39m] = asdict(conf)\n\u001b[32m-> \u001b[39m\u001b[32m1656\u001b[39m \u001b[43mDefaultParamsWriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msaveMetadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1657\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextraMetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextraMetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjsonParams\u001b[49m\n\u001b[32m   1658\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m init_booster \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1660\u001b[39m     ser_init_booster = serialize_booster(init_booster)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/PersonalProjects/fraud-detection/.venv/lib/python3.13/site-packages/pyspark/ml/util.py:816\u001b[39m, in \u001b[36mDefaultParamsWriter.saveMetadata\u001b[39m\u001b[34m(instance, path, sc, extraMetadata, paramMap)\u001b[39m\n\u001b[32m    812\u001b[39m metadataJson = DefaultParamsWriter._get_metadata_to_save(\n\u001b[32m    813\u001b[39m     instance, sc, extraMetadata, paramMap\n\u001b[32m    814\u001b[39m )\n\u001b[32m    815\u001b[39m spark = sc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sc, SparkSession) \u001b[38;5;28;01melse\u001b[39;00m SparkSession._getActiveSessionOrCreate()\n\u001b[32m--> \u001b[39m\u001b[32m816\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadataJson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadataPath\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/PersonalProjects/fraud-detection/.venv/lib/python3.13/site-packages/pyspark/sql/readwriter.py:2056\u001b[39m, in \u001b[36mDataFrameWriter.text\u001b[39m\u001b[34m(self, path, compression, lineSep)\u001b[39m\n\u001b[32m   2008\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Saves the content of the DataFrame in a text file at the specified path.\u001b[39;00m\n\u001b[32m   2009\u001b[39m \u001b[33;03mThe text files will be encoded as UTF-8.\u001b[39;00m\n\u001b[32m   2010\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2053\u001b[39m \u001b[33;03m+---------+\u001b[39;00m\n\u001b[32m   2054\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2055\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(compression=compression, lineSep=lineSep)\n\u001b[32m-> \u001b[39m\u001b[32m2056\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/PersonalProjects/fraud-detection/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/PersonalProjects/fraud-detection/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_ALREADY_EXISTS] Path file:/home/matheus/Documents/Projects/PersonalProjects/fraud-detection/models/xgb_fraud_detection_v1_0.spark/metadata already exists. Set mode as \"overwrite\" to overwrite the existing path. SQLSTATE: 42K04"
     ]
    }
   ],
   "source": [
    "xgb_model_native.save_model(f\"../models/{model_name}.json\")\n",
    "xgb_model.save(f\"../models/{model_name}.spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06020570",
   "metadata": {},
   "source": [
    "### Fairness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "new_fairness_df = results_pdf.join(pdf_fairness, how='left', rsuffix='_right').drop(columns=[\"id_right\"])\n",
    "y_test = new_fairness_df[\"label\"]\n",
    "y_pred = new_fairness_df[\"prediction\"]\n",
    "sensitive_features = new_fairness_df[[\"foreign_request\", \"age_group\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833ca02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Selection Rate              0.165259\n",
       "F Beta 1.4                  0.166346\n",
       "False Positive Rate         0.156625\n",
       "counts                 205011.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairlearn.metrics import MetricFrame, equalized_odds_ratio, selection_rate, false_positive_rate, count\n",
    "from sklearn.metrics import fbeta_score\n",
    "import functools\n",
    "\n",
    "fbeta_14 = functools.partial(fbeta_score, beta=1.4, zero_division=1)\n",
    "eq_odds_ratio = functools.partial(equalized_odds_ratio, sensitive_features=sensitive_features)\n",
    "\n",
    "metrics = {\n",
    "    \"Selection Rate\": selection_rate,\n",
    "    \"F Beta 1.4\": fbeta_14,\n",
    "    #\"Equalized Odds Ratio\": eq_odds_ratio,\n",
    "    \"False Positive Rate\": false_positive_rate,\n",
    "    \"counts\": count\n",
    "}\n",
    "\n",
    "grouped_sensitive_features = MetricFrame(\n",
    "    metrics=metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=sensitive_features\n",
    ")\n",
    "grouped_sensitive_features.overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f77e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Selection Rate</th>\n",
       "      <th>F Beta 1.4</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foreign_request</th>\n",
       "      <th>age_group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Foreign</th>\n",
       "      <th>Adolescent</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adult</th>\n",
       "      <td>0.263190</td>\n",
       "      <td>0.237628</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>3241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elderly</th>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.475965</td>\n",
       "      <td>0.345238</td>\n",
       "      <td>189.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Young Adult</th>\n",
       "      <td>0.129062</td>\n",
       "      <td>0.088921</td>\n",
       "      <td>0.126058</td>\n",
       "      <td>1077.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Not Foreign</th>\n",
       "      <th>Adolescent</th>\n",
       "      <td>0.055334</td>\n",
       "      <td>0.083469</td>\n",
       "      <td>0.053768</td>\n",
       "      <td>5024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adult</th>\n",
       "      <td>0.183252</td>\n",
       "      <td>0.159357</td>\n",
       "      <td>0.174395</td>\n",
       "      <td>142230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elderly</th>\n",
       "      <td>0.349507</td>\n",
       "      <td>0.259007</td>\n",
       "      <td>0.325502</td>\n",
       "      <td>7914.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Young Adult</th>\n",
       "      <td>0.081610</td>\n",
       "      <td>0.124713</td>\n",
       "      <td>0.078143</td>\n",
       "      <td>45264.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Selection Rate  F Beta 1.4  False Positive Rate  \\\n",
       "foreign_request age_group                                                      \n",
       "Foreign         Adolescent         0.125000    0.000000             0.126761   \n",
       "                Adult              0.263190    0.237628             0.244898   \n",
       "                Elderly            0.407407    0.475965             0.345238   \n",
       "                Young Adult        0.129062    0.088921             0.126058   \n",
       "Not Foreign     Adolescent         0.055334    0.083469             0.053768   \n",
       "                Adult              0.183252    0.159357             0.174395   \n",
       "                Elderly            0.349507    0.259007             0.325502   \n",
       "                Young Adult        0.081610    0.124713             0.078143   \n",
       "\n",
       "                               counts  \n",
       "foreign_request age_group              \n",
       "Foreign         Adolescent       72.0  \n",
       "                Adult          3241.0  \n",
       "                Elderly         189.0  \n",
       "                Young Adult    1077.0  \n",
       "Not Foreign     Adolescent     5024.0  \n",
       "                Adult        142230.0  \n",
       "                Elderly        7914.0  \n",
       "                Young Adult   45264.0  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_sensitive_features.by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135915ea",
   "metadata": {},
   "source": [
    "# Wrapping up\n",
    "\n",
    "We trained our model and evaluated it, looking into its fairness too. The model performance is not good enough, but we only experimented with one model. For the next steps, we will test our model in the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
